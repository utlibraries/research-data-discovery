# Scripted process for retrieving metadata on institutional-affiliated research dataset publications

## Metadata
* *Version*: 2.1.0.
* *Released*: 2025/03/11
* *Author(s)*: Bryan Gee (UT Libraries, University of Texas at Austin; bryan.gee@austin.utexas.edu; ORCID: [0000-0003-4517-3290](https://orcid.org/0000-0003-4517-3290))
* *Contributor(s)*: None
* *License*: [MIT](https://opensource.org/license/mit)
* *README last updated*: 2025/03/11

## Table of Contents
1. [Purpose](#purpose)
2. [Organization & file list](#organization--file-list)
3. [Overview](#overview)
4. [Important caveats](#important-caveats)
5. [Re-use](#re-use)
6. [Planned development](#planned-development)

## Purpose
This repository contains Python code that is designed to gather and organize metadata from a number of individual research data repository/platform APIs in order to analyze and summarize research dataset publications that are affiliated with at least one researcher from a particular institution. This code is being developed in the specific context of retrieving data for the University of Texas at Austin and is intended to be eventually be used in tandem with [separate but related work](https://github.com/UT-OSPO/institutional-innovation-grapher) that searches for UT-Austin-affiliated GitHub repositories in order to build a more comprehensive understanding of how researchers on campus are sharing their research outputs. However, this code has been constructed to be stand-alone and can be adapted for use at other institutions.

## Organization & file list
1. **dataset-records-retrieval.py**: This is the primary Python script for conducting large-scale records retrieval through the DataCite API. It also includes functionality for using a set of different APIs to try and identify deposits on Figshare that lack affiliation metadata but that can be connected to an article with at least one author from a focal institution.
2. **config-template.json**: This is the config file that contains most parameters and that stores API keys. This file should be populated with personal information as necessary, with affiliation permutations modified if applying this to a different institution, and renamed as *config.json* in order for scripts to work.
3. **accessory-scripts/dataset-records-retrieval-visualization.py**: This file contains the code used to generate visuals for the 2025 RDAP Summit.
4. **accessory-scripts/figshare-plos-osi-search.py**: This file contains the code used to retrieve the latest version of the [PLOS Open Science Indicators (OSI) Dataset](https://plos.figshare.com/articles/dataset/PLOS_Open_Science_Indicators/21687686), identifies articles that list data as having been shared in part or in whole through Supplemental Information (mediated Figshare deposit for all PLOS titles), retrieves a list of PLOS articles with at least one author from a focal institution, and searches for matches to identify PLOS articles co-authored by a university researcher where 'data' were deposited on Figshare through the mediated process.
5. **accessory-data/20250310-mediated-figshare-metadata-summary.csv**: This file contains a manually compiled summary of select metadata for Figshare deposits mediated through [publisher partners](https://info.figshare.com/working-with/)(filter on 'Publishers'); it is intended to provide insight into possible filter parameters that may permit their programmatic retrieval. This is a static file created on 2025/03/10, and partners/metadata may change in the future (e.g., SciELO journals was listed the last time I examined this in October 2024). Briefly, I accessed each publisher's Figshare collection through the web interface and selected 10 random deposits, with preference given to recent deposits. A few listed publishers are not recorded in the CSV file: JACC and SAGE redirect to the publishers' homepage, not a Figshare collection; Human Genome Variation is a database; and IEEE Standards, Medical Affairs Professional Society, Optica Open, and Physiome appeared to contain out-of-scope topic (e.g., only preprints in Optica Open). I recorded which indexer (DataCite vs. Crossref) was used to mint the DOI; what the listed publisher name is (*listed_publisher*); the *client-id* and *provider-id* if minted through DataCite, as these represent queryable fields that indicate a Figshare connection; up to 10 DOIs that were examined; whether the DOIs contain the string 'figshare' (*doi_figshare*); and how the DOIs were constructed (*doi_construction*). There are only a few DOI construction models: a default Figshare DOI that is randomly created and assigned with the prefix '10.6084/m9.figshare'; appending .t00x or .s00x to the end of the article DOI, where 'x' is a sequential integer; or a randomly created DOI using the associated journal/publisher's DOI prefix.

Several scripts will create additional subdirectories as part of their workflow (e.g., 'outputs'); these subdirectories are not provided here.

## Overview
### Primary workflow
The core workflow contained in **dataset-records-retrieval.py** makes use of four REST APIs in order to conduct a large-scale initial sweep for university-affiliated datasets based on a set of permutations for the institutional name: [DataCite](https://support.datacite.org/docs/api); [Dataverse](https://guides.dataverse.org/en/latest/api/index.html); [Dryad](https://datadryad.org/stash/api); and [Zenodo](https://developers.zenodo.org/). Note that the Dataverse code is configured specifically for the [Texas Data Repository (TDR)'s](https://dataverse.tdl.org/) instance. Other APIs have been cursorily explored for dataset retrieval but are not currently incorporated here: [Crossref](https://www.crossref.org/documentation/retrieve-metadata/rest-api/); [Figshare](https://docs.figshare.com/#figshare_documentation_api_description_searching_filtering_and_pagination); [Mendeley Data](https://data.mendeley.com/api/docs/); [OpenAlex](https://docs.openalex.org/how-to-use-the-api/api-overview); and [Open Science Framework (OSF)](https://developer.osf.io/#tag/Filtering). 

The code is designed to maximize the potential retrieval scope of a given API query, specifically as it relates to the fields in which an affiliation may be found (which is not always the 'affiliation' field) and the various permutations for UT Austin specifically (e.g., 'University of Texas at Austin' versus 'University of Texas, Austin'). Even though the three repositories that are integrated into the primary workflow (Dataverse, Dryad, Zenodo) all mint their DOIs through DataCite and should thus be discoverable collectively through the DataCite API, the individual repository APIs were queried as both a cross-validation process and an exploration of whether there might be some important variability in metadata cross-walks; an initial inability to perfectly cross-validate all three repositories records in the early stages of this code's development facilitated refinement of the workflow and identified edge-case scenarios. An additional benefit of exploring repository-specific APIs is the potential to identify additional metadata that are not cross-walked to DataCite (possibly because they are not supported in the present schema), such as certain controlled vocabularies.

The primary script consists of four major components: 
1) API query construction and calls;
2) Filtering of the JSON response and conversion to a pandas dataframe;
3) Cross-validation checks of the responses from individual repositories' API against their equivalent output as retrieved from the DataCite API; and
4) Concatenation and de-duplication, with the 'original' (specific repository API) source preferred when a dataset was returned by both the repository API and the DataCite API.

The cross-validation step is optional and can be enabled/disabled with a single Boolean variable; if disabled, DataCite will be the exclusive source of retrieved information. De-duplication is necessary regardless of whether cross-validation is implemented or not, primarily due to variable granularity of DOI assignment between repositories.

### Secondary workflows
Based on the results of early testing with the primary workflow, I am now developing targeted secondary workflows that attempt to fill known gaps (e.g., paucity of Figshare deposits). **Version 2.0.0** introduces several secondary workflows aimed at finding Figshare deposits, which largely lack affiliation metadata but which can be discovered through a number of different means, especially when they have been automatically created through a partner journal's manuscript submission process/portal. The **dataset-records-retrieval.py** script contains three secondary workflows that can be toggled on or off with Boolean variables. 

The first workflow takes Figshare deposits that were recovered with affiliation metadata and searches for article metadata in order to identify patterns (e.g., for UT Austin, all such deposits are linked to articles in Springer Nature journals). The second and third workflows represent two different ways to identify affiliated Figshare deposits that lack affiliation metadata. 

The second workflow takes advantage of the fact that for many partner journals, mediated Figshare deposits are listed with the publisher in the 'publisher' metadata field, rather than Figshare. This workflow thus retrieves all datasets with a publisher listing like 'Taylor & Francis' through DataCite, retrieves university-affiliated articles published by that same publisher from OpenAlex (which uses ROR to standardize affiliation metadata), and looks for matches. It should be noted that there is widespread variation in how mediated Figshare deposits are classified in the DataCite resource type schema, so not all objects labeled as 'dataset' are perhaps datasets, and not all objects containing 'data proper' will be labeled as 'dataset' (other options include 'text', 'component', and 'collection'). 

The third workflow takes advantage of a different configuration in certain journals in which mediated Figshare deposits are minted through Crossref with a DOI that appends '.s00x' to the end of the associated article DOI where 'x' is a sequential number. This workflow retrieves all university-affiliated articles from a publisher that is known to do this (e.g., PLOS) with **journal-list.json**, constructs a hypothetical mediated Figshare DOI by adding '.s001' to the article DOI, and then tests whether that link exists. This is a more time-intensive process and will only identify that there is some sort of Figshare deposit - this may not be classified as a 'dataset' in the metadata or in a conceptual sense. 

There is also a highly specific script, **figshare-plos-osi-search.py**, which retrieves the [PLOS Open Science Indicators (OSI) dataset](https://plos.figshare.com/articles/dataset/PLOS_Open_Science_Indicators/21687686) through the Figshare API. This dataset encompasses all PLOS articles (through a certain timeframe) and has identified locations of data sharing. Because PLOS uses the mediated Figshare process, any article listed as having shared data as Supplemental Information has created a mediated Figshare deposit. This workflow (which runs in about 30 seconds), will identify all such articles, retrieve a list of university-affiliated PLOS articles through OpenAlex, and find matches. This script was mostly used as an initial test of concept for developing the above workarounds for Figshare deposits that can be applied across more publishers, but it can be useful for quickly getting an estimate of what proportion of articles have generated a Figshare deposit. Refer to the PLOS OSI methodology documentation for more details on how their dataset was generated; the same caveats of whether a deposit is a 'dataset proper' remain.

The [DataCite Citation Corpus](https://support.datacite.org/docs/data-citation-corpus) was explored but is not presently incorporated because it returned very few results for UT Austin. 

## Important caveats
### Object classification
The primary workflow and certain secondary workflows only collect items that are labeled as a 'Dataset' in the [DataCite metadata schema](https://datacite-metadata-schema.readthedocs.io/en/4.6/introduction/about-schema/) (for some repositories, this is the only allowable object type). It is a given that not all of these meet the criteria for 'data' proper, in part or in whole, and may include other materials like appendices or software; the present workflow does not attempt to make inferences on the precise nature of content (although this is planned). Conversely, some deposits that do constitute 'data' proper are labeled as another object type (e.g., 'Component,' 'Text'), and these are not presently detected. Retrieving objects through the DataCite API requires downstream processing, as some objects that labeled as 'datasets' are either individual files within a DOI-backed deposit (common to Dataverse installations) or are versions of the same deposit ([Zenodo, which mints a parent DOI and then a separate DOI for each version](https://zenodo.org/help/versioning)). The primary script omits individual files that are part of a larger project and restricts the Zenodo deposits to a single record per 'lineage' of deposits. 

### Distinctiveness of deposits
There are additional considerations to keep in mind related to how research organize materials within a single project. In some instances, distinct deposits with separate DOIs may in fact be part of the same project (e.g., associated with a single manuscript), and some calculations might wish to further consolidate these to attempt to capture the number of 'unique projects' with at least one dataset. For example, Dataverse has the relatively unique '[dataverse](https://guides.dataverse.org/en/latest/user/dataverse-management.html)' object, a non-DOI-backed structure in which other dataverses and DOI-backed datasets can be nested. For this reason, some researchers will separate the materials for a single manuscript along some logical delineation (e.g., by data format; data vs. software) into multiple DOI-backed deposits that are housed within a single dataverse ([example in the Texas Data Repository](https://dataverse.tdl.org/dataverse/DMD-MLA-01)), whereas if those materials had been deposited in a different repository without an equivalent higher-level structure, they might have been deposited together in one PID-backed deposit. 

Consolidation along these lines can be done by deduplicating with a stricter combination of attributes on the assumption that related deposits likely have nearly identical metadata (e.g., publication date, author list); it may also be possible to use relations to other objects, if provided (this is more likely to be exclusively recorded in a repository-specific API). The theoretical concept of consolidation that is given above for Dataverse could be accomplished with the Dataverse API since the dataverse in which a dataset is housed can be retrieved, but this would not be possible through the DataCite API since this information does not cross-walk (likely because dataverses do not receive DOIs and an equivalent structure is otherwise rare in other repositories). The present version of the primary workflow does not currently consolidate Dataverse deposits, but given the potential use of the Harvard Dataverse by a large number of non-Harvard researchers, this functionality may be developed in the future.

The Zenodo process of minting two DOIs for an initial release of a deposit was previously noted. Other repositories also do the same (e.g., Figshare, ICPSR, Mendeley Data) and need to be de-duplicated in the same fashion. Whether all versions of a single dataset should be counted as separate datasets may vary between institutions, but this workflow usually treats a 'lineage' of many versions as a single dataset. A final consideration with Figshare deposits (with or without affiliation metadata) is that there is variation in whether a journal-mediated process will create one deposit for all files associated with one manuscript or one deposit for each file associated with that manuscript. The latter is considered to be overly granular since those objects probably would not be deposited as separate deposits (e.g., two supplemental tables) in a human-controlled process. The workflow also accounts for this and uses the *relatedIdentifier* field to consolidate entries.

### Use of 'contact' rather than 'creator' for Dataverse retrieval
It goes without saying that how affiliation metadata are cross-walked and exposed impacts the scale of what can and cannot be retrieved with this workflow. Specifically for Dataverse, the [Search API](https://guides.dataverse.org/en/latest/api/search.html), which casts a wide net to retrieve many records (efficient, not impacted by rate limiting), does not return affiliation metadata for the 'creator' (author) field, only for the 'contact.' It is possible to get the 'creator' affiliation metadata through the Native API, but this requires passing a list of DOIs of interest to the [Native API](https://guides.dataverse.org/en/latest/api/native-api.html), which then makes a request for each DOI (less efficient, can be impacted by rate limiting). The Dataverse component of the cross-validation process thus returns the listed 'contact' affiliation; it is inferred that in the overwhelming majority of cases, the point of contact(s) is probably also listed as an author or, at the very least, is from the same institution (i.e. that 'contact' affiliation is a good proxy for 'creator' affiliation). 

## Re-use
This script can be freely re-used, re-distributed, and modified in line with the associated [MIT license](https://opensource.org/license/mit). If a re-user is only seeking to replicate a UT-Austin-specific output or to retrieve an equivalent output for a different institution, the script will require very little modification - essentially only the defining of affiliation parameters will be necessary. For other Dataverse-based platforms that have significantly altered the metadata framework, it is possible that additional edits to the API call and subsetting of the response will be necessary. If additional fields or processing of the output are desired, the script will require more substantive modification and knowledge of the specific structure of a given API response. 

### Disclaimer
This workflow is, and likely will always be, perpetually under development. Because of the marked heterogeneity in how datasets are shared (e.g., lack of persistent identifiers; use of identifiers other than DOIs; variation in affiliation metadata), it is practically assured that not all datasets will be captured by this workflow or any other and that substantial gaps may exist for certain platforms/avenues for data sharing. Reusers should be cognizant of these limitations in determining how data gained from this workflow may inform decision-making. The creator(s) and contributor(s) of this repository and any entities to which they are affiliated are not responsible for any decisions, policies, or other actions that are made on the basis of obtained data.

### Config file
API keys and numerical API query parameters (e.g., records per page, page limit) are defined in a *config.json* file. The file included in this repository called *config-template.json* should be populated with API keys (see below) and renamed. 

### Third-party API access
Users will need to create accounts for [Dataverse](https://guides.dataverse.org/en/latest/api/auth.html) and [Zenodo](https://developers.zenodo.org/) in order to obtain personalized API keys, add those to the *config-template.json* file, and rename it as *config.json*. Note that if you wish to query multiple Dataverse installations (e.g., a non-Harvard institutional dataverse and Harvard Dataverse), you will need to create an account and get a separate API key for each installation. Crossref, DataCite, Dryad, Figshare, and OpenAlex do not require API keys for standard access. 

### Constructing API query parameters
If users need to modify the existing API queries, they should refer to the previously linked API documentation for specific APIs. For targeting a different institution (or set of institutions), users will need to identify a list of possible permutations of the institutional name; the use of of [ROR identifiers](https://ror.org/) in either the DataCite API or most repositories' specific API will fail to retrieve most related deposits because most repositories have not implemented ROR into their platforms given its relatively recent added support in the DataCite schema (Dryad is a notable example as an early adopter of ROR). It may also not be feasible for platforms to retroactively add ROR identifiers for all previously published deposits in an efficient programmatic fashion without potentially introducing errors. The optional cross-validation step can facilitate identification of some permutations if querying an API that does not require an exact string match for retrieval based on affiliation. Another approach is to compile a list of known affiliated deposits within and across different repositories and then to examine their metadata in the DataCite API; testing this on some of my own datasets led to the discovery of a lack of recording of affiliation in Figshare metadata, for example. A third approach would be to survey affiliated scholarly articles, books, and preprints (e.g., through the Crossref API, which does not require an exact string match for affiliation).

If the Dataverse cross-validation step is enabled for a different installation than the Texas Data Repository, the DOI prefix should be modified accordingly. The 'subtree' parameter can probably be removed as well since most Dataverse installations are not multi-institutional (each Texas-based partner has a separate subtree within TDR).

### Test environment
A Boolean variable called *test*, located immediately after the importing of packages, can be used to create a 'test environment.' If this setting is set to TRUE, the script is set to only retrieve 5 pages from the DataCite API (currently a full run requires more than 77 pages with page size of 1,000 records for UT Austin). Currently, the number of records retrieved from the three other APIs utilized here (Dataverse, Dryad, Zenodo) is significantly smaller, so different page limits for a test run are not defined for these (but could be added). 

### Cross-validation
Similar to the test environment, a Boolean value called *crossValidate* (located immediately after *test*) can be used to toggle the cross-validation component on and off (TRUE will retrieve records from other APIs and cross-validate against DataCite). A future version of the script will allow for toggling of the use of the Dataverse API in the cross-validation process.

* For UT Austin users: there should be no reason why you need to run the cross-validation process since I have used it to refine the workflow into the present state (e.g., to account for different permutations of the institutional name).
* For non-UT Austin users: if you are at another institution and want to adapt this workflow, running the cross-validation is recommended, especially if you are at an institution that similarly is part of a broader system or that has its name listed in a wide variety of ways.

### Rate limiting
In the present configuration, any rate limiting is unlikely to affect the workflows or require modification because of how queries are not targeting specific DOIs (i.e. a few requests return many records). However, potential/planned expansion may necessitate the use of targeted single-object retrieval, and users should be aware that many public APIs impose some kind of rate limiting (e.g., [Dryad](https://datadryad.org/api); [OpenAlex](https://help.openalex.org/hc/en-us/articles/24397762024087-Pricing); [Zenodo](https://developers.zenodo.org/#rate-limiting)). Dataverse installations may or may not have rate limits; for users attempting to retrieve data from the Texas Data Repository, there are currently no rate limits, although a to-be-determined limit is planned in the near future.

### Predicted runtime
Exact runtime will vary on local internet speed and external server traffic. Typically, a run of the primary workflow without cross-validation (only retrieving from DataCite) or any of the secondary Figshare workflows should complete in under 15 minutes for UT Austin or an institution of predicted similar research output. If cross-validation is employed, a run should complete in under 20 minutes for UT Austin or an institution of predicted similar research output. The test environment without cross-validation should complete in about 1 minute; if cross-validation is employed, it should complete in about 8-9 minutes. 

Adding one of the Figshare workflows can significantly increase the runtime, especially if many publishers are queried; significant variation between institutions in publishing volume with a certain publisher is expected. 

If a script finishes significantly faster than you expect or have experienced in previous runs, check the number of returned records; sometimes server instability or high traffic leads to incomplete retrieval. 

## Planned development
This workflow is intended to be continually developed by members of the Research Data Services team at the UT Libraries (guidelines for external contributions are in development) in order to continually refine the process and expand the capture potential. Product development ideas/plans are listed as '[Issues](https://github.com/utlibraries/research-data-discovery/issues)'. In general, the next stage of development will seek to expand the secondary Figshare workflows to capture data for all partner publishers (see [list of partner journals](https://info.figshare.com/working-with/)). 

Another major task will be using specific repository APIs (mostly generalist repositories) to gain additional metadata that is not / cannot be cross-walked to DataCite, such as file formats in a given deposit, in order to assess the classification accuracy of objects (e.g., whether an object not labeled as 'dataset' contains unambiguous data formats like shapefiles). A second major goal is to explore development of workflows that can identify deposits without a digital PID URL (e.g., NCBI deposits). 

The use of OAI-PMH protocols and large "data dumps" (like the 200 GB [Crossref public data file](https://www.crossref.org/learning/public-data-file/)) are under consideration for future incorporation, but a secondary objective of this workflow is to employ code and data sources that are both accessible and computationally tractable for a wide range of potential users who may not have access to above-average storage or computing capacities or even much exposure to code. 

Finally, a number of other relatively well-known specialist repositories that were not detected thus far (e.g., Qualitative Data Repository) will be examined in order to determine how best to ensure their retrieval; this may require inefficient batch capture of all deposits in a repository with subsequent filtering.

## Version notes (2.1.0)

The current version scheme follows a MAJOR.MINOR.PATCH format, with a 'major' change involving added functionality or significant revisions to the workflow; a 'minor' change involving addition of accessory files or minor revisions to the workflow (e.g., refactoring); and a 'patch' is a bug fix. We plan to make formal releases synced with a DOI-backed deposit and will reset the version at that point.

Version **2.1.0** adds the summary CSV of publisher-mediated Figshare deposits and makes minor edits to the README, including adding a disclaimer about completeness of records.

Version **2.0.0** adds functionality to the primary *dataset-records-retrieval.py* script to enable workarounds to the lack of affiliation metadata for most Figshare deposits and was slightly refactored in some areas for consolidation. A sample input file for one of the possible Figshare workarounds (*journal-list.json*) has also been added. This version also adds the two scripts in *accessory-scripts*. Ancillary files (*config-template.json*, *.gitignore*, and this README have been updated)