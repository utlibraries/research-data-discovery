{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4d8190",
   "metadata": {},
   "source": [
    "# Scripted process for retrieving metadata on institutional-affiliated research dataset publications\n",
    "\n",
    "## Metadata\n",
    "* *Version*: 2.2.0 (**not released to Zenodo**)\n",
    "* *Released*: 2026/02/09\n",
    "* *Author(s)*: Bryan Gee (UT Libraries, University of Texas at Austin; bryan.gee@austin.utexas.edu; ORCID: [0000-0003-4517-3290](https://orcid.org/0000-0003-4517-3290))\n",
    "* *Contributor(s)*: None\n",
    "* *License*: [MIT](https://opensource.org/license/mit)\n",
    "* *README last updated*: 2026/02/09\n",
    "\n",
    "**For more information and supporting documentation, see the [GitHub repository](https://github.com/utlibraries/research-data-discovery)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89203b94",
   "metadata": {},
   "source": [
    "## Core packages\n",
    "\n",
    "The following packages are required to run both the core and secondary parts of this workflow; all of them should either be pre-installed with Python or can be installed via *pip*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pprint import pformat\n",
    "from rapidfuzz import process, fuzz\n",
    "from urllib.parse import quote\n",
    "from utils import adjust_descriptive_count, check_link, count_words, determine_affiliation, retrieve_all_journals, retrieve_crossref, retrieve_datacite, retrieve_dataverse, retrieve_dryad, retrieve_openalex, retrieve_zenodo #custom functions file\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf81433",
   "metadata": {},
   "source": [
    "## Toggles\n",
    "\n",
    "The following toggles can be set to `True` or `False` to enable or disable certain parts of the workflow. For the NCBI workflow, additional packages will need to be installed and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fa9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#read in config file\n",
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "#operator for quick test runs\n",
    "test = config['TOGGLES']['test']\n",
    "\n",
    "#operator for resource types to query for (use OR and put in parentheses for multiple types)\n",
    "##GENERAL DataCite query\n",
    "resource_types = ['Dataset', 'Software']\n",
    "# Datacite format\n",
    "datacite_resource_type = '(' + ' OR '.join(resource_types) + ')'\n",
    "# Zenodo format\n",
    "zenodo_resource_type = '(' + ' OR '.join([f'type:{rt.lower()}' for rt in resource_types]) + ')'\n",
    "\n",
    "##create string to include in filenames based on resource type\n",
    "resource_filename = '-'.join([rt.lower() for rt in resource_types])\n",
    "##toggle based on whether resource_type is used in the API query\n",
    "resource_type_filter = config['TOGGLES']['resource_type_filter']\n",
    "if resource_type_filter:\n",
    "    resource_filename = resource_filename\n",
    "else:\n",
    "    resource_filename = 'all-resource-types'\n",
    "\n",
    "#operator for resource type(s) to query for (use OR and put in parentheses for multiple types)\n",
    "##Figshare workflow\n",
    "figshare_resource_types = ['Dataset', 'Software']\n",
    "figshare_datacite_resource_type = '(' + ' OR '.join(figshare_resource_types) + ')'\n",
    "figshare_resource_filename = '-'.join([rt.lower() for rt in figshare_resource_types])\n",
    "##toggle based on whether resource_type is used in the API query\n",
    "figshare_resource_type_filter = config['TOGGLES']['figshare_resource_type_filter']\n",
    "if figshare_resource_type_filter:\n",
    "    figshare_resource_filename = figshare_resource_filename\n",
    "else:\n",
    "    figshare_resource_filename = 'all-resource-types'\n",
    "\n",
    "#toggle for cross-validation steps\n",
    "cross_validate = config['TOGGLES']['cross_validate']\n",
    "##toggle for Dataverse cross-validation specifically\n",
    "dataverse = config['TOGGLES']['dataverse']\n",
    "##toggle for de-duplicating partial Dataverse replicates (multiple deposits for one manuscript within one dataverse) - see README for details\n",
    "dataverse_duplicates = config['TOGGLES']['dataverse_duplicates']\n",
    "##toggle for UT Austin specific edge cases (set to False if you are not at UT Austin)\n",
    "austin = config['TOGGLES']['austin']\n",
    "\n",
    "#toggles for executing Figshare processes (see README for details)\n",
    "##looking for datasets with a journal publisher listed as publisher, X-ref'ing with university articles from that publisher\n",
    "figshare_workflow_1 = config['TOGGLES']['figshare_workflow_1']\n",
    "##finding university articles from publisher that uses certain formula for Figshare DOIs, construct hypothetical DOI, test if it exists\n",
    "figshare_workflow_2 = config['TOGGLES']['figshare_workflow_2']\n",
    "\n",
    "##if you have done a previous DataCite retrieval and don't want to re-run the entire main process (skip to Figshare steps)\n",
    "load_previous_data = config['TOGGLES']['load_previous_data']\n",
    "#if you have done a previous DataCite retrieval and Figshare workflow 1 and don't want to re-run these\n",
    "load_previous_data_plus = config['TOGGLES']['load_previous_data_plus']\n",
    "#toggle for executing NCBI process\n",
    "ncbi_workflow = config['TOGGLES']['ncbi_workflow']\n",
    "##loading package in only if running NCBI workflow\n",
    "if ncbi_workflow:\n",
    "    import xml.etree.ElementTree as ET\n",
    "#toggle for whether to use biopython approach to NCBI (TRUE = biopython; FALSE = Selenium)\n",
    "biopython = config['TOGGLES']['biopython']\n",
    "##loading packages in only if running NCBI workflow and depending on selection\n",
    "if biopython and ncbi_workflow:\n",
    "    from Bio import Entrez\n",
    "elif not biopython and ncbi_workflow:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "    from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "#toggle for skipping web retrieval of NCBI data (just XML to dataframe conversion)\n",
    "load_ncbi_data = config['TOGGLES']['load_ncbi_data']\n",
    "#toggle for loading previous DataCite + Figshare workflow 1 + NCBI\n",
    "load_previous_data_plus_ncbi = config['TOGGLES']['load_previous_data_plus_ncbi']\n",
    "#toggle to load in externally generated Crossref data\n",
    "load_crossref = config['TOGGLES']['load_crossref']\n",
    "\n",
    "#conditional toggles, if loading in previous data, automatically set certain other toggles to False regardless of how they are set\n",
    "##should minimize how much you need to edit multiple toggles (W.I.P.)\n",
    "if load_previous_data_plus:\n",
    "    figshare_workflow_1 = False\n",
    "    figshare_workflow_2 = False\n",
    "if load_previous_data_plus_ncbi:\n",
    "    figshare_workflow_1 = False\n",
    "    figshare_workflow_2 = False\n",
    "    ncbi_workflow = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d1457",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The following codeblock defines various parameters that are used in API calls, sets up the various working directories, and pulls in the *config.json* file, which provides additional parameters. API keys and institution-specific parameters will need to be set in the *config.json* file. If you want to change certain parameters for a given API, refer to the official API documentation; in most instances, the ones included in the *config-template.json* file are set at or near the maximum allowed except for the total number of pages/results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c297dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating directories\n",
    "##write logs regardless of env\n",
    "if os.path.isdir('logs'):\n",
    "        print('logs directory found - no need to recreate')\n",
    "else:\n",
    "    os.mkdir('logs')\n",
    "    print('logs directory has been created')\n",
    "if test:\n",
    "    if os.path.isdir('test'):\n",
    "        print('test directory found - no need to recreate')\n",
    "    else:\n",
    "        os.mkdir('test')\n",
    "        print('test directory has been created')\n",
    "    os.chdir('test')\n",
    "    if os.path.isdir('outputs'):\n",
    "        print('test outputs directory found - no need to recreate')\n",
    "    else:\n",
    "        os.mkdir('outputs')\n",
    "        print('test outputs directory has been created')\n",
    "    if os.path.isdir('logs'):\n",
    "        print('logs directory found - no need to recreate')\n",
    "    else:\n",
    "        os.mkdir('logs')\n",
    "        print('test logs directory has been created')\n",
    "else:\n",
    "    if os.path.isdir('outputs'):\n",
    "        print('outputs directory found - no need to recreate')\n",
    "    else:\n",
    "        os.mkdir('outputs')\n",
    "        print('outputs directory has been created')\n",
    "\n",
    "#setting timestamp to calculate run time\n",
    "start_time = datetime.now() \n",
    "##with timezone for log file\n",
    "start_timezone = datetime.now().astimezone()\n",
    "start_timezone_formatted = start_timezone.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
    "#creating variable with current date for appending to filenames\n",
    "today = datetime.now().strftime('%Y%m%d') \n",
    "\n",
    "#read in email address for polite requests (required for biopython NCBI workflow, can be used for other APIs)\n",
    "email = config['EMAIL']['user_email']\n",
    "\n",
    "#create permutation string with OR for API parameters\n",
    "ut_variations = config['PERMUTATIONS']\n",
    "institution_query = ' OR '.join([f'\"{variation}\"' for variation in ut_variations])\n",
    "##if you need a smaller set of previously identified permutations for an easier API call\n",
    "ut_variations_small = config['PERMUTATIONS_IDENTIFIED']\n",
    "institution_query_small = ' OR '.join([f'\"{variation}\"' for variation in ut_variations_small])\n",
    "\n",
    "#pull in ROR ID (necessary for Dryad API)\n",
    "ror_id = config['INSTITUTION']['ror']\n",
    "\n",
    "#pulling in 'uniqueIdentifer' term used as quick, reliable filter ('Austin' for filtering an affiliation field for UT Austin)\n",
    "uni_identifier = config['INSTITUTION']['uniqueIdentifier']\n",
    "\n",
    "#API endpoints\n",
    "url_crossref = 'https://api.crossref.org/works/'\n",
    "url_crossref_issn = 'https://api.crossref.org/journals/{issn}/works'\n",
    "url_dryad = f'https://datadryad.org/api/v2/search?affiliation={ror_id}' #Dryad requires ROR for affiliation search\n",
    "url_datacite = 'https://api.datacite.org/dois'\n",
    "url_dataverse = 'https://dataverse.tdl.org/api/search/'\n",
    "url_openalex = 'https://api.openalex.org/works?'\n",
    "url_zenodo = 'https://zenodo.org/api/records'\n",
    "\n",
    "##per page\n",
    "per_page_datacite = config['VARIABLES']['PAGE_SIZES']['datacite']\n",
    "per_page_dryad = config['VARIABLES']['PAGE_SIZES']['dryad']\n",
    "per_page_dataverse = config['VARIABLES']['PAGE_SIZES']['dataverse']\n",
    "per_page_zenodo = config['VARIABLES']['PAGE_SIZES']['zenodo']\n",
    "\n",
    "##page start\n",
    "page_start_dryad = config['VARIABLES']['PAGE_STARTS']['dryad']\n",
    "page_start_dataverse = config['VARIABLES']['PAGE_STARTS']['dataverse']\n",
    "page_start_datacite = config['VARIABLES']['PAGE_STARTS']['datacite']\n",
    "page_start_zenodo = config['VARIABLES']['PAGE_STARTS']['zenodo']\n",
    "\n",
    "#page count, differ based on 'test' vs. 'prod' env\n",
    "page_limit_datacite = config['VARIABLES']['PAGE_LIMITS']['datacite_test'] if test else config['VARIABLES']['PAGE_LIMITS']['datacite_prod']\n",
    "page_limit_zenodo = config['VARIABLES']['PAGE_LIMITS']['zenodo_test'] if test else config['VARIABLES']['PAGE_LIMITS']['zenodo_prod']\n",
    "page_limit_openalex = config['VARIABLES']['PAGE_LIMITS']['openalex_test'] if test else config['VARIABLES']['PAGE_LIMITS']['openalex_prod']\n",
    "\n",
    "\n",
    "params_dryad= {\n",
    "    'per_page': per_page_dryad,\n",
    "}\n",
    "\n",
    "if resource_type_filter:\n",
    "    params_datacite = {\n",
    "        'affiliation': 'true',\n",
    "        'query': f'(creators.affiliation.name:({institution_query}) OR creators.name:({institution_query}) OR contributors.affiliation.name:({institution_query}) OR contributors.name:({institution_query})) AND types.resourceTypeGeneral:{datacite_resource_type}',\n",
    "        'page[size]': per_page_datacite,\n",
    "        'page[cursor]': 1,\n",
    "    }\n",
    "else:\n",
    "    params_datacite = {\n",
    "        'affiliation': 'true',\n",
    "        'query': f'(creators.affiliation.name:({institution_query}) OR creators.name:({institution_query}) OR contributors.affiliation.name:({institution_query}) OR contributors.name:({institution_query}))',\n",
    "        'page[size]': per_page_datacite,\n",
    "        'page[cursor]': 1,\n",
    "    }\n",
    "\n",
    "headers_dataverse = {\n",
    "    'X-Dataverse-key': config['KEYS']['dataverseToken']\n",
    "}\n",
    "params_dataverse = {\n",
    "    'q': '10.18738/T8/',\n",
    "    #UT Austin dataverse, may contain non-UT affiliated objects, and UT-affiliated objects may be in other TDR installations\n",
    "    #'subtree': 'utexas', \n",
    "    'type': 'dataset', #dataverse may also mint DOIs for files\n",
    "    'start': page_start_dataverse,\n",
    "    'page': config['VARIABLES']['PAGE_INCREMENTS']['dataverse'],\n",
    "    'per_page': per_page_dataverse\n",
    "}\n",
    "\n",
    "params_zenodo = {\n",
    "    'q': f'(creators.affiliation:({institution_query_small}) OR creators.name:({institution_query_small}) OR contributors.affiliation:({institution_query_small}) OR contributors.name:({institution_query_small})) AND {zenodo_resource_type}',\n",
    "    'size': per_page_zenodo,\n",
    "    'access_token': config['KEYS']['zenodoToken']\n",
    "}\n",
    "\n",
    "#defining some metadata assessment objects\n",
    "##assess 'descriptiveness of dataset title'\n",
    "words = config['WORDS']\n",
    "###add integers\n",
    "numbers = list(map(str, range(1, 1000000)))\n",
    "###combine all into a single set\n",
    "nondescriptive_words = set(\n",
    "    words['articles'] +\n",
    "    words['conjunctions'] +\n",
    "    words['prepositions'] +\n",
    "    words['auxiliary_verbs'] +\n",
    "    words['possessives'] +\n",
    "    words['descriptors'] +\n",
    "    words['order'] +\n",
    "    words['version'] +\n",
    "    numbers\n",
    ")\n",
    "##software formats\n",
    "software_formats = set(config['SOFTWARE_FORMATS'].values())\n",
    "##convert mimeType to readable format\n",
    "format_map = config['FORMAT_MAP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ecffb",
   "metadata": {},
   "source": [
    "## Primary workflow\n",
    "### Step 1: Initial affiliation-based DataCite query\n",
    "\n",
    "This step queries the DataCite API for publications affiliated with the institution. The query is based on the various permutations of the institution's name as defined in the *config.json* file and searches for any one of those permutations across four DataCite metadata fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_previous_data and not load_previous_data_plus and not load_previous_data_plus_ncbi:\n",
    "    print('Starting DataCite retrieval based on affiliation.\\n')\n",
    "    data_datacite = retrieve_datacite(url_datacite, params_datacite, page_start_datacite, page_limit_datacite, per_page_datacite)\n",
    "    print(f'Number of datasets found by DataCite API: {len(data_datacite)}\\n')\n",
    "\n",
    "    if cross_validate:\n",
    "        print('Starting Dryad retrieval.\\n')\n",
    "        data_dryad = retrieve_dryad(url_dryad, params_dryad, page_start_dryad, per_page_dryad)\n",
    "        print(f'Number of Dryad datasets found by Dryad API: {len(data_dryad)}\\n')\n",
    "        if dataverse:\n",
    "            print('Starting Dataverse retrieval.\\n')\n",
    "            data_dataverse = retrieve_dataverse(url_dataverse, params_dataverse, headers_dataverse, page_start_dataverse, per_page_dataverse)\n",
    "            print(f'Number of Dataverse datasets found by Dataverse API: {len(data_dataverse)}\\n')\n",
    "        print('Starting Zenodo retrieval.\\n')\n",
    "        data_zenodo = retrieve_zenodo(url_zenodo, params_zenodo, page_start_zenodo, page_limit_zenodo, per_page_zenodo)\n",
    "        print(f'Number of Zenodo datasets found by Zenodo API: {len(data_zenodo)}\\n')\n",
    "\n",
    "    print('Beginning dataframe generation.\\n')\n",
    "\n",
    "    data_select_datacite = [] \n",
    "    for item in data_datacite:\n",
    "        attributes = item.get('attributes', {})\n",
    "        doi = attributes.get('doi', None)\n",
    "        state = attributes.get('state', None)\n",
    "        publisher = attributes.get('publisher', '')\n",
    "        registered = attributes.get('registered', '')\n",
    "        if registered:\n",
    "            publisher_year = datetime.fromisoformat(registered.rstrip('Z')).year\n",
    "            publisher_date = datetime.fromisoformat(registered.rstrip('Z')).date()\n",
    "        else:\n",
    "            publisher_year = None\n",
    "            publisher_date = None\n",
    "        title = attributes.get('titles', [{}])[0].get('title', '')\n",
    "        creators = attributes.get('creators', [{}])\n",
    "        creators_names = [creator.get('name', '') for creator in creators]\n",
    "        creators_formatted = []\n",
    "        for creator in creators:\n",
    "            name = creator.get('name', '').strip()\n",
    "            affiliations = creator.get('affiliation', [])\n",
    "            updated_affiliations = []\n",
    "            for affil in affiliations:\n",
    "                affil_name = affil.get('name', '') if isinstance(affil, dict) else affil\n",
    "                if 'Austin' in affil_name:\n",
    "                    affil_name = 'University of Texas at Austin'\n",
    "                updated_affiliations.append(affil_name)\n",
    "            affil_str = ', '.join(updated_affiliations) if updated_affiliations else 'No affiliation listed'\n",
    "            creators_formatted.append(f'{name} ({affil_str})')\n",
    "        first_creator = creators[0].get('name', None) if creators else None\n",
    "        last_creator = creators[-1].get('name', None) if creators else None\n",
    "        creators_affiliations = [\n",
    "            aff.get('name', '')\n",
    "            for creator in creators\n",
    "            for aff in (creator.get('affiliation') if isinstance(creator.get('affiliation'), list) else [])\n",
    "            if isinstance(aff, dict)\n",
    "        ]\n",
    "        first_affiliation = creators_affiliations[0] if creators_affiliations else None\n",
    "        last_affiliation = creators_affiliations[-1] if creators_affiliations else None\n",
    "        contributors = attributes.get('contributors', [{}])\n",
    "        contributors_names = [contributor.get('name', '') for contributor in contributors]\n",
    "        contributors_affiliations = [\n",
    "            aff.get('name', '')\n",
    "            for contributor in contributors\n",
    "            for aff in (contributor.get('affiliation') if isinstance(contributor.get('affiliation'), list) else [])\n",
    "            if isinstance(aff, dict)\n",
    "        ]\n",
    "        contributors_formatted = []\n",
    "        for contributor in contributors:\n",
    "            name = contributor.get('name', '').strip()\n",
    "            affiliations = contributor.get('affiliation', [])\n",
    "            updated_affiliations = []\n",
    "            for affil in affiliations:\n",
    "                affil_name = affil.get('name', '') if isinstance(affil, dict) else affil\n",
    "                if 'Austin' in affil_name:\n",
    "                    affil_name = 'University of Texas at Austin'\n",
    "                updated_affiliations.append(affil_name)\n",
    "            affil_str = ', '.join(updated_affiliations) if updated_affiliations else 'No affiliation listed'\n",
    "            contributors_formatted.append(f'{name} ({affil_str})')\n",
    "        container = attributes.get('container', {})\n",
    "        container_identifier = container.get('identifier', None)\n",
    "        related_identifiers = attributes.get('relatedIdentifiers', [])\n",
    "        for identifier in related_identifiers:\n",
    "            relation_type = identifier.get('relationType', '')\n",
    "            related_identifier = identifier.get('relatedIdentifier', '')\n",
    "        types = attributes.get('types', {})\n",
    "        resource_type = types.get('resourceTypeGeneral', '')\n",
    "        subjects = attributes.get('subjects', [])\n",
    "        if subjects:\n",
    "            subject_list = [subj.get('subject', '').strip() for subj in subjects if subj.get('subject')]\n",
    "            subjects_combined = '; '.join(subject_list) if subject_list else 'No keywords provided'\n",
    "        else:\n",
    "            subjects_combined = 'No keywords provided'\n",
    "        sizes = attributes.get('sizes', [])\n",
    "        cleaned_sizes = [int(re.sub(r'\\D', '', size)) for size in sizes if re.sub(r'\\D', '', size).isdigit()]\n",
    "        total_size = sum(cleaned_sizes) if cleaned_sizes else 'No file size information'   \n",
    "        formats_list = attributes.get('formats', [])\n",
    "        formats = set(formats_list) if formats_list else 'No file information' \n",
    "        file_count = len(formats_list) if formats_list else 'No file information'   \n",
    "        rights_list = attributes.get('rightsList', [])\n",
    "        rights = [right['rights'] for right in rights_list if 'rights' in right] or ['Rights unspecified']\n",
    "        rights_code = [right['rightsIdentifier'] for right in rights_list if 'rightsIdentifier' in right] or ['Unknown']\n",
    "        views = attributes.get('viewCount', 0)\n",
    "        downloads = attributes.get('downloadCount', 0)\n",
    "        citations = attributes.get('citationCount', 0)\n",
    "        data_select_datacite.append({\n",
    "            'doi': doi,\n",
    "            'state': state,\n",
    "            'publisher': publisher,\n",
    "            'publisher_original': publisher,\n",
    "            'publication_year': publisher_year,\n",
    "            'publication_date': publisher_date,\n",
    "            'title': title,\n",
    "            'first_author': first_creator,\n",
    "            'last_author': last_creator,\n",
    "            'first_affiliation': first_affiliation,\n",
    "            'last_affiliation': last_affiliation,\n",
    "            'creators_names': creators_names,\n",
    "            'creators_affiliations': creators_affiliations,\n",
    "            'creators_formatted': creators_formatted,\n",
    "            'contributors_names': contributors_names,\n",
    "            'contributors_affiliations': contributors_affiliations,\n",
    "            'contributors_formatted': contributors_formatted,\n",
    "            'relation_type': relation_type,\n",
    "            'related_identifier': related_identifier,\n",
    "            'container_identifier': container_identifier,\n",
    "            'type': resource_type,\n",
    "            'subjects': subjects_combined,\n",
    "            'deposit_size': total_size,\n",
    "            'formats': formats,\n",
    "            'file_count': file_count,\n",
    "            'rights': rights,\n",
    "            'rights_code': rights_code,\n",
    "            'views': views,\n",
    "            'downloads': downloads,\n",
    "            'citations': citations,\n",
    "            'source': 'DataCite'\n",
    "        })\n",
    "\n",
    "    df_datacite_initial = pd.json_normalize(data_select_datacite)\n",
    "    df_datacite_initial.to_csv(f'outputs/{today}_{resource_filename}_datacite-initial-output.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    if cross_validate:\n",
    "        #first processing DataCite outputs\n",
    "        #split out DataCite results for repos to be cross-validated against\n",
    "        ##coercing all DOIs with 'zenodo' to have publisher of 'Zenodo'\n",
    "        df_datacite_initial.loc[df_datacite_initial['doi'].str.contains('zenodo', case=False, na=False), 'publisher'] = 'Zenodo'\n",
    "        ##using str.contains to account for any potential name inconsistency for one repository\n",
    "        df_datacite_dryad = df_datacite_initial[df_datacite_initial['publisher'].str.contains('Dryad')]\n",
    "        df_datacite_dataverse = df_datacite_initial[df_datacite_initial['publisher'].str.contains('Texas Data Repository')]\n",
    "        df_datacite_zenodo = df_datacite_initial[df_datacite_initial['publisher'].str.contains('Zenodo')]\n",
    "        df_remainder = df_datacite_initial[df_datacite_initial['publisher'].str.contains('Dryad|Texas Data Repository|Zenodo') == False]\n",
    "\n",
    "        print(f'Number of Dryad datasets found by DataCite API: {len(df_datacite_dryad)}\\n')\n",
    "        print(f'Number of Dataverse datasets found by DataCite API: {len(df_datacite_dataverse)}\\n')\n",
    "        print(f'Number of Zenodo datasets found by DataCite API: {len(df_datacite_zenodo)}\\n')\n",
    "\n",
    "        df_datacite_dryad_pruned = df_datacite_dryad[['publisher', 'doi', 'publication_year', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'type']]\n",
    "        df_datacite_dataverse_pruned = df_datacite_dataverse[['publisher', 'doi', 'publication_year', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'type']] \n",
    "        df_datacite_zenodo_pruned = df_datacite_zenodo[['publisher', 'doi', 'publication_year', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation','type']] \n",
    "        df_datacite_remainder_pruned = df_remainder[['publisher', 'doi', 'publication_year', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation','type']] \n",
    "\n",
    "        #create new lists for recursive modification\n",
    "        datacite_dataframes_pruned = [df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned, df_datacite_remainder_pruned]\n",
    "        datacite_dataframes_specific_pruned = [df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned]\n",
    "\n",
    "        #standardizing how Texas Data Repository is displayed\n",
    "        df_datacite_dataverse_pruned['publisher'] = df_datacite_dataverse_pruned['publisher'].str.replace('Texas Data Repository Dataverse','Texas Data Repository')\n",
    "\n",
    "        for df in datacite_dataframes_specific_pruned:\n",
    "            df['doi'] = df['doi'].str.lower()\n",
    "\n",
    "        columns_to_rename = {\n",
    "            'publisher': 'repository'\n",
    "        }\n",
    "        for i in range(len(datacite_dataframes_pruned)):\n",
    "            datacite_dataframes_pruned[i] = datacite_dataframes_pruned[i].rename(columns=columns_to_rename)\n",
    "            datacite_dataframes_pruned[i]['source'] = 'DataCite'\n",
    "        #assign modified dfs back to original\n",
    "        df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned, df_datacite_remainder_pruned = datacite_dataframes_pruned\n",
    "\n",
    "        #reload list\n",
    "        datacite_dataframes_specific_pruned = [df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned]\n",
    "        for i in range(len(datacite_dataframes_specific_pruned)):\n",
    "            datacite_dataframes_specific_pruned[i] = datacite_dataframes_specific_pruned[i].rename(columns={c: c+'_dc' for c in datacite_dataframes_specific_pruned[i].columns if c not in ['doi']})\n",
    "        #assign modified dfs back to original\n",
    "        df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned = datacite_dataframes_specific_pruned\n",
    "\n",
    "        #initialize dfs for APIs\n",
    "        df_dryad_undetected = pd.DataFrame()\n",
    "        df_dataverse_undetected = pd.DataFrame()\n",
    "        df_zenodo_undetected = pd.DataFrame()\n",
    "\n",
    "        print('Dryad step\\n')\n",
    "        if data_dryad:\n",
    "            data_select_dryad = [] \n",
    "            for item in data_dryad:\n",
    "                links = item.get('_links', {})\n",
    "                doi_dr = item.get('identifier', None)\n",
    "                pubDate_dr = item.get('publicationDate', '')\n",
    "                title_dr=item.get('title', [{}])\n",
    "                authors_dr = item.get('authors', [{}])\n",
    "                first_author_first = authors_dr[0].get('firstName', None)\n",
    "                first_author_last = authors_dr[0].get('lastName', None)\n",
    "                last_author_first = authors_dr[-1].get('firstName', None)\n",
    "                last_author_last = authors_dr[-1].get('lastName', None)\n",
    "                first_affiliation_dr = authors_dr[0].get('affiliation', None)\n",
    "                last_affiliation_dr = authors_dr[-1].get('affiliation', None)\n",
    "                related_works_list_dr = [rel.get('identifier', None) for rel in item.get('relatedWorks', [{}])]\n",
    "                related_works_list_dr = related_works_list_dr if related_works_list_dr else None\n",
    "                author_last_order_dr = authors_dr[-1].get('order', None)\n",
    "                data_select_dryad.append({\n",
    "                    'doi': doi_dr,\n",
    "                    'publication_date': pubDate_dr,\n",
    "                    'title': title_dr,\n",
    "                    'first_author_first': first_author_first,\n",
    "                    'last_author_first': last_author_first,\n",
    "                    'first_author_last': first_author_last,\n",
    "                    'last_author_last': last_author_last,\n",
    "                    'first_affiliation': first_affiliation_dr,\n",
    "                    'last_affiliation': last_affiliation_dr,\n",
    "                    'related_works': related_works_list_dr\n",
    "                })\n",
    "            df_dryad = pd.json_normalize(data_select_dryad)\n",
    "            df_dryad.to_csv(f'outputs/{today}_Dryad-API-output.csv', index=False, encoding='utf-8')\n",
    "            #formatting author names to be consistent with others\n",
    "            df_dryad['first_author'] = df_dryad['first_author_last'] + ', ' + df_dryad['first_author_first']\n",
    "            df_dryad['last_author'] = df_dryad['last_author_last'] + ', ' + df_dryad['last_author_first']\n",
    "            df_dryad = df_dryad.drop(columns=['first_author_first', 'first_author_last', 'last_author_first', 'last_author_last'])\n",
    "            df_dryad['publication_year'] = pd.to_datetime(df_dryad['publication_date']).dt.year\n",
    "            df_dryad_pruned = df_dryad[['doi', 'publication_year', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation']]\n",
    "\n",
    "            #editing DOI columns to ensure exact matches\n",
    "            df_dryad_pruned['doi'] = df_dryad_pruned['doi'].str.replace('doi:', '')\n",
    "            df_dryad_pruned['doi'] = df_dryad_pruned['doi'].str.lower()\n",
    "            #adding suffix to column headers to differentiate identically named columns when merged (vs. automatic .x and .y)\n",
    "            df_dryad_pruned = df_dryad_pruned.rename(columns={c: c+'_dryad' for c in df_dryad_pruned.columns if c not in ['doi']})\n",
    "            df_dryad_pruned['source_dryad'] = 'Dryad'        \n",
    "\n",
    "            #DataCite into Dryad\n",
    "            df_dryad_datacite_joint = pd.merge(df_dryad_pruned, df_datacite_dryad_pruned, on='doi', how='left')\n",
    "            df_dryad_datacite_joint['Match_entry'] = np.where(df_dryad_datacite_joint['source_dc'].isnull(), 'Not matched', 'Matched')\n",
    "            print('Counts of matches for DataCite into Dryad')\n",
    "            counts_dryad_datacite = df_dryad_datacite_joint['Match_entry'].value_counts()\n",
    "            print(counts_dryad_datacite, '\\n')\n",
    "            df_dryad_datacite_joint_unmatched = df_dryad_datacite_joint[df_dryad_datacite_joint['Match_entry'] == 'Not matched']\n",
    "            df_dryad_datacite_joint_unmatched.to_csv(f'outputs/{today}_DataCite-into-Dryad_joint-unmatched-dataframe.csv', index=False, encoding='utf-8')\n",
    "            df_dryad_undetected = df_dryad_datacite_joint_unmatched[['doi']]\n",
    "\n",
    "            #Dryad into DataCite\n",
    "            df_datacite_dryad_joint = pd.merge(df_datacite_dryad_pruned, df_dryad_pruned, on='doi', how='left')\n",
    "            df_datacite_dryad_joint['Match_entry'] = np.where(df_datacite_dryad_joint['source_dryad'].isnull(), 'Not matched', 'Matched')\n",
    "            print('Counts of matches for Dryad into DataCite')\n",
    "            counts_datacite_dryad = df_datacite_dryad_joint['Match_entry'].value_counts()\n",
    "            print(counts_datacite_dryad, '\\n')\n",
    "            df_datacite_dryad_joint_unmatched = df_datacite_dryad_joint[df_datacite_dryad_joint['Match_entry'] == 'Not matched']\n",
    "            df_datacite_dryad_joint_unmatched.to_csv(f'outputs/{today}_Dryad-into-DataCite_joint-unmatched-dataframe.csv', index=False, encoding='utf-8')\n",
    "\n",
    "        if dataverse:\n",
    "            if data_dataverse:\n",
    "                print('Dataverse step\\n')\n",
    "                data_select_dataverse = [] \n",
    "                for item in data_dataverse:\n",
    "                    globalID = item.get('global_id', '')\n",
    "                    versionState = item.get('versionState', None)\n",
    "                    pubDate_dataverse = item.get('published_at', '')\n",
    "                    title_dataverse = item.get('name', None)\n",
    "                    authors_dataverse = item.get('authors', [{}])\n",
    "                    contacts_dataverse = item.get('contacts', [{}])\n",
    "                    first_contact_dataverse = contacts_dataverse[0].get('name', None)\n",
    "                    first_affiliation_contact = contacts_dataverse[0].get('affiliation', None)\n",
    "                    last_contact_dataverse = contacts_dataverse[-1].get('name', None)\n",
    "                    last_affiliation_contact = contacts_dataverse[-1].get('affiliation', None)\n",
    "                    type = item.get('type', None)\n",
    "                    dataverse = item.get('name_of_dataverse', None)\n",
    "                    data_select_dataverse.append({\n",
    "                        'doi': globalID,\n",
    "                        'status': versionState,\n",
    "                        'publication_date': pubDate_dataverse,\n",
    "                        'title': title_dataverse,\n",
    "                        'authors': authors_dataverse,\n",
    "                        'contacts': contacts_dataverse,\n",
    "                        'first_contact': first_contact_dataverse,\n",
    "                        'first_contact_affiliation': first_affiliation_contact,\n",
    "                        'last_contact': last_contact_dataverse,\n",
    "                        'last_contact_affiliation': last_affiliation_contact,\n",
    "                        'type': type,\n",
    "                        'dataverse': dataverse\n",
    "                    })\n",
    "                df_dataverse = pd.json_normalize(data_select_dataverse)\n",
    "                df_dataverse.to_csv(f'outputs/{today}_TDR-API-output.csv', index=False, encoding='utf-8')\n",
    "\n",
    "                #subsetting for published datasets\n",
    "                df_dataverse_pub = df_dataverse[df_dataverse['status'].str.contains('RELEASED') == True]\n",
    "                df_dataverse_pub['doi'] = df_dataverse_pub['doi'].str.lower()\n",
    "                #looking for UT Austin in any of four fields\n",
    "                pattern = '|'.join([f'({perm})' for perm in ut_variations])\n",
    "                df_dataverse_pub['authors'] = df_dataverse_pub['authors'].apply(lambda x: str(x))\n",
    "                df_dataverse_pub['contacts'] = df_dataverse_pub['contacts'].apply(lambda x: str(x))\n",
    "                df_dataverse_pub_filtered = df_dataverse_pub[df_dataverse_pub['authors'].str.contains(pattern, case=False, na=False) | df_dataverse_pub['contacts'].str.contains(pattern, case=False, na=False)]\n",
    "                print(f'Number of published Dataverse datasets found by Dataverse API: {len(df_dataverse_pub)}\\n')\n",
    "                df_dataverse_pub_filtered['publication_year'] = pd.to_datetime(df_dataverse_pub_filtered['publication_date'], format='ISO8601').dt.year\n",
    "                df_dataverse_pruned = df_dataverse_pub_filtered[['doi', 'publication_year', 'title', 'first_contact', 'first_contact_affiliation', 'last_contact', 'last_contact_affiliation']]\n",
    "\n",
    "                df_dataverse_pruned['doi'] = df_dataverse_pruned['doi'].str.replace('doi:', '')\n",
    "                df_dataverse_pruned = df_dataverse_pruned.rename(columns={c: c+'_dataverse' for c in df_dataverse_pruned.columns if c not in ['doi']})\n",
    "                df_dataverse_pruned['source_dataverse'] = 'Texas Data Repository'\n",
    "                \n",
    "                #DataCite into Dataverse (using non-de-duplicated DataCite data)\n",
    "                df_dataverse_datacite_joint = pd.merge(df_dataverse_pruned, df_datacite_dataverse_pruned, on='doi', how='left')\n",
    "                df_dataverse_datacite_joint['Match_entry'] = np.where(df_dataverse_datacite_joint['source_dc'].isnull(), 'Not matched', 'Matched')\n",
    "                print('Counts of matches for DataCite into Dataverse')\n",
    "                counts_dataverse_datacite = df_dataverse_datacite_joint['Match_entry'].value_counts()\n",
    "                print(counts_dataverse_datacite, '\\n')\n",
    "                df_dataverse_datacite_joint_unmatched = df_dataverse_datacite_joint[df_dataverse_datacite_joint['Match_entry'] == 'Not matched']\n",
    "                df_dataverse_datacite_joint_unmatched.to_csv(f'outputs/{today}_DataCite-into-Dataverse_joint-unmatched-dataframe.csv', index=False, encoding='utf-8')\n",
    "                df_dataverse_undetected = df_dataverse_datacite_joint_unmatched[['doi']]\n",
    "\n",
    "                #Dataverse into DataCite (using de-duplicated DataCite data)\n",
    "                df_datacite_dataverse_joint = pd.merge(df_datacite_dataverse_pruned, df_dataverse_pruned, on='doi', how='left')\n",
    "                df_datacite_dataverse_joint['Match_entry'] = np.where(df_datacite_dataverse_joint['source_dataverse'].isnull(), 'Not matched', 'Matched')\n",
    "                print('Counts of matches for Dataverse into DataCite')\n",
    "                counts_datacite_dataverse = df_datacite_dataverse_joint['Match_entry'].value_counts()\n",
    "                print(counts_datacite_dataverse, '\\n')\n",
    "                df_datacite_dataverse_joint_unmatched = df_datacite_dataverse_joint[df_datacite_dataverse_joint['Match_entry'] == 'Not matched']\n",
    "                df_datacite_dataverse_joint_unmatched.to_csv(f'outputs/{today}_Dataverse-into-DataCite_joint-unmatched-dataframe.csv', index=False, encoding='utf-8')\n",
    "\n",
    "        print('Zenodo step\\n')\n",
    "        if data_zenodo:\n",
    "            data_select_zenodo = [] \n",
    "            for item in data_zenodo:\n",
    "                metadata = item.get('metadata', {})\n",
    "                doi = item.get('doi', None)\n",
    "                parentDOI = item.get('conceptdoi', None)\n",
    "                conceptID = item.get('conceptrecid', None)\n",
    "                pubDate_zen = metadata.get('publication_date', '')\n",
    "                title_zen = metadata.get('title', '')\n",
    "                description_zen = metadata.get('description', None)\n",
    "                creators_zen = metadata.get('creators', [{}])\n",
    "                first_creator_zen = creators_zen[0].get('name', None)\n",
    "                last_creator_zen = creators_zen[-1].get('name', None)\n",
    "                first_affiliation_zen = creators_zen[0].get('affiliation', None)\n",
    "                last_affiliation_zen = creators_zen[-1].get('affiliation', None)\n",
    "                related_works_list_zen = [name.get('identifier', None) for name in metadata.get('relatedWorks', [{}])]\n",
    "                related_works_list_zen = related_works_list_zen if related_works_list_zen else None\n",
    "                related_works_type_list_zen = [name.get('relation', None) for name in metadata.get('relatedWorks', [{}])]\n",
    "                related_works_type_list_zen = related_works_type_list_zen if related_works_type_list_zen else None\n",
    "                data_select_zenodo.append({\n",
    "                    'doi': parentDOI, #want parent to avoid de-duplication issues later\n",
    "                    'publication_date': pubDate_zen,\n",
    "                    'title': title_zen,\n",
    "                    'description': description_zen,\n",
    "                    'first_author': first_creator_zen,\n",
    "                    'last_author': last_creator_zen,\n",
    "                    'first_affiliation': first_affiliation_zen,\n",
    "                    'last_affiliation': last_affiliation_zen,\n",
    "                    'related_works': related_works_list_zen,\n",
    "                    'related_works_type': related_works_type_list_zen\n",
    "                })\n",
    "            df_data_zenodo = pd.json_normalize(data_select_zenodo)\n",
    "            df_data_zenodo.to_csv(f'outputs/{today}_Zenodo-API-output.csv', index=False, encoding='utf-8')\n",
    "            #removing non-Zenodo deposits indexed by Zenodo (mostly Dryad) from Zenodo output\n",
    "            ##Zenodo has indexed many Dryad deposits <50 GB in size (does not issue a new DOI but does return a Zenodo 'record' in the API)\n",
    "            df_data_zenodo['doi'] = df_data_zenodo['doi'].str.lower()\n",
    "            df_data_zenodo_true = df_data_zenodo[df_data_zenodo['doi'].str.contains('zenodo') == True] \n",
    "            #for some reason, Zenodo's API sometimes returns identical entries of most datasets...\n",
    "            df_data_zenodo_real = df_data_zenodo_true.drop_duplicates(subset=['publication_date', 'doi'], keep='first') \n",
    "            print(f'Number of non-Dryad Zenodo datasets found by Zenodo API: {len(df_data_zenodo_real)}\\n')\n",
    "            df_data_zenodo_real['publication_year'] = pd.to_datetime(df_data_zenodo_real['publication_date'], format='mixed').dt.year\n",
    "            df_zenodo_pruned = df_data_zenodo_real[['doi','publication_year', 'title','first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'publication_date', 'description']]\n",
    "            df_zenodo_pruned = df_zenodo_pruned.rename(columns={c: c+'_zen' for c in df_data_zenodo_real.columns if c not in ['doi']})\n",
    "            df_zenodo_pruned['source_zenodo'] = 'Zenodo'\n",
    "\n",
    "            #DataCite into Zenodo\n",
    "            df_zenodo_datacite_joint = pd.merge(df_zenodo_pruned, df_datacite_zenodo_pruned, on='doi', how='left')\n",
    "            df_zenodo_datacite_joint['Match_entry'] = np.where(df_zenodo_datacite_joint['source_dc'].isnull(), 'Not matched', 'Matched')\n",
    "            ##removing multiple DOIs in same 'lineage'\n",
    "            df_zenodo_datacite_joint = df_zenodo_datacite_joint.sort_values(by=['doi'])\n",
    "            df_zenodo_datacite_joint_deduplicated = df_zenodo_datacite_joint.drop_duplicates(subset=['publicationDate_zen', 'description_zen'], keep='last') \n",
    "            ##one problematic dataset splits incorrectly when exported to CSV (conceptrecID = 616927)\n",
    "            print('Counts of matches for DataCite into Zenodo\\n')\n",
    "            counts_zenodo_datacite = df_zenodo_datacite_joint_deduplicated['Match_entry'].value_counts()\n",
    "            print(counts_zenodo_datacite, '\\n')\n",
    "            df_zenodo_datacite_joint_unmatched = df_zenodo_datacite_joint_deduplicated[df_zenodo_datacite_joint_deduplicated['Match_entry'] == 'Not matched']\n",
    "            df_zenodo_datacite_joint_unmatched.to_excel(f'outputs/{today}_DataCite-into-Zenodo-unmatched-dataframe.xlsx', index=False)\n",
    "            df_zenodo_undetected = df_zenodo_datacite_joint_unmatched[['doi']]\n",
    "\n",
    "            #Zenodo into DataCite\n",
    "            df_datacite_zenodo_joint = pd.merge(df_datacite_zenodo_pruned, df_zenodo_pruned, on='doi', how='left')\n",
    "            df_datacite_zenodo_joint['Match_entry'] = np.where(df_datacite_zenodo_joint['source_zenodo'].isnull(), 'Not matched', 'Matched')\n",
    "            ##removing multiple DOIs in same 'lineage'\n",
    "            df_datacite_zenodo_joint = df_datacite_zenodo_joint.sort_values(by=['doi']) \n",
    "            df_datacite_zenodo_joint_deduplicated = df_datacite_zenodo_joint.drop_duplicates(subset=['publicationDate_zen', 'description_zen'], keep='first') \n",
    "            print('Counts of matches for Zenodo into DataCite\\n')\n",
    "            counts_datacite_zenodo = df_datacite_zenodo_joint_deduplicated['Match_entry'].value_counts()\n",
    "            print(counts_datacite_zenodo, '\\n')\n",
    "            df_datacite_zenodo_joint_unmatched = df_datacite_zenodo_joint_deduplicated[df_datacite_zenodo_joint_deduplicated['Match_entry'] == 'Not matched']\n",
    "            df_datacite_zenodo_joint_unmatched.to_excel(f'outputs/{today}_Zenodo-into-DataCite_joint-unmatched-dataframe.xlsx', index=False)\n",
    "\n",
    "        #get DataCite metadata for all entries not previously detected by DataCite API query\n",
    "        dfs_to_concat = []\n",
    "        if not df_dryad_undetected.empty:\n",
    "            dfs_to_concat.append(df_dryad_undetected)\n",
    "            print(f'Adding missing {len(df_dryad_undetected)} Dryad DOIs.\\n')\n",
    "        if dataverse and not df_dataverse_undetected.empty:\n",
    "            dfs_to_concat.append(df_dataverse_undetected)\n",
    "            print(f'Adding missing {len(df_dataverse_undetected)} Dataverse DOIs.\\n')\n",
    "        if not df_zenodo_undetected.empty:\n",
    "            dfs_to_concat.append(df_zenodo_undetected)\n",
    "            print(f'Adding missing {len(df_zenodo_undetected)} Zenodo DOIs.\\n')\n",
    "        if dfs_to_concat:\n",
    "            datacite_new = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "        else:\n",
    "            print('No repository DataFrames available to concatenate.\\n')\n",
    "\n",
    "        print('Retrieving additional DataCite metadata for unmatched deposits\\n')\n",
    "        results = []\n",
    "        if test:\n",
    "            datacite_new = datacite_new.head(10)\n",
    "        for doi in datacite_new['doi']:\n",
    "            try:\n",
    "                response = requests.get(f'{url_datacite}/{doi}')\n",
    "                if response.status_code == 200:\n",
    "                    print(f'Retrieving {doi}\\n')\n",
    "                    results.append(response.json())\n",
    "                else:\n",
    "                    print(f'Error retrieving {doi}: {response.status_code}, {response.text}')\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f'Timeout error on DOI {doi}: {e}')\n",
    "\n",
    "        data_datacite_new = {\n",
    "            'datasets': results\n",
    "        }\n",
    "\n",
    "        data_select_datacite_new = []\n",
    "        datasets = data_datacite_new.get('datasets', []) \n",
    "        for item in datasets:\n",
    "            data = item.get('data', {})\n",
    "            attributes = item.get('attributes', {})\n",
    "            doi = attributes.get('doi', None)\n",
    "            state = attributes.get('state', None)\n",
    "            publisher = attributes.get('publisher', '')\n",
    "            registered = attributes.get('registered', '')\n",
    "            if registered:\n",
    "                publisher_year = datetime.fromisoformat(registered.rstrip('Z')).year\n",
    "                publisher_date = datetime.fromisoformat(registered.rstrip('Z')).date()\n",
    "            else:\n",
    "                publisher_year = None\n",
    "                publisher_date = None\n",
    "            title = attributes.get('titles', [{}])[0].get('title', '')\n",
    "            creators = attributes.get('creators', [{}])\n",
    "            creators_names = [creator.get('name', '') for creator in creators]\n",
    "            contributors_affiliations = [\n",
    "                '; '.join(aff.get('name', '') for aff in creator.get('affiliation', []))\n",
    "                for creator in creators\n",
    "            ]\n",
    "            creators_formatted = []\n",
    "            for creator in creators:\n",
    "                name = creator.get('name', '').strip()\n",
    "                affiliations = creator.get('affiliation', [])\n",
    "                updated_affiliations = []\n",
    "                for affil in affiliations:\n",
    "                    affil_name = affil.get('name', '') if isinstance(affil, dict) else affil\n",
    "                    if 'Austin' in affil_name:\n",
    "                        affil_name = 'University of Texas at Austin'\n",
    "                    updated_affiliations.append(affil_name)\n",
    "                affil_str = ', '.join(updated_affiliations) if updated_affiliations else 'No affiliation listed'\n",
    "                creators_formatted.append(f'{name} ({affil_str})')\n",
    "            first_creator = creators[0].get('name', None) if creators else None\n",
    "            last_creator = creators[-1].get('name', None) if creators else None\n",
    "            creators_affiliations = [\n",
    "                aff.get('name', '')\n",
    "                for creator in creators\n",
    "                for aff in (creator.get('affiliation') if isinstance(creator.get('affiliation'), list) else [])\n",
    "                if isinstance(aff, dict)\n",
    "            ]\n",
    "            first_affiliation = contributors_affiliations[0] if contributors_affiliations else None\n",
    "            last_affiliation = contributors_affiliations[-1] if contributors_affiliations else None\n",
    "            contributors = attributes.get('contributors', [{}])\n",
    "            contributors_names = [contributor.get('name', '') for contributor in contributors]\n",
    "            contributors_affiliations = [\n",
    "                '; '.join(aff.get('name', '') for aff in contributor.get('affiliation', []))\n",
    "                for contributor in contributors\n",
    "            ]\n",
    "            contributors_formatted = []\n",
    "            for contributor in contributors:\n",
    "                name = contributor.get('name', '').strip()\n",
    "                affiliations = contributor.get('affiliation', [])\n",
    "                updated_affiliations = []\n",
    "                for affil in affiliations:\n",
    "                    affil_name = affil.get('name', '') if isinstance(affil, dict) else affil\n",
    "                    if 'Austin' in affil_name:\n",
    "                        affil_name = 'University of Texas at Austin'\n",
    "                    updated_affiliations.append(affil_name)\n",
    "                affil_str = ', '.join(updated_affiliations) if updated_affiliations else 'No affiliation listed'\n",
    "                contributors_formatted.append(f'{name} ({affil_str})')\n",
    "            container = attributes.get('container', {})\n",
    "            container_identifier = container.get('identifier', None)\n",
    "            related_identifiers = attributes.get('relatedIdentifiers', [])\n",
    "            for identifier in related_identifiers:\n",
    "                relation_type = identifier.get('relationType', '')\n",
    "                related_identifier = identifier.get('relatedIdentifier', '')\n",
    "            types = attributes.get('types', {})\n",
    "            resource_type = types.get('resourceTypeGeneral', '')\n",
    "            subjects = attributes.get('subjects', [])\n",
    "            if subjects:\n",
    "                subject_list = [subj.get('subject', '').strip() for subj in subjects if subj.get('subject')]\n",
    "                subjects_combined = '; '.join(subject_list) if subject_list else 'No keywords provided'\n",
    "            else:\n",
    "                subjects_combined = 'No keywords provided'\n",
    "            sizes = attributes.get('sizes', [])\n",
    "            cleaned_sizes = [int(re.sub(r'\\D', '', size)) for size in sizes if re.sub(r'\\D', '', size).isdigit()]\n",
    "            total_size = sum(cleaned_sizes) if cleaned_sizes else 'No file size information'   \n",
    "            formats_list = attributes.get('formats', [])\n",
    "            formats = set(formats_list) if formats_list else 'No file information' \n",
    "            file_count = len(formats_list) if formats_list else 'No file information'   \n",
    "            rights_list = attributes.get('rightsList', [])\n",
    "            rights = [right['rights'] for right in rights_list if 'rights' in right] or ['Rights unspecified']\n",
    "            rights_code = [right['rightsIdentifier'] for right in rights_list if 'rightsIdentifier' in right] or ['Unknown']\n",
    "            views = attributes.get('viewCount', 0)\n",
    "            downloads = attributes.get('downloadCount', 0)\n",
    "            citations = attributes.get('citationCount', 0)\n",
    "            data_select_datacite.append({\n",
    "                'doi': doi,\n",
    "                'state': state,\n",
    "                'publisher': publisher,\n",
    "                'publisher_original': publisher,\n",
    "                'publication_year': publisher_year,\n",
    "                'publication_date': publisher_date,\n",
    "                'title': title,\n",
    "                'first_author': first_creator,\n",
    "                'last_author': last_creator,\n",
    "                'first_affiliation': first_affiliation,\n",
    "                'last_affiliation': last_affiliation,\n",
    "                'creators_names': creators_names,\n",
    "                'creators_affiliations': creators_affiliations,\n",
    "                'creators_formatted': creators_formatted,\n",
    "                'contributors_names': contributors_names,\n",
    "                'contributors_affiliations': contributors_affiliations,\n",
    "                'contributors_formatted': contributors_formatted,\n",
    "                'relation_type': relation_type,\n",
    "                'related_identifier': related_identifier,\n",
    "                'container_identifier': container_identifier,\n",
    "                'type': resource_type,\n",
    "                'subjects': subjects_combined,\n",
    "                'deposit_size': total_size,\n",
    "                'formats': formats,\n",
    "                'file_count': file_count,\n",
    "                'rights': rights,\n",
    "                'rights_code': rights_code,\n",
    "                'views': views,\n",
    "                'downloads': downloads,\n",
    "                'citations': citations,\n",
    "                'source': 'repository cross-validation'\n",
    "            })\n",
    "\n",
    "        df_datacite_new = pd.json_normalize(data_select_datacite_new)\n",
    "        df_datacite_new.to_csv(f'outputs/{today}_datacite-additional-cross-validation.csv', index=False, encoding='utf-8')\n",
    "    if cross_validate:\n",
    "        df_datacite_all = pd.concat([df_datacite_initial, df_datacite_new], ignore_index=True)\n",
    "    else:\n",
    "        df_datacite_all = df_datacite_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894350a5",
   "metadata": {},
   "source": [
    "### Step 2: Cleaning and filtering\n",
    "\n",
    "This step cleans and filters the results from the DataCite query. Several intermediate files are exported here for specific functions (counting which metadata field an affiliation was detected in and which permutation it is; various metadata assessments). The deduplication process is designed to handle 'parent-child' DOI systems where each deposit receives at least two DOIs; DOI-for-each-version systems where each additional version receives its own DOI; and over-granularization of the automated Figshare process when files linked to a single manuscript are all split into their own deposits. For the Figshare deposits, metadata is 'consolidated' such that the metadata are combined (e.g., DOIs combined into semi-colon-delimited string). DOIs that are for files (e.g., Dataverse installations) are also excluded.\n",
    "\n",
    "It also handles edge cases with respect to inaccurate metadata labels (e.g., deposits in repositories that allow depositors to freeform-edit the 'publisher' field that is crosswalked to DataCite and usually used to indicate the repository where the deposit is stored). Name variation within a given repository is standardized, and all discovered Figshare deposits that were mediated through a publisher partner and that list that publisher as the 'publisher' in the DataCite metadata are standardized to list 'figshare' as the publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea774515",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadPreviousData and not loadPreviousDataPlus and not loadPreviousDataPlusNCBI:\n",
    "    #creating column for source of detected affiliation\n",
    "    pattern = '|'.join([f'({perm})' for perm in ut_variations])\n",
    "    #search for permutations in the 'affiliations' column\n",
    "    df_datacite_all['affiliation_source'] = df_datacite_all.apply(\n",
    "        lambda row: 'creator.affiliationName' if pd.Series(row['creators_affiliations']).str.contains(pattern, case=False, na=False).any()\n",
    "            else ('creator.name' if pd.Series(row['creators_names']).str.contains(pattern, case=False, na=False).any()\n",
    "            else ('contributor.affiliationName' if pd.Series(row['contributors_affiliations']).str.contains(pattern, case=False, na=False).any()\n",
    "            else ('contributor.name' if pd.Series(row['contributors_names']).str.contains(pattern, case=False, na=False).any() else None))),\n",
    "        axis=1)\n",
    "    # pull out the identified permutation and put it into a new column\n",
    "    df_datacite_all['affiliation_permutation'] = df_datacite_all.apply(\n",
    "    lambda row:\n",
    "        next(\n",
    "            #exact match (case-sensitive)\n",
    "            (perm for perm in ut_variations\n",
    "             if any(perm == entry for entry in row['creators_affiliations'] + row['creators_names'] + row['contributors_affiliations'] + row['contributors_names'])),\n",
    "            #full-phrase match (case-insensitive)\n",
    "            next(\n",
    "                (perm for perm in ut_variations\n",
    "                 if any(\n",
    "                     pd.Series(row['creators_affiliations'] + row['creators_names'] + row['contributors_affiliations'] + row['contributors_names'])\n",
    "                     .str.contains(fr'\\b{re.escape(perm)}\\b', case=True, na=False)\n",
    "                 )),\n",
    "                None\n",
    "            )\n",
    "        ),\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "    #handling version duplication (Figshare, ICPSR, etc.)\n",
    "    ##handling duplication of Figshare deposits (parent vs. child with '.v*')\n",
    "    ###creating separate figshare dataframe for downstream processing, not necessary for other repositories with this DOI mechanism in current workflow\n",
    "    figshare = df_datacite_all[df_datacite_all['doi'].str.contains('figshare', na=False)]\n",
    "    df_datacite_no_figshare = df_datacite_all[~df_datacite_all['doi'].str.contains('figshare', na=False)]\n",
    "    figshare_no_versions = figshare[~figshare['doi'].str.contains(r'\\.v\\d+$', na=False)]\n",
    "    #mediated workflow sometimes creates individual deposit for each file, want to treat as single dataset here\n",
    "    for col in figshare_no_versions.columns:\n",
    "        if figshare_no_versions[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            figshare_no_versions[col] = figshare_no_versions[col].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "    figshare_no_versions['had_partial_duplicate'] = figshare_no_versions.duplicated(subset=['publisher', 'publication_date', 'first_author', 'last_author', 'first_affiliation', 'type', 'related_identifier'], keep=False)\n",
    "\n",
    "    #aggregating related entries together\n",
    "    sum_columns = ['deposit_size', 'views', 'citations', 'downloads']\n",
    "\n",
    "    def agg_func(column_name):\n",
    "        sum_columns = ['deposit_size', 'views', 'citations', 'downloads']\n",
    "        if column_name in sum_columns:\n",
    "            return 'sum'\n",
    "        else:\n",
    "            def flatten_and_join(x):\n",
    "                #flatten if elements are lists\n",
    "                flattened = []\n",
    "                for item in x:\n",
    "                    if isinstance(item, list):\n",
    "                        flattened.extend(map(str, item))\n",
    "                    elif pd.notnull(item):\n",
    "                        flattened.append(str(item))\n",
    "                return '; '.join(sorted(set(flattened)))\n",
    "            return flatten_and_join\n",
    "\n",
    "    #handling mixed-type columns that are expected to be only numeric\n",
    "    for col in sum_columns:\n",
    "        if col in figshare_no_versions.columns:\n",
    "            figshare_no_versions[col] = pd.to_numeric(figshare_no_versions[col], errors='coerce')\n",
    "    agg_funcs = {col: agg_func(col)for col in figshare_no_versions.columns if col != 'related_identifier'}\n",
    "\n",
    "    figshare_no_versions_combined = figshare_no_versions.groupby('related_identifier').agg(agg_funcs).reset_index()\n",
    "    # Convert all list-type columns to comma-separated strings\n",
    "    for col in figshare_no_versions_combined.columns:\n",
    "        if figshare_no_versions_combined[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            figshare_no_versions_combined[col] = figshare_no_versions_combined[col].apply(lambda x: '; '.join(map(str, x)))\n",
    "    figshare_deduplicated = figshare_no_versions_combined.drop_duplicates(subset='related_identifier', keep='first')\n",
    "    df_datacite_v1 = pd.concat([df_datacite_no_figshare, figshare_deduplicated], ignore_index=True)\n",
    "\n",
    "    ##handling SESAR (physical geological sample repository)\n",
    "    sesar = df_datacite_v1[df_datacite_v1['publisher'].str.contains('SESAR', na=False, case=True)]\n",
    "    df_datacite_no_sesar = df_datacite_v1[~df_datacite_v1['publisher'].str.contains('SESAR', na=False, case=True)]\n",
    "    ###set multiple column grouping\n",
    "    group_cols = ['publication_date', 'first_author', 'last_author', 'contributors_names']\n",
    "    ####convert list\n",
    "    agg_funcs = {col: agg_func(col) for col in sesar.columns if col not in group_cols}\n",
    "    for col in group_cols:\n",
    "        if col == 'contributors_names':\n",
    "            sesar[col] = sesar[col].apply(\n",
    "                lambda x: ';'.join(map(str, x)) if isinstance(x, list) else (str(x) if pd.notnull(x) else '')\n",
    "            )\n",
    "        else:\n",
    "            sesar[col] = sesar[col].apply(lambda x: str(x) if pd.notnull(x) else '')\n",
    "\n",
    "    sesar_grouped = sesar.groupby(group_cols).agg(agg_funcs).reset_index()\n",
    "    df_datacite_v2 = pd.concat([df_datacite_no_sesar, sesar_grouped], ignore_index=True)\n",
    "\n",
    "    ##handling duplication of ICPSR, SAGE, Mendeley Data, Zenodo deposits (parent vs. child)\n",
    "    lineageRepos = df_datacite_v2[(df_datacite_v2['publisher'].str.contains('ICPSR|Mendeley|SAGE|Zenodo|4TU|Materials Cloud'))|(df_datacite_v2['doi'].str.contains('zenodo'))]\n",
    "    df_datacite_lineageRepos = df_datacite_v2[~(df_datacite_v2['publisher'].str.contains('ICPSR|Mendeley|SAGE|Zenodo|4TU|Materials Cloud')|df_datacite_v2['doi'].str.contains('zenodo'))]\n",
    "    lineageRepos_deduplicated = lineageRepos[~lineageRepos['relation_type'].str.contains('IsVersionOf|IsNewVersionOf', case=False, na=False)]\n",
    "    ###the use of .v* and v* as filters works for these repositories but could accidentally remove non-duplicate DOIs if applied to other repositories\n",
    "    lineageRepos_deduplicated = lineageRepos_deduplicated[~lineageRepos_deduplicated['doi'].str.contains(r'\\.v\\d+$')]\n",
    "    dois_to_remove = lineageRepos_deduplicated[(lineageRepos_deduplicated['doi'].str.contains(r'v\\d$') | lineageRepos_deduplicated['doi'].str.contains(r'v\\d-')) & (lineageRepos_deduplicated['publisher'].str.contains('ICPSR', case=False, na=False))]['doi']\n",
    "    # Remove the identified DOIs\n",
    "    lineageRepos_deduplicated = lineageRepos_deduplicated[~lineageRepos_deduplicated['doi'].isin(dois_to_remove)]\n",
    "    df_datacite_v3 = pd.concat([df_datacite_lineageRepos, lineageRepos_deduplicated], ignore_index=True)\n",
    "    \n",
    "    #handling historic Dryad DOI assignment to some files (may not occur for all institutions, does not occur for UT Austin)\n",
    "    df_datacite_dedup = df_datacite_v3[~((df_datacite_v3['publisher'] == 'Dryad') & (df_datacite_v3['doi'].str.count('/') >= 2))]\n",
    "\n",
    "    #handling Code Ocean (software repo, always ends in v*, only retain v1)\n",
    "    df_datacite_v3 = df_datacite_v3[~((df_datacite_v3['publisher'] == 'Code Ocean') & ~df_datacite_v3['doi'].str.endswith('v1'))]\n",
    "\n",
    "    #handling file-level DOI granularity (all Dataverse installations)\n",
    "    ##may need to expand search terms if you find a Dataverse installation without 'Dataverse' in name\n",
    "    df_datacite_dedup = df_datacite_v3[~(df_datacite_v3['publisher'].str.contains('Dataverse|Texas Data Repository', case=False, na=False) & df_datacite_v3['container_identifier'].notnull())]\n",
    "    ###should catch other Dataverse installations' files but exclude aggregated entries\n",
    "    df_datacite_dedup = df_datacite_dedup[~((df_datacite_dedup['doi'].str.count('/') >= 3) & (df_datacite_dedup['publisher'] != 'figshare') & (~df_datacite_dedup['doi'].str.contains(';')))]\n",
    "\n",
    "    #handling same granularity in other repositories where files have more than one (1) '/'\n",
    "    target_publishers = ['AUSSDA', 'CUHK Research Data Repository', 'Qualitative Data Repository']\n",
    "    df_datacite_dedup = df_datacite_dedup[~((df_datacite_dedup['publisher'].isin(target_publishers)) & (df_datacite_dedup['doi'].str.count('/') > 1))]\n",
    "    #handling blanket 'affiliation' of UT Austin for all DesignSafe deposits\n",
    "    ##DesignSafe is a UT-managed repository and this step is unlikely to be signficant for other institutions; there should also be a metadata fix for this forthcoming\n",
    "    if austin:\n",
    "        df_datacite_dedup = df_datacite_dedup[~((df_datacite_dedup['publisher'] == 'Designsafe-CI') & (df_datacite_dedup['affiliation_permutation'] != 'University of Texas at Austin'))\n",
    "        ]\n",
    "\n",
    "    #handling Dataverse partial duplication (oversplitting of one manuscript's materials)\n",
    "    if dataverse_duplicates:\n",
    "        # Convert list columns to strings\n",
    "        df_datacite_dedup['creators_names'] = df_datacite_dedup['creators_names'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "        df_datacite_dedup['contributors_affiliations'] = df_datacite_dedup['contributors_affiliations'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "        df_datacite_dedup['rights'] = df_datacite_dedup['rights'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "        dataverse = df_datacite_dedup[df_datacite_dedup['publisher'].str.contains('Texas Data Repository|Harvard|Dataverse', case=True, na=False)]\n",
    "        df_datacite_no_dataverse = df_datacite_dedup[~df_datacite_dedup['publisher'].str.contains('Texas Data Repository|Harvard|Dataverse', case=True, na=False)]\n",
    "        group_variables = ['publisher', 'publication_date', 'creators_names', 'contributors_affiliations', 'type', 'rights']\n",
    "        sum_columns = ['deposit_size', 'views', 'citations', 'downloads']\n",
    "\n",
    "        #ensure list-type columns are hashable for grouping\n",
    "        for col in dataverse.columns:\n",
    "            if dataverse[col].apply(lambda x: isinstance(x, list)).any():\n",
    "                dataverse[col] = dataverse[col].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "\n",
    "        #create column for partial duplicates\n",
    "        dataverse['had_partial_duplicate'] = dataverse.duplicated(subset=group_variables, keep=False)\n",
    "\n",
    "        #modified entry aggregation function\n",
    "        def agg_func(column_name):\n",
    "            if column_name in sum_columns:\n",
    "                return 'sum'\n",
    "            else:\n",
    "                return lambda x: sorted(set(\n",
    "                    str(item)\n",
    "                    for sublist in x\n",
    "                    for item in (list(sublist) if isinstance(sublist, (list, set)) else [sublist])\n",
    "                ))\n",
    "\n",
    "        agg_funcs = {\n",
    "            col: agg_func(col)\n",
    "            for col in dataverse.columns\n",
    "            if col not in group_variables\n",
    "        }\n",
    "        for col in sum_columns:\n",
    "            if col in dataverse.columns:\n",
    "                dataverse[col] = pd.to_numeric(dataverse[col], errors='coerce')\n",
    "\n",
    "        dataverse_combined = dataverse.groupby(group_variables).agg(agg_funcs).reset_index()\n",
    "\n",
    "        #convert list-type columns to semicolon-separated strings\n",
    "        for col in dataverse_combined.columns:\n",
    "            if dataverse_combined[col].apply(lambda x: isinstance(x, list)).any():\n",
    "                dataverse_combined[col] = dataverse_combined[col].apply(lambda x: '; '.join(map(str, x)))\n",
    "\n",
    "        dataverse_deduplicated = dataverse_combined.drop_duplicates(subset=group_variables, keep='first')\n",
    "        print(f'Number of deposits cut from {len(dataverse)} to {len(dataverse_combined)}')\n",
    "        df_datacite_dedup = pd.concat([df_datacite_no_dataverse, dataverse_combined], ignore_index=True)\n",
    "\n",
    "    #final sweeping dedpulication step, will catch a few odd edge cases that have been manually discovered\n",
    "    ##mainly addresses hundreds of EMSL datasets that seem overly granularized (many deposits share all metadata other than DOI including detailed titles) - will not be relevant for all institutions\n",
    "    df_sorted = df_datacite_dedup.sort_values(by='doi')\n",
    "    df_datacite = df_sorted.drop_duplicates(subset=['title', 'first_author', 'relation_type', 'related_identifier', 'container_identifier'], keep='first')\n",
    "\n",
    "    #the file exported here is intended only to be used to compare affiliation source fields; the fields will be dropped in later steps in the workflow\n",
    "    df_datacite.to_csv(f'outputs/{today}_{resource_filename}_datacite-output-for-affiliation-source.csv', index=False, encoding='utf-8') \n",
    "\n",
    "    #additional metadata assessment steps, fields are also dropped in later steps\n",
    "    df_datacite['file_format'] = df_datacite['formats'].apply(lambda formats: ('; '.join([format_map.get(fmt, fmt) for fmt in formats])if isinstance(formats, set) else formats))        \n",
    "    ##look for software file formats\n",
    "    df_datacite['contains_code'] = df_datacite['file_format'].apply(lambda x: any(part.strip() in software_formats for part in x.split(';')) if isinstance(x, str) else False)\n",
    "    df_datacite['only_code'] = df_datacite['file_format'].apply(lambda x: all(part.strip() in software_formats for part in x.split(';')) if isinstance(x, str) else False)\n",
    "    df_datacite['title_reformatted'] = df_datacite['title'].str.replace('_', ' ') #gets around text linked by underscores counting as 1 word\n",
    "    df_datacite['title_reformatted'] = df_datacite['title_reformatted'].str.lower()\n",
    "    df_datacite[['total_word_count_title', 'descriptive_word_count_title']] = (df_datacite['title_reformatted'].apply(lambda x: pd.Series(count_words(x, nondescriptive_words))))\n",
    "    df_datacite['descriptive_word_count_title'] = df_datacite.apply(adjust_descriptive_count, axis=1)\n",
    "    df_datacite['nondescriptive_word_count_title'] = df_datacite['total_word_count_title'] - df_datacite['descriptive_word_count_title']\n",
    "\n",
    "    #standardizing licenses\n",
    "    df_datacite['rights'] = df_datacite['rights'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x).astype(str).str.strip('[]')\n",
    "    df_datacite['rights_standardized'] = 'Rights unclear'  #default value\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Creative Commons Zero|CC0'), 'rights_standardized'] = 'CC0'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Creative Commons Attribution Non Commercial Share Alike'), 'rights_standardized'] = 'CC BY-NC-SA'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Creative Commons Attribution Non Commercial'), 'rights_standardized'] = 'CC BY-NC'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Creative Commons Attribution 3.0|Creative Commons Attribution 4.0|Creative Commons Attribution-NonCommercial'), 'rights_standardized'] = 'CC BY'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('GNU General Public License'), 'rights_standardized'] = 'GNU GPL'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Apache License'), 'rights_standardized'] = 'Apache'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('MIT License'), 'rights_standardized'] = 'MIT'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('BSD'), 'rights_standardized'] = 'BSD'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('ODC-BY'), 'rights_standardized'] = 'ODC-BY'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Open Access'), 'rights_standardized'] = 'Rights unclear'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Closed Access'), 'rights_standardized'] = 'Restricted access'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Restricted Access'), 'rights_standardized'] = 'Restricted access'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Databrary'), 'rights_standardized'] = 'Custom terms'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('UCAR'), 'rights_standardized'] = 'Custom terms'\n",
    "    df_datacite.loc[df_datacite['rights'] == '', 'rights_standardized'] = 'Rights unclear'\n",
    " \n",
    "    df_datacite.to_csv(f'outputs/{today}_{resource_filename}_datacite-output-for-metadata-assessment.csv', index=False, encoding='utf-8') \n",
    "\n",
    "    #subsetting dataframe\n",
    "    # df_datacite_pruned = df_datacite[['publisher', 'doi', 'publication_year', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'source', 'type']]\n",
    "    ##currently not pruning to maximize metadata retention for internal processes\n",
    "    df_datacite_pruned = df_datacite\n",
    "\n",
    "    #adding column for select high-volume repos\n",
    "    repo_mapping = {\n",
    "        'Dryad': 'Dryad',\n",
    "        'Zenodo': 'Zenodo',\n",
    "        'Texas Data Repository': 'Texas Data Repository'\n",
    "    }\n",
    "\n",
    "    df_datacite_pruned['repository2'] = df_datacite_pruned['publisher'].map(repo_mapping).fillna('Other')\n",
    "    df_datacite_pruned['uni_lead'] = df_datacite_pruned.apply(lambda row: determine_affiliation(row, ut_variations), axis=1)\n",
    "\n",
    "    #standardizing repositories with multiple versions of name in dataframe\n",
    "    ##different institutions may need to add additional repositories; nothing will happen if you don't have any of the ones listed below and don't comment the lines out\n",
    "    df_datacite_pruned['publisher'] = df_datacite_pruned['publisher'].fillna('None')\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Digital Rocks', case=False), 'publisher'] = 'Digital Porous Media Portal'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Environmental System Science Data Infrastructure for a Virtual Ecosystem|Southeast Texas Urban Integrated Field Laboratory', case=False), 'publisher'] = 'ESS-DIVE'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Texas Data Repository|Texas Research Data Repository', case=False), 'publisher'] = 'Texas Data Repository'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('ICPSR', case=True), 'publisher'] = 'ICPSR'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Environmental Molecular Sciences Laboratory', case=True), 'publisher'] = 'Environ Mol Sci Lab'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('BCO-DMO|Biological and Chemical Oceanography Data', case=True), 'publisher'] = 'Biol Chem Ocean Data Mgmt Office'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('BCO-DMO', case=True), 'publisher'] = 'Biol Chem Ocean Data Mgmt Office'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Taylor & Francis|SAGE|The Royal Society|SciELO journals', case=True), 'publisher'] = 'figshare'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Oak Ridge', case=True), 'publisher'] = 'Oak Ridge National Laboratory'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('PARADIM', case=True), 'publisher'] = 'PARADIM'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('4TU', case=True), 'publisher'] = '4TU.ResearchData'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Scratchpads|Biodiversity Collection|Algae', case=True), 'publisher'] = 'Global Biodiversity Information Facility (GBIF)'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('NCAR', case=True), 'publisher'] = 'NSF NCAR Earth Observing Laboratory'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Consortium of Universities for the Advancement of Hydrologic Science, Inc', case=False), 'publisher'] = 'CUAHSI'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Bureau of Economic Geology (UT-BEG)', case=False), 'publisher'] = 'AmeriFlux'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['doi'].str.contains('zenodo', case=False), 'publisher'] = 'Zenodo'\n",
    "\n",
    "\n",
    "    #EDGE CASES, likely unnecessary for other universities, but you will need to find your own edge cases\n",
    "    ##confusing metadata with UT Austin (but not Dataverse) listed as publisher; have to be manually adjusted over time\n",
    "    if austin:\n",
    "        df_datacite_pruned.loc[(df_datacite_pruned['doi'].str.contains('10.11578/dc')) & (df_datacite_pruned['publisher'].str.contains('University of Texas')), 'publisher'] = 'Department of Energy (DOE) CODE'\n",
    "        ##other edge cases\n",
    "        df_datacite_pruned.loc[df_datacite_pruned['doi'].str.contains('10.23729/547d8c47-3723-4396-8f84-322c02ccadd0'), 'publisher'] = 'Finnish Fairdata' #labeled publisher as author's name\n",
    "\n",
    "    #adding categorization\n",
    "    ##identifying institutional repositories that are not the Texas Data Repository\n",
    "    df_datacite_pruned['non_TDR_IR'] = np.where(df_datacite_pruned['publisher'].str.contains('University|UCLA|UNC|Harvard|ASU Library|Dataverse|DaRUS', case=True), 'non-TDR institutional', 'not university or TDR')\n",
    "    df_datacite_pruned['US_federal'] = np.where(df_datacite_pruned['publisher'].str.contains('NOAA|NIH|NSF|U.S.|DOE|DOD|DOI|National|Designsafe', case=True), 'Federal US repo', 'not federal US repo')\n",
    "    df_datacite_pruned['GREI'] = np.where(df_datacite_pruned['publisher'].str.contains('Dryad|figshare|Harvard|Zenodo|Vivli|Mendeley|Open Science Framework', case=False), 'GREI member', 'not GREI member')\n",
    "    generalist_keywords = 'Dryad|figshare|Zenodo|Mendeley|Open Science Framework|Science Data Bank'\n",
    "    institutional_keywords = 'ASU Library|Boise State|Borealis|Caltech|CUHK|Dataverse|Oregon|Princeton|University|Wyoming|DaRUS|Texas|Institut Laue-Langevin|Jagiellonian|Hopkins|Purdue|Yale|GRO.data|DR-NTU|CUAHSI'\n",
    "\n",
    "    df_datacite_pruned['scope'] = df_datacite_pruned['publisher'].apply(\n",
    "        lambda x: (\n",
    "            'Generalist' if pd.notnull(x) and re.search(generalist_keywords, x, re.IGNORECASE)\n",
    "            else 'Institutional' if pd.notnull(x) and re.search(institutional_keywords, x, re.IGNORECASE)\n",
    "            else 'Specialist'\n",
    "        )\n",
    "    )\n",
    "    df_datacite_pruned = df_datacite_pruned.rename(columns={'publisher': 'repository'})\n",
    "\n",
    "    #manually reclassifying certain resourceTypes\n",
    "    conditions = [\n",
    "        df_datacite_pruned['type'].isin(['Dataset', 'Image', 'PhysicalObject']),\n",
    "        df_datacite_pruned['type'].isin(['Software', 'ComputationalNotebook']), \n",
    "        df_datacite_pruned['type'] == 'Collection'\n",
    "    ]\n",
    "    types = ['Dataset', 'Software', 'Collection']\n",
    "    df_datacite_pruned['type_reclassified'] = np.select(conditions, types, default='Other')\n",
    "\n",
    "    #isolating affiliated authors\n",
    "    ##cloning df\n",
    "    df_datacite_researchers = df_datacite_pruned\n",
    "    df_datacite_researchers['affiliated_creators'] = df_datacite_researchers['creators_formatted'].apply(\n",
    "    lambda creators: [creator for creator in creators if any(perm.lower() in creator.lower() for perm in ut_variations)] if isinstance(creators, list) else [])\n",
    "    df_datacite_researchers['affiliated_contributors'] = df_datacite_researchers['contributors_formatted'].apply(\n",
    "    lambda contributors: [contributor for contributor in contributors if any(perm.lower() in contributor.lower() for perm in ut_variations)] if isinstance(contributors, list)else [])\n",
    "    df_datacite_researchers['affiliated_creators'] = df_datacite_researchers['affiliated_creators'].apply(lambda x: '; '.join(x))\n",
    "    df_datacite_researchers['affiliated_contributors'] = df_datacite_researchers['affiliated_contributors'].apply(lambda x: '; '.join(x))\n",
    "\n",
    "    df_datacite_researchers['affiliated_combined'] = df_datacite_researchers.apply(\n",
    "    lambda row: list({researcher for col in ['creators_formatted', 'contributors_formatted'] if isinstance(row[col], list) for researcher in row[col] if any(perm.lower() in researcher.lower() for perm in ut_variations)}),axis=1)\n",
    "    df_datacite_researchers['affiliated_combined'] = df_datacite_researchers['affiliated_combined'].apply(lambda x: '; '.join(x))\n",
    "    df_datacite_researchers['affiliated_combined'] = df_datacite_researchers['affiliated_combined'].apply(\n",
    "    lambda x: [i.strip() for i in x.split(';')] if pd.notnull(x) and x != '' else []\n",
    "    )\n",
    "    df_researchers = df_datacite_researchers.explode('affiliated_combined')\n",
    "    df_researchers['researcher'] = df_researchers['affiliated_combined'].str.split('(').str[0].str.strip()\n",
    "\n",
    "    df_researchers_pruned = df_researchers[['researcher','repository', 'doi', 'publication_year', 'type', 'affiliation_permutation', 'US_federal', 'scope']]\n",
    "\n",
    "    unique_names = df_researchers_pruned['researcher'].unique()\n",
    "    standardized_names = {}\n",
    "\n",
    "    for name in unique_names:\n",
    "    # Only try to match if standardized_names is not empty\n",
    "        if standardized_names:\n",
    "            result = process.extractOne(name, standardized_names.keys(), scorer=fuzz.token_sort_ratio)\n",
    "            if result is not None:\n",
    "                match, score, _ = result  # rapidfuzz returns (match, score, index)\n",
    "                if score > 90:  # Adjust threshold as needed\n",
    "                    standardized_names[name] = match\n",
    "                else:\n",
    "                    standardized_names[name] = name\n",
    "            else:\n",
    "                standardized_names[name] = name\n",
    "        else:\n",
    "            standardized_names[name] = name  # First name, nothing to match yet\n",
    "\n",
    "    df_researchers_pruned['researcher_standardized'] = df_researchers_pruned['researcher'].map(standardized_names)\n",
    "\n",
    "    df_researchers_unique = df_researchers_pruned.groupby('researcher_standardized').agg(lambda x: '; '.join(x.astype(str).unique())).reset_index()\n",
    "    df_researchers_unique['dataset_count'] = df_researchers_unique['researcher_standardized'].map(df_researchers_pruned.groupby('researcher_standardized')['doi'].count())\n",
    "    df_researchers_unique['name_count'] = df_researchers_unique['researcher_standardized'].map(df_researchers_pruned.groupby('researcher_standardized')['researcher'].nunique())\n",
    "    df_researchers_unique['repository_count'] = df_researchers_unique['researcher_standardized'].map(df_researchers_pruned.groupby('researcher_standardized')['repository'].nunique())\n",
    "\n",
    "    df_researchers_unique.to_csv(f'outputs/{today}_{resource_filename}_unique-affiliated-researchers.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    df_datacite_pruned.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eee7ed",
   "metadata": {},
   "source": [
    "### Step 3: Figshare workaround\n",
    "\n",
    "This step presents two different partial solutions to the general absence of affiliation metadata recording/crosswalking for Figshare deposits. The first solution takes advantage of how the *publisher* is listed for publisher-mediated deposits (as the scholarly publisher, not as 'figshare') to identify deposits mediated by a certain scholarly publisher that mints the Figshare DOIs through DataCite (not all of them do; Taylor & Francis is an example of one that does). These mediated deposits almost always indicate the DOI of the article that they are linked to in a metadata field. It then retrieves a list of articles published by that publisher and affiliated to the focal institution. The article DOIs can then be cross-matched against the mediated Figshare deposits' related identifier DOIs to identify Figshare deposits without affiliation metadata that can be conclusively linked to an affiliated article.\n",
    "\n",
    "The second solution is more time-intensive and less efficient, but it may be the most viable approach for certain use cases, including publishers that mint their mediated Figshare DOIs through Crossref instead of DataCite. It also retrieves affiliated articles from a scholarly publisher partner (e.g., Taylor & Francis) but then constructs a hypothetical dataset DOI based on the observation that many publishers use a system in which '.s00*' is appended to the article DOI, where '*' is a sequential integer. Each hypothetical dataset DOI is then tested to see if it exists. This approach's shortcomings lie in the fact that not all publisher partners mint Figshare DOIs in this way; that there is no way of knowing how many sequential integers to test; and that additional steps would be necessary to retrieve the same level of metadata as the first solution (would require another API call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e334b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_previous_data:\n",
    "    #for reading in previously generated file of all associated datasets\n",
    "    print('Reading in previous DataCite output file\\n')\n",
    "    directory = './outputs' \n",
    "    pattern = '_full-concatenated-dataframe.csv'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(reverse=True)\n",
    "    latest_file = None\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            latest_file = file\n",
    "            break\n",
    "\n",
    "    if latest_file:\n",
    "        file_path = os.path.join(directory, latest_file)\n",
    "        df_datacite_pruned = pd.read_csv(file_path)\n",
    "        print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "    else:\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "### This codeblock will retrieve all figshare deposits with a listed journal/publisher as 'publisher,' extract related identifiers, retrieve all articles published by a certain publisher, cross-reference article DOIs against dataset related identifiers, and produce a match list. ###\n",
    "if figshare_workflow_1:\n",
    "\n",
    "    #figshare DOIs sometimes have a .v* for version number; this toggles whether to include them (True) or only include the parent (False)\n",
    "    countVersions = config['TOGGLES']['figshare_versions']\n",
    "\n",
    "    #pull in map of publisher names and OpenAlex codes\n",
    "    publisher_mapping = config['FIGSHARE_PARTNERS']\n",
    "    #create empty object to store results\n",
    "    data_select_datacite = [] \n",
    "    data_select_openalex = []\n",
    "\n",
    "    for publisher_name, openalex_code in publisher_mapping.items():\n",
    "        try:\n",
    "            #update both params for each publisher in map\n",
    "            params_openalex = {\n",
    "            'filter': f'authorships.institutions.ror:https://ror.org/00hj54h04,type:article,from_publication_date:2000-01-01,locations.source.host_organization:{openalex_code}',\n",
    "            'per_page': config['VARIABLES']['PAGE_SIZES']['openalex'],\n",
    "            'select': 'id,doi,title,authorships,publication_year,primary_location,type',\n",
    "            'mailto': config['EMAIL']['user_email']\n",
    "            }\n",
    "            j = 0\n",
    "            #define different number of pages to retrieve from OpenAlex API based on 'test' vs. 'prod' env\n",
    "            page_limit_openalex = config['VARIABLES']['PAGE_LIMITS']['openalex_test'] if test else config['VARIABLES']['PAGE_LIMITS']['openalex_prod']\n",
    "            #DataCite params (different from general affiliation-based retrieval params)\n",
    "            ## !! Warning: if you do not set a resource_type in the query (recommended if you want to get broad coverage), this will be a very large retrieval. In the test env, there may not be enough records to find a match with a university-affiliated article !!\n",
    "            \n",
    "            #reset to default after large-scale general retrieval through DataCite\n",
    "            page_limit_datacite = config['VARIABLES']['PAGE_LIMITS']['datacite_test'] if test else config['VARIABLES']['PAGE_LIMITS']['datacite_prod']\n",
    "            page_start_datacite = config['VARIABLES']['PAGE_STARTS']['datacite']\n",
    "            if figshare_resource_type_filter:\n",
    "                params_datacite_figshare = {\n",
    "                    # 'affiliation': 'true',\n",
    "                    'query': f'(publisher:\"{publisher_name}\" AND types.resourceTypeGeneral:{figshare_datacite_resource_type})',\n",
    "                    'page[size]': config['VARIABLES']['PAGE_SIZES']['datacite'],\n",
    "                    'page[cursor]': 1,\n",
    "                }\n",
    "            else:\n",
    "                params_datacite_figshare = {\n",
    "                # 'affiliation': 'true',\n",
    "                'query': f'(publisher:\"{publisher_name}\")',\n",
    "                'page[size]': config['VARIABLES']['PAGE_SIZES']['datacite'],\n",
    "                'page[cursor]': 1,\n",
    "                }\n",
    "\n",
    "            print(f'Starting DataCite retrieval for {publisher_name}.\\n')\n",
    "            data_datacite_figshare = retrieve_datacite(url_datacite, params_datacite_figshare, page_start_datacite, page_limit_datacite, per_page_datacite)\n",
    "            print(f'Number of datasets associated with {publisher_name} found by DataCite API: {len(data_datacite_figshare)}\\n')\n",
    "            \n",
    "            for item in data_datacite_figshare:\n",
    "                if not isinstance(item, dict):\n",
    "                    print(f'ERROR: item is not a dict! Type: {type(item)}, Value: {item}')\n",
    "                    continue\n",
    "                attributes = item.get('attributes', {})\n",
    "                doi_dc = attributes.get('doi', None)\n",
    "                state = attributes.get('state', None)\n",
    "                publisher_dc = attributes.get('publisher', '')\n",
    "                publisher_year_dc = attributes.get('publicationYear', '')\n",
    "                registered = attributes.get('registered', '')\n",
    "                if registered:\n",
    "                    publisher_year_dc = datetime.fromisoformat(registered.rstrip('Z')).year\n",
    "                    publisher_date_dc = datetime.fromisoformat(registered.rstrip('Z')).date()\n",
    "                else:\n",
    "                    publisher_year_dc = None\n",
    "                    publisher_date_dc = None\n",
    "                title_dc = attributes.get('titles', [{}])[0].get('title', '')\n",
    "                creators_dc = attributes.get('creators', [{}])\n",
    "                if not isinstance(creators_dc, list):\n",
    "                    print(f'ERROR: creators_dc is not a list! Type: {type(creators_dc)}, Value: {creators_dc}')\n",
    "                    creators_dc = [{}]\n",
    "                creators_names = []\n",
    "                contributors_affiliations = []\n",
    "                for creator in creators_dc:\n",
    "                    if not isinstance(creator, dict):\n",
    "                        print(f'ERROR: creator is not a dict! Type: {type(creator)}, Value: {creator}')\n",
    "                        continue\n",
    "                    creators_names.append(creator.get('name', ''))\n",
    "                    affiliations = creator.get('affiliation', [])\n",
    "                    aff_names = []\n",
    "                    if not isinstance(affiliations, list):\n",
    "                        print(f'ERROR: affiliations is not a list! Type: {type(affiliations)}, Value: {affiliations}')\n",
    "                        continue\n",
    "                    for aff in affiliations:\n",
    "                        if isinstance(aff, dict):\n",
    "                            aff_names.append(aff.get('name', ''))\n",
    "                        elif isinstance(aff, str):\n",
    "                            aff_names.append(aff)\n",
    "                        else:\n",
    "                            print(f'ERROR: affiliation is not dict or str! Type: {type(aff)}, Value: {aff}')\n",
    "                    contributors_affiliations.append('; '.join(aff_names) if aff_names else '')\n",
    "                creators_formatted = []\n",
    "                for creator in creators:\n",
    "                    name = creator.get('name', '').strip()\n",
    "                    affiliations = creator.get('affiliation', [])\n",
    "                    updated_affiliations = []\n",
    "                    for affil in affiliations:\n",
    "                        affil_name = affil.get('name', '') if isinstance(affil, dict) else affil\n",
    "                        if 'Austin' in affil_name:\n",
    "                            affil_name = 'University of Texas at Austin'\n",
    "                        updated_affiliations.append(affil_name)\n",
    "                    affil_str = ', '.join(updated_affiliations) if updated_affiliations else 'No affiliation listed'\n",
    "                    creators_formatted.append(f'{name} ({affil_str})')\n",
    "                contributors_dc = attributes.get('contributors', [{}])\n",
    "                if not isinstance(contributors_dc, list):\n",
    "                    print(f'ERROR: contributors_dc is not a list! Type: {type(contributors_dc)}, Value: {contributors_dc}')\n",
    "                    contributors_dc = [{}]\n",
    "                contributors_names = []\n",
    "                contributors_affiliations = []\n",
    "                for contributor in contributors_dc:\n",
    "                    if not isinstance(contributor, dict):\n",
    "                        print(f'ERROR: creator is not a dict! Type: {type(contributor)}, Value: {contributor}')\n",
    "                        continue\n",
    "                    contributors_names.append(contributor.get('name', ''))\n",
    "                    affiliations = contributor.get('affiliation', [])\n",
    "                    aff_names = []\n",
    "                    if not isinstance(affiliations, list):\n",
    "                        print(f'ERROR: affiliations is not a list! Type: {type(affiliations)}, Value: {affiliations}')\n",
    "                        continue\n",
    "                    for aff in affiliations:\n",
    "                        if isinstance(aff, dict):\n",
    "                            aff_names.append(aff.get('name', ''))\n",
    "                        elif isinstance(aff, str):\n",
    "                            aff_names.append(aff)\n",
    "                        else:\n",
    "                            print(f'ERROR: affiliation is not dict or str! Type: {type(aff)}, Value: {aff}')\n",
    "                    contributors_affiliations.append('; '.join(aff_names) if aff_names else '')\n",
    "                for contributor in contributors:\n",
    "                    name = contributor.get('name', '').strip()\n",
    "                    affiliations = contributor.get('affiliation', [])\n",
    "                    updated_affiliations = []\n",
    "                    for affil in affiliations:\n",
    "                        affil_name = affil.get('name', '') if isinstance(affil, dict) else affil\n",
    "                        if 'Austin' in affil_name:\n",
    "                            affil_name = 'University of Texas at Austin'\n",
    "                        updated_affiliations.append(affil_name)\n",
    "                    affil_str = ', '.join(updated_affiliations) if updated_affiliations else 'No affiliation listed'\n",
    "                    contributors_formatted.append(f'{name} ({affil_str})')\n",
    "                related_identifiers = attributes.get('relatedIdentifiers', [])\n",
    "                container_dc = attributes.get('container', {})\n",
    "                container_identifier_dc = container_dc.get('identifier', None)\n",
    "                types = attributes.get('types', {})\n",
    "                resource_type = types.get('resourceTypeGeneral', '')\n",
    "                subjects = attributes.get('subjects', [])\n",
    "                if subjects:\n",
    "                    subject_list = [subj.get('subject', '').strip() for subj in subjects if subj.get('subject')]\n",
    "                    subjects_combined = '; '.join(subject_list) if subject_list else 'No keywords provided'\n",
    "                else:\n",
    "                    subjects_combined = 'No keywords provided'\n",
    "                sizes = attributes.get('sizes', [])\n",
    "                cleaned_sizes = [int(re.sub(r'\\D', '', size)) for size in sizes if re.sub(r'\\D', '', size).isdigit()]\n",
    "                total_size = sum(cleaned_sizes) if cleaned_sizes else 'No file size information'   \n",
    "                formats_list = attributes.get('formats', [])\n",
    "                formats = set(formats_list) if formats_list else 'No file information' \n",
    "                file_count = len(formats_list) if formats_list else 'No file information'   \n",
    "                rights_list = attributes.get('rightsList', [])\n",
    "                rights = [right['rights'] for right in rights_list if 'rights' in right] or ['Rights unspecified']\n",
    "                rights_code = [right['rightsIdentifier'] for right in rights_list if 'rightsIdentifier' in right] or ['Unknown']\n",
    "                views = attributes.get('viewCount', 0)\n",
    "                downloads = attributes.get('downloadCount', 0)\n",
    "                citations = attributes.get('citationCount', 0)\n",
    "\n",
    "                for rel in related_identifiers: #'explodes' deposits with multiple relatedIdentifiers\n",
    "                    data_select_datacite.append({\n",
    "                        'doi': doi_dc,\n",
    "                        'state': state,\n",
    "                        'repository': publisher_dc,\n",
    "                        'publisher_original': publisher_dc,\n",
    "                        'publication_year': publisher_year_dc,\n",
    "                        'publication_date': publisher_date_dc,\n",
    "                        'title': title_dc,\n",
    "                        'creators_names': creators_names,\n",
    "                        'contributors_affiliations': contributors_affiliations,\n",
    "                        'creators_formatted': creators_formatted,\n",
    "                        'relation_type': rel.get('relationType'),\n",
    "                        'related_identifier': rel.get('relatedIdentifier'),\n",
    "                        'related_identifier_type': rel.get('relatedIdentifierType'),\n",
    "                        'container_identifier': container_identifier_dc,\n",
    "                        'type': resource_type,\n",
    "                        'subjects': subjects_combined,\n",
    "                        'deposit_size': total_size,\n",
    "                        'formats': formats,\n",
    "                        'file_count': file_count,\n",
    "                        'rights': rights,\n",
    "                        'rights_code': rights_code,\n",
    "                        'views': views,\n",
    "                        'downloads': downloads,\n",
    "                        'citations': citations\n",
    "                    })\n",
    "            print(f'Starting OpenAlex retrieval for {publisher_name}.\\n')\n",
    "            openalex = retrieve_openalex(url_openalex, params_openalex, page_limit_openalex)\n",
    "            if openalex:\n",
    "                print(f'Number of articles associated with {publisher_name} found by OpenAlex API: {len(openalex)}\\n')\n",
    "            else:\n",
    "                print('WARNING: DATA NOT RETRIEVED')\n",
    "            for item in openalex:\n",
    "                doi = item.get('doi')\n",
    "                title = item.get('title')\n",
    "                publication_year = item.get('publication_year')\n",
    "                primary_location = item.get('primary_location')\n",
    "                if primary_location and isinstance(primary_location, dict):\n",
    "                    source = primary_location.get('source')\n",
    "                    if source and isinstance(source, dict):\n",
    "                        source_display_name = source.get('display_name')\n",
    "                    else:\n",
    "                        source_display_name = None\n",
    "                else:\n",
    "                    source_display_name = None\n",
    "                for authorship in item.get('authorships', []):\n",
    "                    if authorship.get('author_position') == 'first':\n",
    "                        first_author = authorship.get('author', {}).get('display_name')\n",
    "                        first_affiliation = [inst.get('display_name') for inst in authorship.get('institutions', [])]\n",
    "                    if authorship.get('author_position') == 'last':\n",
    "                        last_author = authorship.get('author', {}).get('display_name')\n",
    "                        last_affiliation = [inst.get('display_name') for inst in authorship.get('institutions', [])]\n",
    "                        \n",
    "                        data_select_openalex.append({\n",
    "                            'doi_article': doi,\n",
    "                            'title_article': title,\n",
    "                            'publication_year': publication_year,\n",
    "                            'journal': source_display_name,\n",
    "                            'first_author': first_author,\n",
    "                            'first_affiliation': first_affiliation,\n",
    "                            'last_author': last_author,\n",
    "                            'last_affiliation': last_affiliation\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f'An error occurred with the retrieval for {publisher_name}: {e}')\n",
    "            continue \n",
    "\n",
    "    df_datacite_initial = pd.json_normalize(data_select_datacite)\n",
    "    df_datacite_initial.to_csv(f'outputs/{today}_{figshare_resource_filename}_figshare-discovery-initial.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    if countVersions:\n",
    "        ##These steps will count different versions as distinct datasets and remove the 'parent' (redundant with most recent version)\n",
    "        df_datacite_initial['base'] = df_datacite_initial['doi'].apply(lambda x: x.split('.v')[0])\n",
    "        df_datacite_initial['version'] = df_datacite_initial['doi'].apply(lambda x: int(x.split('.v')[1]) if '.v' in x else 0)\n",
    "        max_versions = df_datacite_initial.groupby('base')['version'].max().reset_index()\n",
    "        df_datacite_initial = df_datacite_initial.merge(max_versions, on='base', suffixes=('', '_max'))\n",
    "        df_deduplicated = df_datacite_initial(subset='base')\n",
    "    else:\n",
    "        ##This step will remove all child deposits with a .v*  to retain only the 'parent'\n",
    "        df_deduplicated = df_datacite_initial[~df_datacite_initial['doi'].str.contains(r'\\.v\\d+$')]\n",
    "\n",
    "    df_datacite_supplement = df_deduplicated[df_deduplicated['relation_type'] == 'IsSupplementTo']\n",
    "    #mediated workflow sometimes creates individual deposit for each file, want to treat as single dataset here\n",
    "    df_datacite_supplement['had_partial_duplicate'] = df_datacite_supplement.duplicated(subset='related_identifier', keep='first')\n",
    "    df_datacite_supplement_dedup = df_datacite_supplement.drop_duplicates(subset='related_identifier', keep='first')\n",
    "    \n",
    "    df_openalex = pd.json_normalize(data_select_openalex)\n",
    "    df_openalex['related_identifier'] = df_openalex['doi_article'].str.replace('https://doi.org/', '')\n",
    "    df_openalex = df_openalex.drop_duplicates(subset='doi_article', keep='first')\n",
    "    df_openalex.to_csv(f'outputs/{today}_openalex-articles.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    #output all UT linked deposits, no deduplication (for Figshare validator workflow)\n",
    "    df_openalex_datacite = pd.merge(df_openalex, df_datacite_supplement, on='related_identifier', how='left')\n",
    "    df_openalex_datacite = df_openalex_datacite[df_openalex_datacite['doi'].notnull()]\n",
    "    df_openalex_datacite.to_csv(f'outputs/{today}_{figshare_resource_filename}_figshare-discovery-affiliated.csv', index=False, encoding='utf-8')\n",
    "    df_openalex_datacite = df_openalex_datacite.drop_duplicates(subset='related_identifier', keep='first')\n",
    "\n",
    "    #working with deduplicated dataset for rest of process\n",
    "    df_openalex_datacite_dedup = pd.merge(df_openalex, df_datacite_supplement_dedup, on='related_identifier', how='left')\n",
    "    new_figshare = df_openalex_datacite_dedup[df_openalex_datacite_dedup['doi'].notnull()]\n",
    "    new_figshare = new_figshare.drop_duplicates(subset='doi', keep='first')\n",
    "    new_figshare.to_csv(f'outputs/{today}_{figshare_resource_filename}_figshare-discovery-affiliated-deduplicated.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    ##currently not pruning to maximize metadata retention for internal processes\n",
    "    # new_figshare = new_figshare[['doi','publication_year','title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'type']]\n",
    "\n",
    "    new_figshare['title_reformatted'] = new_figshare['title'].str.replace('_', ' ') #gets around text linked by underscores counting as 1 word\n",
    "    new_figshare['title_reformatted'] = new_figshare['title_reformatted'].str.lower()\n",
    "    new_figshare[['total_word_count_title', 'descriptive_word_count_title']] = (new_figshare['title_reformatted'].apply(lambda x: pd.Series(count_words(x, nondescriptive_words))))\n",
    "    new_figshare['descriptive_word_count_title'] = new_figshare.apply(adjust_descriptive_count, axis=1)\n",
    "    new_figshare['nondescriptive_word_count_title'] = new_figshare['total_word_count_title'] - new_figshare['descriptive_word_count_title']\n",
    "\n",
    "    #standardizing licenses\n",
    "    new_figshare['rights'] = new_figshare['rights'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x).astype(str).str.strip('[]')\n",
    "    new_figshare['rights_standardized'] = 'Rights unclear'  #default value\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Creative Commons Zero|CC0'), 'rights_standardized'] = 'CC0'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Creative Commons Attribution Non Commercial Share Alike'), 'rights_standardized'] = 'CC BY-NC-SA'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Creative Commons Attribution Non Commercial'), 'rights_standardized'] = 'CC BY-NC'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Creative Commons Attribution 3.0|Creative Commons Attribution 4.0|Creative Commons Attribution-NonCommercial'), 'rights_standardized'] = 'CC BY'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('GNU General Public License'), 'rights_standardized'] = 'GNU GPL'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Apache License'), 'rights_standardized'] = 'Apache'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('MIT License'), 'rights_standardized'] = 'MIT'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('BSD'), 'rights_standardized'] = 'BSD'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('ODC-BY'), 'rights_standardized'] = 'ODC-BY'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Open Access'), 'rights_standardized'] = 'Rights unclear'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Closed Access'), 'rights_standardized'] = 'Restricted access'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Restricted Access'), 'rights_standardized'] = 'Restricted access'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('Databrary'), 'rights_standardized'] = 'Custom terms'\n",
    "    new_figshare.loc[new_figshare['rights'].str.contains('UCAR'), 'rights_standardized'] = 'Custom terms'\n",
    "    new_figshare.loc[new_figshare['rights'] == '', 'rights_standardized'] = 'Rights unclear'\n",
    "\n",
    "    #file formats (not presently returned for mediated deposits)\n",
    "    new_figshare['file_format'] = new_figshare['formats'].apply(\n",
    "    lambda formats: ('; '.join([format_map.get(fmt, fmt) for fmt in formats])if isinstance(formats, set) else formats))   \n",
    "    # Assume software_formats is a set of friendly software format names\n",
    "    new_figshare['contains_code'] = new_figshare['file_format'].apply(lambda x: any(part.strip() in software_formats for part in x.split(';')) if isinstance(x, str) else False)\n",
    "    new_figshare['only_code'] = new_figshare['file_format'].apply(lambda x: all(part.strip() in software_formats for part in x.split(';')) if isinstance(x, str) else False)\n",
    "\n",
    "    #adding in columns to reconcatenate with full dataset\n",
    "    new_figshare['first_affiliation'] = new_figshare['first_affiliation'].apply(lambda x: ' '.join([str(item) for item in x if item is not None]) if isinstance(x, list) else x)\n",
    "    new_figshare['last_affiliation'] = new_figshare['last_affiliation'].apply(lambda x: ' '.join([str(item) for item in x if item is not None]) if isinstance(x, list) else x)    \n",
    "    new_figshare['uni_lead'] = new_figshare.apply(lambda row: determine_affiliation(row, ut_variations), axis=1)\n",
    "    new_figshare['repository'] = 'figshare'\n",
    "    new_figshare['source'] = 'DataCite+' #slight differentiation from records only retrieved from DataCite\n",
    "    new_figshare['repository2'] = 'Other'\n",
    "    new_figshare['non_TDR_IR'] = 'not university or TDR'\n",
    "    new_figshare['US_federal'] = 'not federal US repo'\n",
    "    new_figshare['GREI'] = 'GREI member'\n",
    "    new_figshare['scope'] = 'Generalist'\n",
    "    conditions = [\n",
    "        new_figshare['type'].isin(['Dataset', 'Image', 'PhysicalObject']),\n",
    "        new_figshare['type'].isin(['Software', 'ComputationalNotebook']), \n",
    "        new_figshare['type'] == 'Collection'\n",
    "    ]\n",
    "    types = ['Dataset', 'Software', 'Collection']\n",
    "    new_figshare['type_reclassified'] = np.select(conditions, types, default='Other')\n",
    "    new_figshare['affiliation_permutation'] = 'Not applicable' #filler to match original DataCite since affiliation only in linked article and not detected through DataCite\n",
    "    new_figshare['affiliation_source'] = 'Not applicable' #filler to match original DataCite since affiliation only in linked article and not detected through DataCite\n",
    "\n",
    "    df_datacite_plus = pd.concat([df_datacite_pruned, new_figshare], ignore_index=True)\n",
    "    #de-duplicating in case some DOIs were caught twice (for the few publishers that do cross-walk affiliation metadata), you could use a sorting method to determine which one to 'keep'; the default will retain the ones returned from the main workflow\n",
    "    df_datacite_plus_dedup = df_datacite_plus.drop_duplicates(subset='doi', keep='first')\n",
    "    df_datacite_plus_dedup.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-figshare.csv', index=False, encoding='utf-8')\n",
    "\n",
    "### This codeblock identifies publishers known to create figshare deposits (can be any object resource type) with a '.s00*' system, finds affiliated articles, constructs a hypothetical figshare DOI for them, and tests its existence ###\n",
    "# !! Warning: Depending on the number of articles, this can be an extremely time-intensive process !! #\n",
    "\n",
    "if figshare_workflow_2:\n",
    "    #toggle to select which indexer to use: 'OpenAlex' or 'Crossref'\n",
    "    indexer = config['TOGGLES']['figshare_workflow_2_indexer']\n",
    "\n",
    "    #OpenAlex params\n",
    "    j = 0\n",
    "    if test:\n",
    "        page_limit_openalex = config['VARIABLES']['PAGE_LIMITS']['openalex_test']\n",
    "    else:\n",
    "        page_limit_openalex = config['VARIABLES']['PAGE_LIMITS']['openalex_prod']\n",
    "\n",
    "    #Crossref params\n",
    "    k = 0\n",
    "    if test:\n",
    "        page_limit_crossref = config['VARIABLES']['PAGE_LIMITS']['crossref_test']\n",
    "    else:\n",
    "        page_limit_crossref = config['VARIABLES']['PAGE_LIMITS']['crossref_prod']\n",
    "    params_crossref_journal = {\n",
    "        'select': 'DOI,prefix,title,author,container-title,publisher,created',\n",
    "        'filter': 'type:journal-article',\n",
    "        'rows': config['VARIABLES']['PAGE_SIZES']['crossref'],\n",
    "        'query': 'affiliation:University+Texas+Austin',\n",
    "        'mailto': config['EMAIL']['user_email'],\n",
    "        'cursor': '*',\n",
    "    }\n",
    "\n",
    "    params_openalex = {\n",
    "        'filter': 'authorships.institutions.ror:https://ror.org/00hj54h04,locations.source.host_organization:https://openalex.org/P4310315706', #PLOS ID in OpenAlex\n",
    "        'per-page': config['VARIABLES']['PAGE_SIZES']['openalex'],\n",
    "        'select': 'id,doi,title,authorships,primary_location,type',\n",
    "        'mailto': config['EMAIL']['user_email']\n",
    "    }\n",
    "\n",
    "    #JSON dictionary of journals for Crossref API query (PLOS in this example)\n",
    "    with open('journal-list.json', 'r') as file:\n",
    "        journal_list = json.load(file)\n",
    "\n",
    "    if indexer == 'OpenAlex':\n",
    "        openalex = retrieve_openalex(url_openalex, params_openalex, page_limit_openalex)\n",
    "        df_openalex = pd.json_normalize(openalex)\n",
    "        df_openalex['hypothetical_dataset'] = df_openalex['doi'] + '.s001'\n",
    "        \n",
    "        #Check if each DOI with suffix redirects to a real page and create a new column\n",
    "        df_openalex['Valid'] = df_openalex['hypothetical_dataset'].apply(check_link)\n",
    "        df_openalex.to_csv(f'outputs/{today}_openalex-articles-with-hypothetical-deposits.csv', index=False, encoding='utf-8')\n",
    "        print(f'Number of valid datasets: {len(df_openalex)}.')\n",
    "    else:\n",
    "        crossref_data = retrieve_all_journals(url_crossref_issn, journal_list, params_crossref_journal, page_limit_crossref, retrieve_crossref)\n",
    "\n",
    "        data_journals_select = []\n",
    "        for item in crossref_data:\n",
    "            publisher = item.get('publisher', None)\n",
    "            journal = item.get('container-title', None)[0]\n",
    "            doi = item.get('DOI', '')\n",
    "            title_list = item.get('title', [])\n",
    "            title = title_list[0] if title_list else None\n",
    "            author = item.get('author', None)\n",
    "            created = item.get('created', {})\n",
    "            createdDate = created.get('date-time', None)\n",
    "            \n",
    "            data_journals_select.append({\n",
    "                'publisher': publisher,\n",
    "                'journal': journal, \n",
    "                'doi': doi,\n",
    "                'author': author,\n",
    "                'title': title,\n",
    "                'published': createdDate,\n",
    "        })\n",
    "\n",
    "        df_crossref = pd.DataFrame(data_journals_select)\n",
    "        df_crossref['doi_html'] = 'https://doi.org/' + df_crossref['doi']\n",
    "        df_crossref['hypothetical_dataset'] = df_crossref['doi_html'] + '.s001'\n",
    "\n",
    "        # Check if each DOI with suffix redirects to a real page and create a new column\n",
    "        df_crossref['Valid'] = df_crossref['hypothetical_dataset'].apply(check_link)\n",
    "        df_crossref.to_csv(f'outputs/{today}_crossref-articles-with-hypothetical-deposits.csv', index=False, encoding='utf-8')\n",
    "        print(f'Number of valid datasets: {len(df_crossref)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93871ccd",
   "metadata": {},
   "source": [
    "### Step 4: NCBI workflow\n",
    "\n",
    "This step provides two different mechanisms for programmatically retrieving metadata on institutionally-affiliated BioProjects (the object most similar to a 'dataset' in the DataCite or Crossref schema), one based on automating a manual download of an XML file and the other using the *biopython* module to access the Entrez system. Either way, users should refer to the [NCBI documentation](https://www.ncbi.nlm.nih.gov/books/NBK25497/) for guidance on rate limiting and other restrictions on use. The output is returned as an XML and then coerced into the same structure as the DataCite output, although not all fields are equivalent (e.g., the project number is substituted for a DOI); certain fields are also metadata-deficient since NCBI does not use the DataCite metadata schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ncbi_workflow:\n",
    "    print('Starting NCBI process.\\n')\n",
    "\n",
    "    #set path for browser\n",
    "    ##works differently for Jupyter vs. .py file\n",
    "    try:\n",
    "        #for .py file\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        #for Jupyter\n",
    "        script_dir = os.getcwd()\n",
    "    if test:\n",
    "        outputs_dir = os.path.join(script_dir, 'test/outputs')\n",
    "    else:\n",
    "        outputs_dir = os.path.join(script_dir, 'outputs')\n",
    "\n",
    "    #check if previous output file exists\n",
    "    directory = './outputs'\n",
    "    pattern = 'bioproject_result'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            existingOutput = True\n",
    "            print(f'A previous \"{pattern}\" download was found in the directory \"{directory}\".')\n",
    "            break\n",
    "    else:\n",
    "        existingOutput = False\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    #read in config file\n",
    "    if not load_ncbi_data:\n",
    "        institution_name = config['INSTITUTION']['name']\n",
    "        #URL encode name\n",
    "        encoded_institution_name = quote(institution_name)\n",
    "        if not biopython:\n",
    "            #set up temporary Firefox 'profile' to direct downloads (profile not saved outside of script)\n",
    "            options = Options()\n",
    "            options.set_preference('browser.download.folderList', 2)\n",
    "            options.set_preference('browser.download.dir', outputs_dir)\n",
    "            options.set_preference('browser.helperApps.neverAsk.saveToDisk', 'application/octet-stream')\n",
    "            #blocking pop-up window to cancel download\n",
    "            options.set_preference('browser.download.manager.showWhenStarting', False)\n",
    "            options.set_preference('browser.download.manager.focusWhenStarting', False)\n",
    "            options.set_preference('browser.download.useDownloadDir', True)\n",
    "            options.set_preference('browser.download.manager.alertOnEXEOpen', False)\n",
    "            options.set_preference('browser.download.manager.closeWhenDone', True)\n",
    "            options.set_preference('browser.download.manager.showAlertOnComplete', False)\n",
    "            options.set_preference('browser.download.manager.useWindow', False)\n",
    "            options.set_preference('services.sync.prefs.sync.browser.download.manager.showWhenStarting', False)\n",
    "            options.set_preference('browser.download.alwaysOpenPanel', False)  # Disable the download panel\n",
    "            options.set_preference('browser.download.panel.shown', False)  # Ensure the download panel is not shown\n",
    "\n",
    "            #initialize Selenium WebDriver\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            ##searches all fields; searching Submitter Organization specifically does not recover all results\n",
    "            ncbi_url = f'https://www.ncbi.nlm.nih.gov/bioproject?term={encoded_institution_name}'\n",
    "            driver.get(ncbi_url)\n",
    "\n",
    "            try:\n",
    "                #load page and find the 'Send to' dropdown\n",
    "                send_to_link = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'sendto')))\n",
    "                send_to_link.click()\n",
    "\n",
    "                #load dropdown and select 'File' radio button\n",
    "                file_option = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'dest_File')))\n",
    "                file_option.click()\n",
    "\n",
    "                #load 'Format' dropdown and select 'XML'\n",
    "                format_dropdown = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'file_format')))\n",
    "                format_dropdown.click()\n",
    "                xml_option = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//option[@value='xml']\")))\n",
    "                xml_option.click()\n",
    "\n",
    "                #click the 'Create File' button\n",
    "                create_file_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//button[@cmd='File']\")))\n",
    "                create_file_button.click()\n",
    "\n",
    "                print('Download complete, about to close window.\\n')\n",
    "                time.sleep(10)\n",
    "\n",
    "                #overwrite any existing file with 'bioproject_result.xml' rather than continually creating new version with (*) appended in filename (e.g., bioproject_result(1).xml)\n",
    "                ##will delete previous one and then rename the just-downloaded one with (*) appended\n",
    "                if existingOutput:\n",
    "                    downloaded_file = max([os.path.join(outputs_dir, f) for f in os.listdir(outputs_dir)], key=os.path.getctime)\n",
    "                    target_file = os.path.join(outputs_dir, 'bioproject_result.xml')\n",
    "                    if os.path.exists(target_file):\n",
    "                        os.remove(target_file)\n",
    "                        print(f'Deleted existing file: {target_file}')\n",
    "                    os.rename(downloaded_file, target_file)\n",
    "                    print(f'Renamed {downloaded_file} to {target_file}')\n",
    "\n",
    "            except TimeoutException:\n",
    "                print('Element not found or not clickable within the specified time.')\n",
    "\n",
    "            finally:\n",
    "                driver.quit()\n",
    "        else:\n",
    "            print('Starting biopython retrieval')\n",
    "            #NCBI requires email to be provided\n",
    "            Entrez.email = f'{email}'\n",
    "\n",
    "            #if you get a free API key, increases rate limit from 3/sec to 10/sec\n",
    "            #Entrez.api_key = 'YOUR_NCBI_API_KEY'\n",
    "\n",
    "            search_term = config['INSTITUTION']['name'] #check that this string is the right one in the web interface\n",
    "            handle = Entrez.esearch(db='bioproject', term=search_term, usehistory='y', retmax=1200) #currently at 955\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            webenv = record['WebEnv']\n",
    "            query_key = record['QueryKey']\n",
    "\n",
    "            handle = Entrez.efetch(db='bioproject', query_key=query_key, WebEnv=webenv, retmode='xml')\n",
    "            xml_data = handle.read().decode('utf-8')\n",
    "            handle.close()\n",
    "\n",
    "            with open(f'{outputs_dir}/bioproject_result.xml', 'w', encoding='utf-8') as f:\n",
    "                f.write(xml_data)\n",
    "\n",
    "            print(f'Saved XML record to \"{outputs_dir}/bioproject_result.xml\"')\n",
    "\n",
    "    #read in XML file (required regardless of whether you downloaded version in this run or not)\n",
    "    print('Loading previously generated XML file.\\n')\n",
    "    with open(f'{outputs_dir}/bioproject_result.xml', 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    #wrapping in a root element for parsing if from Selenium output\n",
    "    if not data.strip().startswith('<?xml'):\n",
    "        data = f'<root>{data}</root>'\n",
    "    root = ET.fromstring(data)\n",
    "\n",
    "    #select certain fields from XML\n",
    "    def filter_ncbi(doc):\n",
    "        data_select = {}\n",
    "        project = doc.find('Project')\n",
    "        if project is not None:\n",
    "            project_id = project.find('ProjectID')\n",
    "            if project_id is not None:\n",
    "                archive_id = project_id.find('ArchiveID')\n",
    "                if archive_id is not None:\n",
    "                    data_select['doi'] = archive_id.get('accession') #this is not a DOI but will be aligned with DOI column in main dataframe\n",
    "                    data_select['repository'] = archive_id.get('archive')\n",
    "                    data_select['ID'] = archive_id.get('id')\n",
    "                center_id = project_id.find('CenterID')\n",
    "                if center_id is not None:\n",
    "                    data_select['Center'] = center_id.get('center')\n",
    "                    data_select['CenterName'] = center_id.text\n",
    "            project_descr = project.find('ProjectDescr')\n",
    "            if project_descr is not None:\n",
    "                name = project_descr.find('Name')\n",
    "                if name is not None:\n",
    "                    data_select['Name'] = name.text\n",
    "                title = project_descr.find('Title')\n",
    "                if title is not None:\n",
    "                    data_select['title'] = title.text\n",
    "                description = project_descr.find('Description')\n",
    "                if description is not None:\n",
    "                    data_select['Description'] = description.text\n",
    "        submission = doc.find('Submission')\n",
    "        if submission is not None:\n",
    "            data_select['LastUpdate'] = submission.get('last_update')\n",
    "            data_select['SubmissionID'] = submission.get('submission_id')\n",
    "            data_select['publication_date'] = submission.get('submitted')\n",
    "            organization = submission.find('.//Organization/Name')\n",
    "            if organization is not None:\n",
    "                data_select['Affiliation'] = organization.text\n",
    "\n",
    "        return data_select\n",
    "\n",
    "    #cxtract data from each element and store in a list\n",
    "    data_list = []\n",
    "    for doc in root.findall('DocumentSummary'):\n",
    "        data_list.append(filter_ncbi(doc))\n",
    "\n",
    "    #dataframe conversion and standardization for alignment with main dataframe\n",
    "    ncbi = pd.DataFrame(data_list)\n",
    "    ncbi['publication_year'] = pd.to_datetime(ncbi['publication_date']).dt.year\n",
    "    ##look for one of the permutation strings listed in config.json\n",
    "    ncbi['first_affiliation'] = ncbi.apply(lambda row: next((perm for perm in ut_variations if perm in row['Affiliation']), None), axis=1)\n",
    "\n",
    "    ##removing hits that have one of the keywords in a different field like the title\n",
    "    ncbi_df_select = ncbi[ncbi['Affiliation'].str.contains(uni_identifier)]\n",
    "    ncbi_df_select = ncbi_df_select[['publication_date', 'repository','doi', 'publication_year', 'title','first_affiliation']]   \n",
    "    ##adding columns for alignment with main dataframe\n",
    "    ncbi_df_select['first_author'] = 'Not specified' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['first_affiliation'] = 'Not specified' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['last_author'] = 'Not specified' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['last_affiliation'] = 'Not specified' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['creators_names'] = 'Not specified' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['contributors_affiliations'] = 'Not specified' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['creators_formatted'] = 'Not specified' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['contributors_names'] = 'No equivalent field' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['contributors_affiliations'] = 'No equivalent field' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['contributors_formatted'] = 'Not applicable' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['relation_type'] = 'No equivalent field' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['related_identifier'] = 'No equivalent field' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['container_identifier'] = 'No equivalent field' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['type'] = 'Dataset'\n",
    "    ncbi_df_select['subjects'] = 'No keyword information' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['deposit_size'] = 'No file size information' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['formats'] = 'No file size information' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['file_count'] = 'No file size information' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['rights'] = 'Rights unclear' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['rights_code'] = 'Rights unclear' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['views'] = 'No metrics information' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['downloads'] = 'No metrics information' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['citations'] = 'No metrics information' #filler to match DataCite, no equivalent field\n",
    "    ncbi_df_select['source'] = 'NCBI'\n",
    "    ncbi_df_select['affiliation_source'] = 'Not applicable' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['affiliation_permutation'] = 'University of Texas at Austin' #standardized for search\n",
    "    ncbi_df_select['had_partial_duplicate'] = 'Not applicable' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['file_format'] = 'No file size information' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['contains_code'] = 'No file size information' #filler to match DataCite processing, no equivalent field\n",
    "    ncbi_df_select['only_code'] = 'No file size information' #filler to match DataCite processing, no equivalent field\n",
    "\n",
    "    #select metadata assessment for titles\n",
    "    ncbi_df_select['title_reformatted'] = ncbi_df_select['title'].str.replace('_', ' ') #gets around text linked by underscores counting as 1 word\n",
    "    ncbi_df_select['title_reformatted'] = ncbi_df_select['title_reformatted'].str.lower()\n",
    "    ncbi_df_select[['total_word_count_title', 'descriptive_word_count_title']] = (ncbi_df_select['title_reformatted'].apply(lambda x: pd.Series(count_words(x, nondescriptive_words))))\n",
    "    ncbi_df_select['descriptive_word_count_title'] = ncbi_df_select.apply(adjust_descriptive_count, axis=1)\n",
    "    ncbi_df_select['nondescriptive_word_count_title'] = ncbi_df_select['total_word_count_title'] - ncbi_df_select['descriptive_word_count_title']\n",
    "    ncbi_df_select['rights_standardized'] = 'Rights unclear'\n",
    "    ncbi_df_select['repository2'] = 'NCBI'\n",
    "    ncbi_df_select['uni_lead'] = 'Affiliated (authorship unclear)'    \n",
    "    ncbi_df_select['non_TDR_IR'] = 'not university or TDR'\n",
    "    ncbi_df_select['US_federal'] = 'Federal US repo'\n",
    "    ncbi_df_select['GREI'] = 'not GREI member'\n",
    "    ncbi_df_select['scope'] = 'Specialist'\n",
    "    ncbi_df_select['type_reclassified'] = 'Dataset'\n",
    "    ncbi_df_select['doi_article'] = 'Not applicable' #filler to match Figshare workflow, no equivalent process or field\n",
    "    ncbi_df_select['title_article'] = 'Not applicable' #filler to match Figshare workflow, no equivalent process or field\n",
    "    ncbi_df_select['publication_year'] = 'Not applicable' #filler to match Figshare workflow, no equivalent process or field\n",
    "    ncbi_df_select['journal'] = 'Not applicable' #filler to match Figshare workflow, no equivalent process or field\n",
    "    ncbi_df_select['related_identifier_type'] = 'Not applicable' #filler to match Figshare workflow, no equivalent process or field\n",
    "\n",
    "    if load_previous_data_plus:\n",
    "        #for reading in previously generated file of all associated datasets\n",
    "        print('Reading in existing DataCite+ output file\\n')\n",
    "        directory = './outputs' \n",
    "        pattern = '_full-concatenated-dataframe-plus-figshare.csv'\n",
    "\n",
    "        files = os.listdir(directory)\n",
    "        files.sort(reverse=True)\n",
    "        latest_file = None\n",
    "        for file in files:\n",
    "            if pattern in file:\n",
    "                latest_file = file\n",
    "                break\n",
    "\n",
    "        if latest_file:\n",
    "            file_path = os.path.join(directory, latest_file)\n",
    "            df_datacite_plus_dedup = pd.read_csv(file_path)\n",
    "            print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "        else:\n",
    "            print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    df_datacite_plus_ncbi = pd.concat([df_datacite_plus_dedup, ncbi_df_select], ignore_index=True)\n",
    "    df_datacite_plus_ncbi.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-figshare-ncbi.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb5628",
   "metadata": {},
   "source": [
    "### Step 5: Concatenating with externally produced Crossref results\n",
    "\n",
    "If you made a separate Crossref query using the provided accessory script, the results will need to be pulled in to be concatenated with the DataCite results. This step will read in the Crossref results and concatenate them with the DataCite results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if any([load_previous_data, load_previous_data_plus, load_previous_data_plus_ncbi]) and load_crossref:\n",
    "    print('Reading in existing DataCite+ output file\\n')\n",
    "    directory = './outputs'\n",
    "    if load_previous_data_plus_ncbi: \n",
    "        pattern = '_full-concatenated-dataframe-plus-figshare-ncbi.csv'\n",
    "    elif load_previous_data_plus:\n",
    "        pattern = '_full-concatenated-dataframe-plus.csv'\n",
    "    elif load_previous_data:\n",
    "        pattern = '_full-concatenated-dataframe.csv'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(reverse=True)\n",
    "    latest_file = None\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            latest_file = file\n",
    "            break\n",
    "\n",
    "    if latest_file:\n",
    "        file_path = os.path.join(directory, latest_file)\n",
    "        df_datacite_plus_ncbi = pd.read_csv(file_path)\n",
    "        print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "    else:\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    #set path for browser\n",
    "    print('\\nReading in existing Crossref output file\\n')\n",
    "\n",
    "    directory = './accessory-scripts/accessory-outputs'\n",
    "    pattern = 'true-datasets'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(reverse=True)\n",
    "    latest_file = None\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            latest_file = file\n",
    "            break\n",
    "\n",
    "    if latest_file:\n",
    "        file_path = os.path.join(directory, latest_file)\n",
    "        crossref_true_datasets = pd.read_csv(file_path)\n",
    "        print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "    else:\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    if load_previous_data:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_pruned, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-crossref.csv', index=False, encoding='utf-8')\n",
    "    elif load_previous_data_plus:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-figshare-crossref.csv', index=False, encoding='utf-8')\n",
    "    elif load_previous_data_plus_ncbi:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus_ncbi, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-figshare-ncbi-crossref.csv', index=False, encoding='utf-8')\n",
    "    elif not load_previous_data and not load_previous_data and not figshare_workflow_1:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_pruned, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-crossref.csv', index=False, encoding='utf-8')\n",
    "    elif not load_previous_data and not load_previous_data and figshare_workflow_1:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-figshare-crossref.csv', index=False, encoding='utf-8')\n",
    "\n",
    "if not any([load_previous_data, load_previous_data_plus, load_previous_data_plus_ncbi]) and load_crossref:\n",
    "    print('\\nReading in existing Crossref output file\\n')\n",
    "    directory = './accessory-scripts/accessory-outputs'\n",
    "    pattern = 'true-datasets'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(reverse=True)\n",
    "    latest_file = None\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            latest_file = file\n",
    "            break\n",
    "\n",
    "    if latest_file:\n",
    "        file_path = os.path.join(directory, latest_file)\n",
    "        crossref_true_datasets = pd.read_csv(file_path)\n",
    "        print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "    else:\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    if not df_datacite_pruned.empty and df_datacite_plus_dedup.empty:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_pruned, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-crossref.csv', index=False, encoding='utf-8')\n",
    "    elif not df_datacite_plus_dedup.empty and df_datacite_plus_ncbi.empty:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-figshare-crossref.csv', index=False, encoding='utf-8')\n",
    "    elif not df_datacite_plus_ncbi.empty:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus_ncbi, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{today}_{resource_filename}_full-concatenated-dataframe-plus-figshare-ncbi-crossref.csv', index=False, encoding='utf-8')\n",
    "\n",
    "runtime = datetime.now() - start_time\n",
    "\n",
    "print('Dataframe processing completed, beginning log writing.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf15a5",
   "metadata": {},
   "source": [
    "# Step 6: Logging run details\n",
    "\n",
    "Logging functionality is still being developed, but the present script is configured to write a text file that records all of the parameters that are controllable via the config file, along with runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_selected_config(config, keys, file):\n",
    "    for key in keys:\n",
    "        value = config.get(key)\n",
    "        if value is not None:\n",
    "            if isinstance(value, dict):\n",
    "                # Pretty-print the nested dict\n",
    "                file.write(f'{key}:\\n{pformat(value, indent=2, width=120)}\\n\\n')\n",
    "            else:\n",
    "                file.write(f'{key}: {value}\\n\\n')\n",
    "\n",
    "#writes one file specific to this run:\n",
    "unique_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "with open(f'logs/{unique_timestamp}-log.txt', 'w') as resultssummaryfile:\n",
    "    resultssummaryfile.write(f'Affiliated research object discovery for: {config['INSTITUTION']['name']}, run on {start_timezone_formatted} for {runtime} (hours:minutes:seconds.milliseconds).\\n\\n')\n",
    "    resultssummaryfile.write(f'User: {config['EMAIL']['user_email']}\\n\\n')\n",
    "\n",
    "    env = 'test' if config['TOGGLES']['test'] else 'production'\n",
    "    cross = 'with cross-validation' if config['TOGGLES']['cross_validate'] else 'without cross-validation'\n",
    "    dataverse = 'included the Dataverse API' if config['TOGGLES']['dataverse_duplicates'] else 'did not include the Dataverse API'\n",
    "    dataversededup = 'with dataverse deduplication' if config['TOGGLES']['dataverse_duplicates'] else 'without dataverse deduplication'\n",
    "    figshare1 = 'with the first secondary figshare workflow' if config['TOGGLES']['figshare_workflow_1'] else 'without the first secondary figshare workflow'\n",
    "    figshare2 = 'with the second secondary figshare workflow' if config['TOGGLES']['figshare_workflow_2'] else 'without the second secondary figshare workflow'\n",
    "    figshareVers = 'removing versions for multi-version deposits' if config['TOGGLES']['figshare_versions'] else 'retaining versions for multi-version deposits'\n",
    "    ncbi = 'with the secondary NCBI workflow' if config['TOGGLES']['ncbi_workflow'] else 'without the secondary NCBI workflow'\n",
    "    biopy = 'using the biopython module' if config['TOGGLES']['biopython'] else 'using the Selenium approach'\n",
    "    loadPrev = 'previous primary output was loaded' if config['TOGGLES']['load_previous_data'] else 'previous primary output was not loaded'\n",
    "    loadPrevPlus = 'previous primary output with secondary Figshare data was loaded' if config['TOGGLES']['load_previous_data_plus'] else 'previous primary output with secondary Figshare data was not loaded'\n",
    "    loadPrevPlusNCBI = 'previous primary output with secondary Figshare and NCBI data was loaded' if config['TOGGLES']['load_previous_data_plus_ncbi'] else 'previous primary output with secondary Figshare and NCBI data was not loaded'\n",
    "    loadCross = 'separate Crossref output was loaded' if config['TOGGLES']['load_crossref'] else 'separate Crossref output was not loaded'\n",
    "\n",
    "    resultssummaryfile.write(f'Short summary: The script was run in {env} mode and applied a filter to search for {resource_filename} objects. The initial search was performed {cross}, which was itself performed {dataverse} and {dataversededup}. The script was run {figshare1}, with a filter to search for {figshare_resource_filename}; {figshare2}; and {ncbi}, {biopy}. The {loadPrev}; the {loadPrevPlus}; the {loadPrevPlusNCBI}; and the {loadCross}. \\n\\n')\n",
    "\n",
    "    #writes select fields from the config.json file\n",
    "    fields_to_log = ['TOGGLES', 'VARIABLES', 'PERMUTATIONS', 'FIGSHARE_PARTNERS']\n",
    "    log_selected_config(config, fields_to_log, resultssummaryfile)\n",
    "    resultssummaryfile.write('\\n')\n",
    "\n",
    "#writes to master CSV file\n",
    "##ensuring it writes to the same file regardless of env\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "comp_log_dir = os.path.join(script_dir, 'logs')\n",
    "comp_log_file = os.path.join(comp_log_dir, 'composite-log.csv')\n",
    "file_exists = os.path.exists(comp_log_file)\n",
    "\n",
    "log_entry = {\n",
    "    'script_name': os.path.basename(__file__),\n",
    "    'timestamp': start_timezone_formatted,\n",
    "    'runtime': runtime,\n",
    "    'institution': config['INSTITUTION']['name'],\n",
    "    'user': config['EMAIL']['user_email'],\n",
    "    'test_mode': config['TOGGLES']['test'],\n",
    "    'cross-validation': config['TOGGLES']['cross_validate'],\n",
    "    'dataverse_deduplication': config['TOGGLES']['dataverse_duplicates'],\n",
    "    'figshare1': config['TOGGLES']['figshare_workflow_1'],\n",
    "    'figshare2': config['TOGGLES']['figshare_workflow_2'],\n",
    "    'figshare_versions': config['TOGGLES']['figshare_versions'],\n",
    "    'ncbi': config['TOGGLES']['ncbi_workflow'],\n",
    "    'ncbi_method':'Not applicable'\n",
    "        if not config['TOGGLES']['ncbi_workflow']\n",
    "        else 'BioPython'\n",
    "        if config['TOGGLES']['biopython']\n",
    "        else 'Selenium',\n",
    "    'loadedPrevious': config['TOGGLES']['load_previous_data'],\n",
    "    'loadedPreviousPlus': config['TOGGLES']['load_previous_data_plus'],\n",
    "    'loadedPreviousPlusNCBI': config['TOGGLES']['load_previous_data_plus_ncbi'],\n",
    "    'loadedCrossref': config['TOGGLES']['load_crossref']\n",
    "}\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(comp_log_file)\n",
    "    df = pd.concat([df, pd.DataFrame([log_entry])], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame([log_entry])\n",
    "\n",
    "df.to_csv(comp_log_file, index=False)\n",
    "\n",
    "print('Logging completed. Script completed.\\n')\n",
    "\n",
    "print(f'Time to run: {datetime.now() - start_time}')\n",
    "if test:\n",
    "    print('**REMINDER: THIS IS A TEST RUN, AND ANY RESULTS ARE NOT COMPLETE!**')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
