{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4d8190",
   "metadata": {},
   "source": [
    "# Scripted process for retrieving metadata on institutional-affiliated research dataset publications\n",
    "\n",
    "## Metadata\n",
    "* *Version*: 3.7.0\n",
    "* *Released*: 2025/06/24 (end of sprint 5)\n",
    "* *Author(s)*: Bryan Gee (UT Libraries, University of Texas at Austin; bryan.gee@austin.utexas.edu; ORCID: [0000-0003-4517-3290](https://orcid.org/0000-0003-4517-3290))\n",
    "* *Contributor(s)*: None\n",
    "* *License*: [MIT](https://opensource.org/license/mit)\n",
    "* *README last updated*: 2025/06/24 (end of sprint 5)\n",
    "\n",
    "**For more information and supporting documentation, see the [GitHub repository](https://github.com/utlibraries/research-data-discovery)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89203b94",
   "metadata": {},
   "source": [
    "## Core packages\n",
    "\n",
    "The following packages are required to run both the core and secondary parts of this workflow; all of them should either be pre-installed with Python or can be installed via *pip*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs, quote\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf81433",
   "metadata": {},
   "source": [
    "## Toggles\n",
    "\n",
    "The following toggles can be set to `True` or `False` to enable or disable certain parts of the workflow. For the NCBI workflow, additional packages will need to be installed and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fa9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#operator for quick test runs\n",
    "test = False\n",
    "#operator for resource type(s) to query for (use OR and put in parentheses for multiple types)\n",
    "resourceType = '(Dataset OR Software)'\n",
    "#toggle for cross-validation steps\n",
    "crossValidate = True\n",
    "##toggle for Dataverse cross-validation specifically\n",
    "dataverse = True\n",
    "##toggle for de-duplicating partial Dataverse replicates (multiple deposits for one manuscript within one dataverse) - see README for details\n",
    "dataverseDuplicates = False\n",
    "##toggle for UT Austin specific edge cases (set to False if you are not at UT Austin)\n",
    "austin = True\n",
    "\n",
    "#toggles for executing Figshare processes (see README for details)\n",
    "##looking for datasets with a journal publisher listed as publisher, X-ref'ing with university articles from that publisher\n",
    "figshareWorkflow1 = True\n",
    "##finding university articles from publisher that uses certain formula for Figshare DOIs, construct hypothetical DOI, test if it exists\n",
    "figshareWorkflow2 = False\n",
    "\n",
    "##if you have done a previous DataCite retrieval and don't want to re-run the entire main process (skip to Figshare steps)\n",
    "loadPreviousData = False\n",
    "#if you have done a previous DataCite retrieval and Figshare workflow 1 and don't want to re-run these\n",
    "loadPreviousDataPlus = False\n",
    "#toggle for executing NCBI process\n",
    "ncbiWorkflow = True\n",
    "##loading package in only if running NCBI workflow\n",
    "if ncbiWorkflow:\n",
    "    import xml.etree.ElementTree as ET\n",
    "#toggle for whether to use biopython approach to NCBI (TRUE = biopython; FALSE = Selenium)\n",
    "biopython = True\n",
    "##loading packages in only if running NCBI workflow and depending on selection\n",
    "if biopython and ncbiWorkflow:\n",
    "    from Bio import Entrez\n",
    "elif not biopython and ncbiWorkflow:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException\n",
    "    from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "#toggle for skipping web retrieval of NCBI data (just XML to dataframe conversion)\n",
    "loadNCBIdata = False\n",
    "#toggle for loading previous DataCite + Figshare workflow 1 + NCBI\n",
    "loadPreviousDataPlusNCBI = False\n",
    "#toggle to load in externally generated Crossref data\n",
    "loadCrossrefData = True\n",
    "\n",
    "#conditional toggles, if loading in previous data, automatically set certain other toggles to False regardless of how they are set\n",
    "##should minimize how much you need to edit multiple toggles (WIP)\n",
    "if loadPreviousDataPlus:\n",
    "    figshareWorkflow1 = False\n",
    "    figshareWorkflow2 = False\n",
    "if loadPreviousDataPlusNCBI:\n",
    "    figshareWorkflow1 = False\n",
    "    figshareWorkflow2 = False\n",
    "    ncbiWorkflow = False\n",
    "\n",
    "#read in config file\n",
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d1457",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The following codeblock defines various parameters that are used in API calls, sets up the various working directories, and pulls in the *config.json* file, which provides additional parameters. API keys and institution-specific parameters will need to be set in the *config.json* file. If you want to change certain parameters for a given API, refer to the official API documentation; in most instances, the ones included in the *config-template.json* file are set at or near the maximum allowed except for the total number of pages/results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c297dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating directories\n",
    "if test:\n",
    "    if os.path.isdir('test'):\n",
    "        print('test directory found - no need to recreate')\n",
    "    else:\n",
    "        os.mkdir('test')\n",
    "        print('test directory has been created')\n",
    "    os.chdir('test')\n",
    "    if os.path.isdir('outputs'):\n",
    "        print('test outputs directory found - no need to recreate')\n",
    "    else:\n",
    "        os.mkdir('outputs')\n",
    "        print('test outputs directory has been created')\n",
    "else:\n",
    "    if os.path.isdir('outputs'):\n",
    "        print('outputs directory found - no need to recreate')\n",
    "    else:\n",
    "        os.mkdir('outputs')\n",
    "        print('outputs directory has been created')\n",
    "\n",
    "#setting timestamp to calculate run time\n",
    "startTime = datetime.now() \n",
    "#creating variable with current date for appending to filenames\n",
    "todayDate = datetime.now().strftime('%Y%m%d') \n",
    "\n",
    "#read in email address for polite requests (required for biopython NCBI workflow, can be used for other APIs)\n",
    "email = config['EMAIL']['user_email']\n",
    "\n",
    "#create permutation string with OR for API parameters\n",
    "ut_variations = config['PERMUTATIONS']\n",
    "institution_query = ' OR '.join([f'\"{variation}\"' for variation in ut_variations])\n",
    "\n",
    "#pull in ROR ID (necessary for Dryad API)\n",
    "ror_id = config['INSTITUTION']['ror']\n",
    "\n",
    "#pulling in 'uniqueIdentifer' term used as quick, reliable filter ('Austin' for filtering an affiliation field for UT Austin)\n",
    "uni_identifier = config['INSTITUTION']['uniqueIdentifier']\n",
    "\n",
    "#API endpoints\n",
    "url_crossref = 'https://api.crossref.org/works/'\n",
    "url_crossref_issn = 'https://api.crossref.org/journals/{issn}/works'\n",
    "url_dryad = f'https://datadryad.org/api/v2/search?affiliation={ror_id}' #Dryad requires ROR for affiliation search\n",
    "url_datacite = 'https://api.datacite.org/dois'\n",
    "url_dataverse = 'https://dataverse.tdl.org/api/search/'\n",
    "url_figshare = 'https://api.figshare.com/v2/articles/{id}/files?page_size=10'\n",
    "url_openalex = 'https://api.openalex.org/works'\n",
    "url_zenodo = 'https://zenodo.org/api/records'\n",
    "\n",
    "params_dryad= {\n",
    "    'per_page': config['VARIABLES']['PAGE_SIZES']['dryad'],\n",
    "}\n",
    "\n",
    "params_datacite = {\n",
    "    'affiliation': 'true',\n",
    "    'query': f'(creators.affiliation.name:({institution_query}) OR creators.name:({institution_query}) OR contributors.affiliation.name:({institution_query}) OR contributors.name:({institution_query})) AND types.resourceTypeGeneral:{resourceType}',\n",
    "    'page[size]': config['VARIABLES']['PAGE_SIZES']['datacite'],\n",
    "    'page[cursor]': 1,\n",
    "}\n",
    "\n",
    "headers_dataverse = {\n",
    "    'X-Dataverse-key': config['KEYS']['dataverseToken']\n",
    "}\n",
    "params_dataverse = {\n",
    "    'q': '10.18738/T8/',\n",
    "    #UT Austin dataverse, may contain non-UT affiliated objects, and UT-affiliated objects may be in other TDR installations\n",
    "    #'subtree': 'utexas', \n",
    "    'type': 'dataset',\n",
    "    'start': config['VARIABLES']['PAGE_STARTS']['dataverse'],\n",
    "    'page': config['VARIABLES']['PAGE_INCREMENTS']['dataverse'],\n",
    "    'per_page': config['VARIABLES']['PAGE_SIZES']['dataverse']\n",
    "}\n",
    "\n",
    "params_zenodo_data = {\n",
    "    'q': f'(creators.affiliation:({institution_query}) OR creators.name:({institution_query}) OR contributors.affiliation:({institution_query}) OR contributors.name:({institution_query}))',\n",
    "    'size': config['VARIABLES']['PAGE_SIZES']['zenodo'],\n",
    "    'type': 'dataset',\n",
    "    'access_token': config['KEYS']['zenodoToken']\n",
    "}\n",
    "\n",
    "#define different number of pages to retrieve from DataCite API based on 'test' vs. 'prod' env\n",
    "page_limit_datacite = config['VARIABLES']['PAGE_LIMITS']['datacite_test'] if test else config['VARIABLES']['PAGE_LIMITS']['datacite_prod']\n",
    "page_limit_zenodo = config['VARIABLES']['PAGE_LIMITS']['zenodo_test'] if test else config['VARIABLES']['PAGE_LIMITS']['zenodo_prod']\n",
    "page_limit_openalex = config['VARIABLES']['PAGE_LIMITS']['openalex_test'] if test else config['VARIABLES']['PAGE_LIMITS']['openalex_prod']\n",
    "\n",
    "#define variables to be called recursively in function\n",
    "page_start_datacite = config['VARIABLES']['PAGE_STARTS']['datacite']\n",
    "page_start_zenodo = config['VARIABLES']['PAGE_STARTS']['zenodo']\n",
    "page_size_zenodo = config['VARIABLES']['PAGE_SIZES']['zenodo']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34f13d",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "The following codeblock defines various functions that are used throughout the workflow. These functions handle API calls, data processing, and other tasks. The functions are designed to be modular and reusable, allowing for easy updates and modifications as needed. Some of them are only used once in this script but may have been incorporated into, or taken from, other scripts in this repository or from other repositories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b30849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define global functions\n",
    "##retrieves single page of results\n",
    "def retrieve_page_dryad(url, params):\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  \n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error retrieving page: {e}\")\n",
    "        return {'_embedded': {'stash:datasets': []}, 'total': {}}\n",
    "##recursively retrieves pages\n",
    "def retrieve_all_data_dryad(url, params):\n",
    "    page_start_dryad = config['VARIABLES']['PAGE_STARTS']['dryad']\n",
    "    all_data_dryad = []\n",
    "    data = retrieve_page_dryad(url, params)\n",
    "    total_count = data.get('total', None)\n",
    "    total_pages = math.ceil(total_count/params_dryad['per_page'])\n",
    "\n",
    "    print(f\"Total: {total_count} entries over {total_pages} pages\\n\")\n",
    "\n",
    "    while True:\n",
    "        params.update({\"page\": page_start_dryad})  \n",
    "        print(f\"Retrieving page {page_start_dryad} of {total_pages} from Dryad...\\n\")  \n",
    "\n",
    "        data = retrieve_page_dryad(url, params)\n",
    "        \n",
    "        if not data['_embedded']:\n",
    "            print(\"No data found.\")\n",
    "            return all_data_dryad\n",
    "        \n",
    "        all_data_dryad.extend(data['_embedded']['stash:datasets'])\n",
    "        \n",
    "        page_start_dryad += 1\n",
    "\n",
    "        if not data['_embedded']['stash:datasets']:\n",
    "            print(\"End of Dryad response.\\n\")          \n",
    "            break\n",
    "    \n",
    "    return all_data_dryad\n",
    "\n",
    "##retrieves single page of results\n",
    "def retrieve_page_datacite(url, params=None):\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  \n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error retrieving page: {e}\")\n",
    "        return {'data': [], 'links': {}}\n",
    "##recursively retrieves pages\n",
    "def retrieve_all_data_datacite(url, params):\n",
    "    global page_start_datacite\n",
    "    all_data_datacite = []\n",
    "    data = retrieve_page_datacite(url, params)\n",
    "    \n",
    "    if not data['data']:\n",
    "        print(\"No data found.\")\n",
    "        return all_data_datacite\n",
    "\n",
    "    all_data_datacite.extend(data['data'])\n",
    "\n",
    "    total_count = data.get('meta', {}).get('total', None)\n",
    "    total_pages = math.ceil(total_count/config['VARIABLES']['PAGE_SIZES']['datacite'])\n",
    "\n",
    "    current_url = data.get('links', {}).get('next', None)\n",
    "    \n",
    "    while current_url and page_start_datacite < page_limit_datacite:\n",
    "        page_start_datacite+=1\n",
    "        print(f\"Retrieving page {page_start_datacite} of {total_pages} from DataCite...\\n\")\n",
    "        data = retrieve_page_datacite(current_url)\n",
    "        \n",
    "        if not data['data']:\n",
    "            print(\"End of response.\")\n",
    "            break\n",
    "        \n",
    "        all_data_datacite.extend(data['data'])\n",
    "        \n",
    "        current_url = data.get('links', {}).get('next', None)\n",
    "    \n",
    "    return all_data_datacite\n",
    "\n",
    "##retrieves single page of results\n",
    "def retrieve_page_dataverse(url, params=None, headers=None):\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status() \n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error retrieving page: {e}\")\n",
    "        return {'data': {'items': [], 'total_count': 0}}\n",
    "##recursively retrieves pages\n",
    "def retrieve_all_data_dataverse(url, params, headers):\n",
    "    all_data_dataverse = []\n",
    "\n",
    "    while True: \n",
    "        data = retrieve_page_dataverse(url, params, headers)  \n",
    "        total_count = data['data']['total_count']\n",
    "        total_pages = math.ceil(total_count/params_dataverse['per_page'])\n",
    "        print(f\"Retrieving page {params_dataverse['page']} of {total_pages} pages...\\n\")\n",
    "\n",
    "        if not data['data']:\n",
    "            print(\"No data found.\")\n",
    "            break\n",
    "    \n",
    "        all_data_dataverse.extend(data['data']['items'])\n",
    "        \n",
    "        params_dataverse['start'] += params_dataverse['per_page']\n",
    "        params_dataverse['page'] += 1\n",
    "        \n",
    "        if params_dataverse['start'] >= total_count:\n",
    "            print(\"End of response.\")\n",
    "            break\n",
    "\n",
    "    return all_data_dataverse\n",
    "\n",
    "##retrieves single page of results\n",
    "def retrieve_page_zenodo(url, params=None):\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  \n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error retrieving page: {e}\")\n",
    "        return {'hits': {'hits': [], 'total':{}}, 'links': {}}\n",
    "##extracting the 'page' parameter value\n",
    "def extract_page_number(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    return query_params.get('page', [None])[0]  \n",
    "##recursively retrieves pages\n",
    "def retrieve_all_data_zenodo(url, params):\n",
    "    page_start_zenodo = config['VARIABLES']['PAGE_STARTS']['zenodo']\n",
    "    all_data_zenodo = []\n",
    "    data = retrieve_page_zenodo(url, params)\n",
    "    \n",
    "    if not data['hits']['hits']:\n",
    "        print(\"No data found.\")\n",
    "        return all_data_zenodo\n",
    "\n",
    "    all_data_zenodo.extend(data['hits']['hits'])\n",
    "    \n",
    "    current_url = data.get('links', {}).get('self', None)\n",
    "    total_count = data.get('hits', {}).get('total',None)\n",
    "    total_pages = math.ceil(total_count/config['VARIABLES']['PAGE_SIZES']['zenodo'])\n",
    "\n",
    "    print(f\"Total: {total_count} entries over {total_pages} pages\\n\")\n",
    "    \n",
    "    while current_url and page_start_zenodo < page_limit_zenodo:\n",
    "        page_start_zenodo+=1\n",
    "        page_number = extract_page_number(current_url)\n",
    "        print(f\"Retrieving page {page_start_zenodo} of {total_pages} from Zenodo...\\n\")\n",
    "        data = retrieve_page_zenodo(current_url)\n",
    "        \n",
    "        if not data['hits']['hits']:\n",
    "            print(\"End of Zenodo response.\\n\")\n",
    "            break\n",
    "        \n",
    "        all_data_zenodo.extend(data['hits']['hits'])\n",
    "        \n",
    "        current_url = data.get('links', {}).get('next', None)\n",
    "    \n",
    "    return all_data_zenodo\n",
    "\n",
    "##retrieves single page of results\n",
    "def retrieve_page_openalex(url, params=None):\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  \n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error retrieveing page: {e}\")\n",
    "        return {'results': [], 'meta':{}}\n",
    "##recursively retrieves pages\n",
    "def retrieve_all_data_openalex(url, params):\n",
    "    global j\n",
    "    all_data_openalex = []\n",
    "    data = retrieve_page_openalex(url, params)\n",
    "    params = params_openalex.copy()\n",
    "    params['cursor'] = '*'\n",
    "    next_cursor = '*'\n",
    "    previous_cursor = None\n",
    "    \n",
    "    if not data['results']:\n",
    "        print(\"No data found.\")\n",
    "        return all_data_openalex\n",
    "\n",
    "    all_data_openalex.extend(data['results'])\n",
    "    \n",
    "    total_count = data.get('meta', {}).get('count', None)\n",
    "    per = data.get('meta', {}).get('per_page', None)\n",
    "    total_pages = math.ceil(total_count/per) + 1\n",
    "\n",
    "    print(f\"Total: {total_count} entries over {total_pages} pages\")\n",
    "    print()\n",
    "\n",
    "    while j < page_limit_openalex:\n",
    "        j += 1\n",
    "        print(f\"Retrieving page {j} of {total_pages} from OpenAlex...\")\n",
    "        print()\n",
    "        data = retrieve_page_openalex(url, params)\n",
    "        next_cursor = data.get('meta', {}).get('next_cursor', None)\n",
    "\n",
    "        if next_cursor == previous_cursor:\n",
    "            print(\"Cursor did not change. Ending loop to avoid infinite loop.\")\n",
    "            break\n",
    "        \n",
    "        if not data['results']:\n",
    "            print(\"End of OpenAlex response.\")\n",
    "            print()\n",
    "            break\n",
    "        \n",
    "        all_data_openalex.extend(data['results'])\n",
    "        \n",
    "        previous_cursor = next_cursor\n",
    "        params['cursor'] = next_cursor\n",
    "    \n",
    "    return all_data_openalex\n",
    "\n",
    "##retrieves single page of results\n",
    "def retrieve_page_crossref(url, params=None):\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  \n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error retrieving page: {e}\")\n",
    "        return {'message': {'items': [], 'total-results':{}}}\n",
    "##recursively retrieves pages\n",
    "def retrieve_all_data_crossref(url, params):\n",
    "    global k\n",
    "\n",
    "    all_data_crossref = []\n",
    "    data = retrieve_page_crossref(url, params)\n",
    "    params = params_crossref_journal.copy()\n",
    "    params['cursor'] = '*'\n",
    "    next_cursor = '*'\n",
    "    previous_cursor = None\n",
    "    \n",
    "    if not data['message']['items']:\n",
    "        print(\"No data found.\")\n",
    "        return all_data_crossref\n",
    "\n",
    "    all_data_crossref.extend(data['message']['items'])\n",
    "    \n",
    "    while k < page_limit_crossref:\n",
    "        k += 1\n",
    "        print(f\"Retrieving page {k} from CrossRef...\")\n",
    "        print()\n",
    "\n",
    "        data = retrieve_page_crossref(url, params)\n",
    "        next_cursor = data.get('message', {}).get('next-cursor', None)\n",
    "        \n",
    "        if not data['message']['items']:\n",
    "            print(\"Finished this journal.\")\n",
    "            print()\n",
    "            break\n",
    "        \n",
    "        all_data_crossref.extend(data['message']['items'])\n",
    "        \n",
    "        previous_cursor = next_cursor\n",
    "        params['cursor'] = next_cursor\n",
    "    \n",
    "    return all_data_crossref\n",
    "\n",
    "#determines which author (first vs. last or both) is affiliated\n",
    "def determine_affiliation(row):\n",
    "    if row['first_author'] == row['last_author']:\n",
    "        return 'single author'\n",
    "\n",
    "    first_affiliated = any(variation in (row['first_affiliation'] or '') for variation in ut_variations)\n",
    "    last_affiliated = any(variation in (row['last_affiliation'] or '') for variation in ut_variations)\n",
    "\n",
    "    if first_affiliated and last_affiliated:\n",
    "        return 'both lead and senior'\n",
    "    elif first_affiliated and not last_affiliated:\n",
    "        return 'only lead'\n",
    "    elif last_affiliated and not first_affiliated:\n",
    "        return 'only senior'\n",
    "    else:\n",
    "        return 'neither lead nor senior'\n",
    "\n",
    "#recursively retrieves specified journals in Crossref API\n",
    "def retrieve_all_journals(url_template, journal_list):\n",
    "    all_data = []  \n",
    "\n",
    "    for journal_name, issn in journal_list.items():\n",
    "        print(f'Retrieving data from {journal_name} (ISSN: {issn})')\n",
    "        custom_url = url_template.format(issn=issn)\n",
    "        params = params_crossref_journal.copy()\n",
    "        params['filter'] += f',issn:{issn}'\n",
    "        \n",
    "        journal_data = retrieve_all_data_crossref(custom_url, params)\n",
    "        all_data.extend(journal_data)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "#checks if hypothetical DOI exists\n",
    "def check_link(doi):\n",
    "    url = f'https://doi.org/{doi}'\n",
    "    response = requests.head(url, allow_redirects=True)\n",
    "    return response.status_code == 200\n",
    "\n",
    "#function to count descriptive words\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    descriptive_count = sum(1 for word in words if word not in nondescriptive_words)\n",
    "    return total_words, descriptive_count\n",
    "\n",
    "## account for when a single word may or may not be descriptive but is certainly uninformative if in a certain combination\n",
    "def adjust_descriptive_count(row):\n",
    "    if ('supplemental material' in row['title_reformatted'].lower() or\n",
    "            'supplementary material' in row['title_reformatted'].lower() or\n",
    "            'supplementary materials' in row['title_reformatted'].lower() or\n",
    "            'supplemental materials' in row['title_reformatted'].lower()):\n",
    "        return max(0, row['descriptive_word_count_title'] - 1)\n",
    "    return row['descriptive_word_count_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ecffb",
   "metadata": {},
   "source": [
    "## Primary workflow\n",
    "### Step 1: Initial affiliation-based DataCite query\n",
    "\n",
    "This step queries the DataCite API for publications affiliated with the institution. The query is based on the various permutations of the institution's name as defined in the *config.json* file and searches for any one of those permutations across four DataCite metadata fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a9ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadPreviousData and not loadPreviousDataPlus and not loadPreviousDataPlusNCBI:\n",
    "    print('Starting DataCite retrieval based on affiliation.\\n')\n",
    "    data_datacite = retrieve_all_data_datacite(url_datacite, params_datacite)\n",
    "    print(f'Number of datasets found by DataCite API: {len(data_datacite)}\\n')\n",
    "\n",
    "    if crossValidate:\n",
    "        print('Starting Dryad retrieval.\\n')\n",
    "        data_dryad = retrieve_all_data_dryad(url_dryad, params_dryad)\n",
    "        print(f'Number of Dryad datasets found by Dryad API: {len(data_dryad)}\\n')\n",
    "        if dataverse:\n",
    "            print('Starting Dataverse retrieval.\\n')\n",
    "            data_dataverse = retrieve_all_data_dataverse(url_dataverse, params_dataverse, headers_dataverse)\n",
    "            print(f'Number of Dataverse datasets found by Dataverse API: {len(data_dataverse)}\\n')\n",
    "        print('Starting Zenodo retrieval.\\n')\n",
    "        data_zenodo = retrieve_all_data_zenodo(url_zenodo, params_zenodo_data)\n",
    "        print(f'Number of Zenodo datasets found by Zenodo API: {len(data_zenodo)}\\n')\n",
    "\n",
    "    print('Beginning dataframe generation.\\n')\n",
    "\n",
    "    data_select_datacite = [] \n",
    "    for item in data_datacite:\n",
    "        attributes = item.get('attributes', {})\n",
    "        doi = attributes.get('doi', None)\n",
    "        state = attributes.get('state', None)\n",
    "        publisher = attributes.get('publisher', '')\n",
    "        # publisher_year = attributes.get('publicationYear', '') #temporarily disabling due to Dryad metadata issue\n",
    "        registered = attributes.get('registered', '')\n",
    "        if registered:\n",
    "            publisher_year = datetime.fromisoformat(registered[:-1]).year\n",
    "            publisher_date = datetime.fromisoformat(registered[:-1]).date()\n",
    "        else:\n",
    "            publisher_year = None\n",
    "            publisher_date = None\n",
    "        title=attributes.get('titles', [{}])[0].get('title','')\n",
    "        creators = attributes.get('creators', [{}])\n",
    "        creatorsNames = [creator.get('name', '') for creator in creators]\n",
    "        creatorsAffiliations = ['; '.join([aff['name'] for aff in creator.get('affiliation', [])]) for creator in creators]        \n",
    "        first_creator = creators[0].get('name', None)\n",
    "        last_creator = creators[-1].get('name', None)\n",
    "        affiliations = [affiliation.get('name' '') for creator in creators for affiliation in creator.get('affiliation', [{}])]\n",
    "        first_affiliation = affiliations[0] if affiliations else None\n",
    "        last_affiliation = affiliations[-1] if affiliations else None\n",
    "        contributors = attributes.get('contributors', [{}])\n",
    "        contributorsNames = [contributor.get('name', '') for contributor in contributors]\n",
    "        contributorsAffiliations = ['; '.join([aff['name'] for aff in contributor.get('affiliation', [])]) for contributor in contributors]        \n",
    "        container = attributes.get('container', {})\n",
    "        container_identifier = container.get('identifier', None)\n",
    "        related_identifiers = attributes.get('relatedIdentifiers', [])\n",
    "        for identifier in related_identifiers:\n",
    "            relationType = identifier.get('relationType', '')\n",
    "            relatedIdentifier = identifier.get('relatedIdentifier', '')\n",
    "        types = attributes.get('types', {})\n",
    "        resourceType = types.get('resourceTypeGeneral', '')\n",
    "        sizes = attributes.get('sizes', [])\n",
    "        cleaned_sizes = [int(re.sub(r'\\D', '', size)) for size in sizes if re.sub(r'\\D', '', size).isdigit()]\n",
    "        total_size = sum(cleaned_sizes) if cleaned_sizes else 'No file size information'   \n",
    "        formats_list = attributes.get('formats', [])\n",
    "        formats = set(formats_list) if formats_list else 'No file information'    \n",
    "        rights_list = attributes.get('rightsList', [])\n",
    "        rights = [right['rights'] for right in rights_list if 'rights' in right] or ['Rights unspecified']\n",
    "        rightsCode = [right['rightsIdentifier'] for right in rights_list if 'rightsIdentifier' in right] or ['Unknown']\n",
    "        views = attributes.get('viewCount', 0)\n",
    "        downloads = attributes.get('downloadCount', 0)\n",
    "        citations = attributes.get('citationCount', 0)\n",
    "        data_select_datacite.append({\n",
    "            'doi': doi,\n",
    "            'state': state,\n",
    "            'publisher': publisher,\n",
    "            'publicationYear': publisher_year,\n",
    "            'publicationDate': publisher_date,\n",
    "            'title': title,\n",
    "            'first_author': first_creator,\n",
    "            'last_author': last_creator,\n",
    "            'first_affiliation': first_affiliation,\n",
    "            'last_affiliation': last_affiliation,\n",
    "            'creatorsNames': creatorsNames,\n",
    "            'creatorsAffiliations': creatorsAffiliations,\n",
    "            'contributorsNames': contributorsNames,\n",
    "            'contributorsAffiliations': contributorsAffiliations,\n",
    "            'relationType': relationType,\n",
    "            'relatedIdentifier': relatedIdentifier,\n",
    "            'containerIdentifier': container_identifier,\n",
    "            'type': resourceType,\n",
    "            'depositSize': total_size,\n",
    "            'formats': formats,\n",
    "            'rights': rights,\n",
    "            'rightsCode': rightsCode,\n",
    "            'views': views,\n",
    "            'downloads': downloads,\n",
    "            'citations': citations,\n",
    "            'source': 'DataCite'\n",
    "        })\n",
    "\n",
    "    df_datacite_initial = pd.json_normalize(data_select_datacite)\n",
    "    df_datacite_initial.to_csv(f'outputs/{todayDate}_datacite-initial-output.csv')\n",
    "\n",
    "    if crossValidate:\n",
    "        print('Dryad step\\n')\n",
    "        data_select_dryad = [] \n",
    "        for item in data_dryad:\n",
    "            links = item.get('_links', {})\n",
    "            doi_dr = item.get('identifier', None)\n",
    "            pubDate_dr = item.get('publicationDate', '')\n",
    "            title_dr=item.get('title', [{}])\n",
    "            authors_dr = item.get('authors', [{}])\n",
    "            first_author_first = authors_dr[0].get('firstName', None)\n",
    "            first_author_last = authors_dr[0].get('lastName', None)\n",
    "            last_author_first = authors_dr[-1].get('firstName', None)\n",
    "            last_author_last = authors_dr[-1].get('lastName', None)\n",
    "            first_affiliation_dr = authors_dr[0].get('affiliation', None)\n",
    "            last_affiliation_dr = authors_dr[-1].get('affiliation', None)\n",
    "            related_works_list_dr = [rel.get('identifier', None) for rel in item.get('relatedWorks', [{}])]\n",
    "            related_works_list_dr = related_works_list_dr if related_works_list_dr else None\n",
    "            author_last_order_dr = authors_dr[-1].get('order', None)\n",
    "            data_select_dryad.append({\n",
    "                'doi': doi_dr,\n",
    "                'publicationDate': pubDate_dr,\n",
    "                'title': title_dr,\n",
    "                'first_author_first': first_author_first,\n",
    "                'last_author_first': last_author_first,\n",
    "                'first_author_last': first_author_last,\n",
    "                'last_author_last': last_author_last,\n",
    "                'first_affiliation': first_affiliation_dr,\n",
    "                'last_affiliation': last_affiliation_dr,\n",
    "                'relatedWorks': related_works_list_dr\n",
    "            })\n",
    "        df_dryad = pd.json_normalize(data_select_dryad)\n",
    "        df_dryad.to_csv(f'outputs/{todayDate}_Dryad-API-output.csv', index=False)\n",
    "\n",
    "        if dataverse:\n",
    "            print('Dataverse step\\n')\n",
    "            data_select_dataverse = [] \n",
    "            for item in data_dataverse:\n",
    "                globalID = item.get('global_id', '')\n",
    "                versionState = item.get('versionState', None)\n",
    "                pubDate_dataverse = item.get('published_at', '')\n",
    "                title_dataverse = item.get('name', None)\n",
    "                authors_dataverse = item.get('authors', [{}])\n",
    "                contacts_dataverse = item.get('contacts', [{}])\n",
    "                first_contact_dataverse = contacts_dataverse[0].get('name', None)\n",
    "                first_affiliation_contact = contacts_dataverse[0].get('affiliation', None)\n",
    "                last_contact_dataverse = contacts_dataverse[-1].get('name', None)\n",
    "                last_affiliation_contact = contacts_dataverse[-1].get('affiliation', None)\n",
    "                type = item.get('type', None)\n",
    "                dataverse = item.get('name_of_dataverse', None)\n",
    "                data_select_dataverse.append({\n",
    "                    'doi': globalID,\n",
    "                    'status': versionState,\n",
    "                    'publicationDate': pubDate_dataverse,\n",
    "                    'title': title_dataverse,\n",
    "                    'authors': authors_dataverse,\n",
    "                    'contacts': contacts_dataverse,\n",
    "                    'first_contact': first_contact_dataverse,\n",
    "                    'first_contact_affiliation': first_affiliation_contact,\n",
    "                    'last_contact': last_contact_dataverse,\n",
    "                    'last_contact_affiliation': last_affiliation_contact,\n",
    "                    'type': type,\n",
    "                    'dataverse': dataverse\n",
    "                })\n",
    "            df_dataverse = pd.json_normalize(data_select_dataverse)\n",
    "            df_dataverse.to_csv(f'outputs/{todayDate}_TDR-API-output.csv', index=False)\n",
    "\n",
    "        print('Zenodo step\\n')\n",
    "        data_select_zenodo = [] \n",
    "        for item in data_zenodo:\n",
    "            metadata = item.get('metadata', {})\n",
    "            doi = item.get('doi', None)\n",
    "            parentDOI = item.get('conceptdoi', None)\n",
    "            conceptID = item.get('conceptrecid', None)\n",
    "            pubDate_zen = metadata.get('publication_date', '')\n",
    "            title_zen=metadata.get('title', '')\n",
    "            description_zen = metadata.get('description', None)\n",
    "            creators_zen = metadata.get('creators', [{}])\n",
    "            first_creator_zen = creators_zen[0].get('name', None)\n",
    "            last_creator_zen = creators_zen[-1].get('name', None)\n",
    "            first_affiliation_zen = creators_zen[0].get('affiliation', None)\n",
    "            last_affiliation_zen = creators_zen[-1].get('affiliation', None)\n",
    "            related_works_list_zen = [name.get('identifier', None) for name in metadata.get('relatedWorks', [{}])]\n",
    "            related_works_list_zen = related_works_list_zen if related_works_list_zen else None\n",
    "            related_works_type_list_zen = [name.get('relation', None) for name in metadata.get('relatedWorks', [{}])]\n",
    "            related_works_type_list_zen = related_works_type_list_zen if related_works_type_list_zen else None\n",
    "            data_select_zenodo.append({\n",
    "                'doi': parentDOI, #want parent to avoid de-duplication issues later\n",
    "                'publicationDate': pubDate_zen,\n",
    "                'title': title_zen,\n",
    "                'description': description_zen,\n",
    "                'first_author': first_creator_zen,\n",
    "                'last_author': last_creator_zen,\n",
    "                'first_affiliation': first_affiliation_zen,\n",
    "                'last_affiliation': last_affiliation_zen,\n",
    "                'relatedWorks': related_works_list_zen,\n",
    "                'relatedWorks_type': related_works_type_list_zen\n",
    "            })\n",
    "        df_data_zenodo = pd.json_normalize(data_select_zenodo)\n",
    "        df_data_zenodo.to_csv(f'outputs/{todayDate}_Zenodo-API-output.csv', index=False)\n",
    "\n",
    "    print('Beginning dataframe editing.\\n')\n",
    "\n",
    "    #split out DataCite results for repos to be cross-validated against\n",
    "    ##coercing all DOIs with 'zenodo' to have publisher of 'Zenodo'\n",
    "    df_datacite_initial.loc[df_datacite_initial['doi'].str.contains('zenodo', case=False, na=False), 'publisher'] = 'Zenodo'\n",
    "    ##using str.contains to account for any potential name inconsistency for one repository\n",
    "    df_datacite_dryad = df_datacite_initial[df_datacite_initial['publisher'].str.contains('Dryad')]\n",
    "    df_datacite_dataverse = df_datacite_initial[df_datacite_initial['publisher'].str.contains('Texas Data Repository')]\n",
    "    df_datacite_zenodo = df_datacite_initial[df_datacite_initial['publisher'].str.contains('Zenodo')]\n",
    "    df_remainder = df_datacite_initial[df_datacite_initial['publisher'].str.contains('Dryad|Texas Data Repository|Zenodo') == False]\n",
    "\n",
    "    print(f'Number of Dryad datasets found by DataCite API: {len(df_datacite_dryad)}\\n')\n",
    "    print(f'Number of Dataverse datasets found by DataCite API: {len(df_datacite_dataverse)}\\n')\n",
    "    print(f'Number of Zenodo datasets found by DataCite API: {len(df_datacite_zenodo)}\\n')\n",
    "\n",
    "    if crossValidate:\n",
    "        print('Repository-specific processing\\n')\n",
    "        if dataverse:\n",
    "            #subsetting for published datasets\n",
    "            df_dataverse_pub = df_dataverse[df_dataverse['status'].str.contains('RELEASED') == True]\n",
    "            #looking for UT Austin in any of four fields\n",
    "            pattern = '|'.join([f'({perm})' for perm in ut_variations])\n",
    "            df_dataverse_pub['authors'] = df_dataverse_pub['authors'].apply(lambda x: str(x))\n",
    "            df_dataverse_pub['contacts'] = df_dataverse_pub['contacts'].apply(lambda x: str(x))\n",
    "            df_dataverse_pub_filtered = df_dataverse_pub[df_dataverse_pub['authors'].str.contains(pattern, case=False, na=False) | df_dataverse_pub['contacts'].str.contains(pattern, case=False, na=False)]\n",
    "            print(f'Number of published Dataverse datasets found by Dataverse API: {len(df_dataverse_pub)}\\n')\n",
    "\n",
    "        #removing non-Zenodo deposits indexed by Zenodo (mostly Dryad) from Zenodo output\n",
    "        ##Zenodo has indexed many Dryad deposits <50 GB in size (does not issue a new DOI but does return a Zenodo 'record' in the API)\n",
    "        df_data_zenodo_true = df_data_zenodo[df_data_zenodo['doi'].str.contains('zenodo') == True] \n",
    "        #for some reason, Zenodo's API sometimes returns identical entries of most datasets...\n",
    "        df_data_zenodo_real = df_data_zenodo_true.drop_duplicates(subset=['publicationDate', 'doi'], keep='first') \n",
    "        print(f'Number of non-Dryad Zenodo datasets found by Zenodo API: {len(df_data_zenodo_real)}\\n')\n",
    "\n",
    "        #formatting author names to be consistent with others\n",
    "        df_dryad['first_author'] = df_dryad['first_author_last'] + ', ' + df_dryad['first_author_first']\n",
    "        df_dryad['last_author'] = df_dryad['last_author_last'] + ', ' + df_dryad['last_author_first']\n",
    "        df_dryad = df_dryad.drop(columns=['first_author_first', 'first_author_last', \n",
    "                            'last_author_first', 'last_author_last'])\n",
    "\n",
    "        df_dryad['publicationYear'] = pd.to_datetime(df_dryad['publicationDate']).dt.year\n",
    "        if dataverse:\n",
    "            df_dataverse_pub_filtered['publicationYear'] = pd.to_datetime(df_dataverse_pub_filtered['publicationDate'], format='ISO8601').dt.year\n",
    "        df_data_zenodo_real['publicationYear'] = pd.to_datetime(df_data_zenodo_real['publicationDate'], format='mixed').dt.year\n",
    "\n",
    "        df_dryad_pruned = df_dryad[['doi', 'publicationYear', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation']]\n",
    "        if dataverse:\n",
    "           df_dataverse_pruned = df_dataverse_pub_filtered[['doi', 'publicationYear', 'title', 'first_contact', 'first_contact_affiliation', 'last_contact', 'last_contact_affiliation']]\n",
    "        df_zenodo_pruned = df_data_zenodo_real[['doi','publicationYear', 'title','first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'publicationDate', 'description']]\n",
    "\n",
    "    df_datacite_dryad_pruned = df_datacite_dryad[['publisher', 'doi', 'publicationYear', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'type']]\n",
    "    df_datacite_dataverse_pruned = df_datacite_dataverse[['publisher', 'doi', 'publicationYear', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'type']] \n",
    "    df_datacite_zenodo_pruned = df_datacite_zenodo[['publisher', 'doi', 'publicationYear', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation','type']] \n",
    "    df_datacite_remainder_pruned = df_remainder[['publisher', 'doi', 'publicationYear', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation','type']] \n",
    "\n",
    "    #create new lists for recursive modification\n",
    "    datacite_dataframes_pruned = [df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned, df_datacite_remainder_pruned]\n",
    "    datacite_dataframes_specific_pruned = [df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned]\n",
    "\n",
    "    #standardizing how Texas Data Repository is displayed\n",
    "    df_datacite_dataverse_pruned['publisher'] = df_datacite_dataverse_pruned['publisher'].str.replace('Texas Data Repository Dataverse','Texas Data Repository')\n",
    "\n",
    "    for df in datacite_dataframes_specific_pruned:\n",
    "        df['doi'] = df['doi'].str.lower()\n",
    "\n",
    "    columns_to_rename = {\n",
    "        'publisher': 'repository'\n",
    "    }\n",
    "    for i in range(len(datacite_dataframes_pruned)):\n",
    "        datacite_dataframes_pruned[i] = datacite_dataframes_pruned[i].rename(columns=columns_to_rename)\n",
    "        datacite_dataframes_pruned[i]['source'] = 'DataCite'\n",
    "    #assign modified dfs back to original\n",
    "    df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned, df_datacite_remainder_pruned = datacite_dataframes_pruned\n",
    "\n",
    "    #reload list\n",
    "    datacite_dataframes_specific_pruned = [df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned]\n",
    "    for i in range(len(datacite_dataframes_specific_pruned)):\n",
    "        datacite_dataframes_specific_pruned[i] = datacite_dataframes_specific_pruned[i].rename(columns={c: c+'_dc' for c in datacite_dataframes_specific_pruned[i].columns if c not in ['doi']})\n",
    "    #assign modified dfs back to original\n",
    "    df_datacite_dryad_pruned, df_datacite_dataverse_pruned, df_datacite_zenodo_pruned = datacite_dataframes_specific_pruned\n",
    "\n",
    "    if crossValidate:\n",
    "        #create list of repository-specific dfs\n",
    "        if dataverse:\n",
    "            repositories_dataframes_pruned = [df_dryad_pruned, df_dataverse_pruned, df_zenodo_pruned]\n",
    "        else:\n",
    "            repositories_dataframes_pruned = [df_dryad_pruned, df_zenodo_pruned]\n",
    "\n",
    "        #editing DOI columns to ensure exact matches\n",
    "        df_dryad_pruned['doi'] = df_dryad_pruned['doi'].str.replace('doi:', '')\n",
    "        if dataverse:\n",
    "            df_dataverse_pruned['doi'] = df_dataverse_pruned['doi'].str.replace('doi:', '')\n",
    "        for df in repositories_dataframes_pruned:\n",
    "            df['doi'] = df['doi'].str.lower()\n",
    "\n",
    "        #adding suffix to column headers to differentiate identically named columns when merged (vs. automatic .x and .y)\n",
    "        df_dryad_pruned = df_dryad_pruned.rename(columns={c: c+'_dryad' for c in df_dryad_pruned.columns if c not in ['doi']})\n",
    "        if dataverse:\n",
    "            df_dataverse_pruned = df_dataverse_pruned.rename(columns={c: c+'_dataverse' for c in df_dataverse_pruned.columns if c not in ['doi']})\n",
    "        df_zenodo_pruned = df_zenodo_pruned.rename(columns={c: c+'_zen' for c in df_data_zenodo_real.columns if c not in ['doi']})\n",
    "\n",
    "        df_dryad_pruned['source_dryad'] = 'Dryad'\n",
    "        if dataverse:\n",
    "            df_dataverse_pruned['source_dataverse'] = 'Texas Data Repository'\n",
    "        df_zenodo_pruned['source_zenodo'] = 'Zenodo'\n",
    "\n",
    "        print('Beginning cross-validation process.\\n')\n",
    "\n",
    "        #DataCite into Dryad\n",
    "        df_dryad_datacite_joint = pd.merge(df_dryad_pruned, df_datacite_dryad_pruned, on='doi', how='left')\n",
    "        df_dryad_datacite_joint['Match_entry'] = np.where(df_dryad_datacite_joint['source_dc'].isnull(), 'Not matched', 'Matched')\n",
    "        print('Counts of matches for DataCite into Dryad')\n",
    "        counts_dryad_datacite = df_dryad_datacite_joint['Match_entry'].value_counts()\n",
    "        print(counts_dryad_datacite, '\\n')\n",
    "        df_dryad_datacite_joint_unmatched = df_dryad_datacite_joint[df_dryad_datacite_joint['Match_entry'] == 'Not matched']\n",
    "        df_dryad_datacite_joint_unmatched.to_csv(f'outputs/{todayDate}_DataCite-into-Dryad_joint-unmatched-dataframe.csv', index=False)\n",
    "        df_dryad_undetected = df_dryad_datacite_joint_unmatched[['doi']]\n",
    "\n",
    "        #Dryad into DataCite\n",
    "        df_datacite_dryad_joint = pd.merge(df_datacite_dryad_pruned, df_dryad_pruned, on='doi', how='left')\n",
    "        df_datacite_dryad_joint['Match_entry'] = np.where(df_datacite_dryad_joint['source_dryad'].isnull(), 'Not matched', 'Matched')\n",
    "        print('Counts of matches for Dryad into DataCite')\n",
    "        counts_datacite_dryad = df_datacite_dryad_joint['Match_entry'].value_counts()\n",
    "        print(counts_datacite_dryad, '\\n')\n",
    "        df_datacite_dryad_joint_unmatched = df_datacite_dryad_joint[df_datacite_dryad_joint['Match_entry'] == 'Not matched']\n",
    "        df_datacite_dryad_joint_unmatched.to_csv(f'outputs/{todayDate}_Dryad-into-DataCite_joint-unmatched-dataframe.csv', index=False)\n",
    "\n",
    "        if dataverse:\n",
    "        #DataCite into Dataverse (using non-de-duplicated DataCite data)\n",
    "            df_dataverse_datacite_joint = pd.merge(df_dataverse_pruned, df_datacite_dataverse_pruned, on='doi', how='left')\n",
    "            df_dataverse_datacite_joint['Match_entry'] = np.where(df_dataverse_datacite_joint['source_dc'].isnull(), 'Not matched', 'Matched')\n",
    "            print('Counts of matches for DataCite into Dataverse')\n",
    "            counts_dataverse_datacite = df_dataverse_datacite_joint['Match_entry'].value_counts()\n",
    "            print(counts_dataverse_datacite, '\\n')\n",
    "            df_dataverse_datacite_joint_unmatched = df_dataverse_datacite_joint[df_dataverse_datacite_joint['Match_entry'] == 'Not matched']\n",
    "            df_dataverse_datacite_joint_unmatched.to_csv(f'outputs/{todayDate}_DataCite-into-Dataverse_joint-unmatched-dataframe.csv', index=False)\n",
    "            df_dataverse_undetected = df_dataverse_datacite_joint_unmatched[['doi']]\n",
    "\n",
    "            #Dataverse into DataCite (using de-duplicated DataCite data)\n",
    "            df_datacite_dataverse_joint = pd.merge(df_datacite_dataverse_pruned, df_dataverse_pruned, on='doi', how='left')\n",
    "            df_datacite_dataverse_joint['Match_entry'] = np.where(df_datacite_dataverse_joint['source_dataverse'].isnull(), 'Not matched', 'Matched')\n",
    "            print('Counts of matches for Dataverse into DataCite')\n",
    "            counts_datacite_dataverse = df_datacite_dataverse_joint['Match_entry'].value_counts()\n",
    "            print(counts_datacite_dataverse, '\\n')\n",
    "            df_datacite_dataverse_joint_unmatched = df_datacite_dataverse_joint[df_datacite_dataverse_joint['Match_entry'] == 'Not matched']\n",
    "            df_datacite_dataverse_joint_unmatched.to_csv(f'outputs/{todayDate}_Dataverse-into-DataCite_joint-unmatched-dataframe.csv', index=False)\n",
    "\n",
    "        #DataCite into Zenodo\n",
    "        # df_zenodo_datacite_joint = pd.merge(df_zenodo_pruned, df_datacite_zenodo_pruned, on='doi', how='left')\n",
    "        df_zenodo_datacite_joint = pd.merge(df_zenodo_pruned, df_datacite_zenodo_pruned, on='doi', how='left')\n",
    "        df_zenodo_datacite_joint['Match_entry'] = np.where(df_zenodo_datacite_joint['source_dc'].isnull(), 'Not matched', 'Matched')\n",
    "        ##removing multiple DOIs in same 'lineage'\n",
    "        df_zenodo_datacite_joint = df_zenodo_datacite_joint.sort_values(by=['doi'])\n",
    "        df_zenodo_datacite_joint_deduplicated = df_zenodo_datacite_joint.drop_duplicates(subset=['publicationDate_zen', 'description_zen'], keep='last') \n",
    "        ##one problematic dataset splits incorrectly when exported to CSV (conceptrecID = 616927)\n",
    "        print('Counts of matches for DataCite into Zenodo\\n')\n",
    "        counts_zenodo_datacite = df_zenodo_datacite_joint_deduplicated['Match_entry'].value_counts()\n",
    "        print(counts_zenodo_datacite, '\\n')\n",
    "        df_zenodo_datacite_joint_unmatched = df_zenodo_datacite_joint_deduplicated[df_zenodo_datacite_joint_deduplicated['Match_entry'] == 'Not matched']\n",
    "        df_zenodo_datacite_joint_unmatched.to_excel(f'outputs/{todayDate}_DataCite-into-Zenodo-unmatched-dataframe.xlsx', index=False)\n",
    "        df_zenodo_undetected = df_zenodo_datacite_joint_unmatched[['doi']]\n",
    "\n",
    "        #Zenodo into DataCite\n",
    "        # df_datacite_zenodo_joint = pd.merge(df_datacite_zenodo_pruned, df_zenodo_pruned, on='doi', how='left')\n",
    "        df_datacite_zenodo_joint = pd.merge(df_datacite_zenodo_pruned, df_zenodo_pruned, on='doi', how='left')\n",
    "        df_datacite_zenodo_joint['Match_entry'] = np.where(df_datacite_zenodo_joint['source_zenodo'].isnull(), 'Not matched', 'Matched')\n",
    "        ##removing multiple DOIs in same 'lineage'\n",
    "        df_datacite_zenodo_joint = df_datacite_zenodo_joint.sort_values(by=['doi']) \n",
    "        df_datacite_zenodo_joint_deduplicated = df_datacite_zenodo_joint.drop_duplicates(subset=['publicationDate_zen', 'description_zen'], keep='first') \n",
    "        print('Counts of matches for Zenodo into DataCite\\n')\n",
    "        counts_datacite_zenodo = df_datacite_zenodo_joint_deduplicated['Match_entry'].value_counts()\n",
    "        print(counts_datacite_zenodo, '\\n')\n",
    "        df_datacite_zenodo_joint_unmatched = df_datacite_zenodo_joint_deduplicated[df_datacite_zenodo_joint_deduplicated['Match_entry'] == 'Not matched']\n",
    "        df_datacite_zenodo_joint_unmatched.to_excel(f'outputs/{todayDate}_Zenodo-into-DataCite_joint-unmatched-dataframe.xlsx', index=False)\n",
    "\n",
    "        #get DataCite metadata for all entries not previously detected by DataCite API query\n",
    "        if dataverse:\n",
    "            datacite_new = pd.concat([df_dryad_undetected, df_dataverse_undetected, df_zenodo_undetected], ignore_index=True)\n",
    "        else:\n",
    "            datacite_new = pd.concat([df_dryad_undetected, df_zenodo_undetected], ignore_index=True)\n",
    "\n",
    "        print('Retrieving additional DataCite metadata for unmatched deposits\\n')\n",
    "        results = []\n",
    "        if test:\n",
    "            datacite_new = datacite_new.head(10)\n",
    "        for doi in datacite_new['doi']:\n",
    "            try:\n",
    "                response = requests.get(f'{url_datacite}/{doi}')\n",
    "                if response.status_code == 200:\n",
    "                    print(f'Retrieving {doi}\\n')\n",
    "                    results.append(response.json())\n",
    "                else:\n",
    "                    print(f'Error retrieving {doi}: {response.status_code}, {response.text}')\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f'Timeout error on DOI {doi}: {e}')\n",
    "\n",
    "        data_datacite_new = {\n",
    "            'datasets': results\n",
    "        }\n",
    "\n",
    "        data_select_datacite_new = []\n",
    "        datasets = data_datacite_new.get('datasets', []) \n",
    "        for item in datasets:\n",
    "            data = item.get('data', {})\n",
    "            attributes = data.get('attributes', {})\n",
    "            doi = attributes.get('doi', None)\n",
    "            state = attributes.get('state', None)\n",
    "            publisher = attributes.get('publisher', '')\n",
    "            registered = attributes.get('registered', '')\n",
    "            if registered:\n",
    "                publisher_year = datetime.fromisoformat(registered.rstrip('Z')).year\n",
    "                publisher_date = datetime.fromisoformat(registered.rstrip('Z')).date()\n",
    "            else:\n",
    "                publisher_year = None\n",
    "                publisher_date = None\n",
    "            title = attributes.get('titles', [{}])[0].get('title', '')\n",
    "            creators = attributes.get('creators', [{}])\n",
    "            creatorsNames = [creator.get('name', '') for creator in creators]\n",
    "            # creatorsAffiliations = []\n",
    "            # for creator in creators:\n",
    "            #     affs = creator.get('affiliation', [])\n",
    "            #     if isinstance(affs, list):\n",
    "            #         names = [aff.get('name', '') for aff in affs if isinstance(aff, dict)]\n",
    "            #         creatorsAffiliations.append('; '.join(names))\n",
    "            #     else:\n",
    "            #         creatorsAffiliations.append(str(affs))\n",
    "            creatorsAffiliations = ['; '.join(creator.get('affiliation', [])) for creator in creators]\n",
    "            first_creator = creators[0].get('name', None) if creators else None\n",
    "            last_creator = creators[-1].get('name', None) if creators else None\n",
    "            affiliations = [\n",
    "                aff.get('name', '')\n",
    "                for creator in creators\n",
    "                for aff in (creator.get('affiliation') if isinstance(creator.get('affiliation'), list) else [])\n",
    "                if isinstance(aff, dict)\n",
    "            ]\n",
    "            first_affiliation = affiliations[0] if affiliations else None\n",
    "            last_affiliation = affiliations[-1] if affiliations else None\n",
    "            contributors = attributes.get('contributors', [{}])\n",
    "            contributorsNames = [contributor.get('name', '') for contributor in contributors]\n",
    "            # contributorsAffiliations = []\n",
    "            # for contributor in contributors:\n",
    "            #     affs = contributor.get('affiliation', [])\n",
    "            #     if isinstance(affs, list):\n",
    "            #         names = [aff.get('name', '') for aff in affs if isinstance(aff, dict)]\n",
    "            #         contributorsAffiliations.append('; '.join(names))\n",
    "            #     else:\n",
    "            #         contributorsAffiliations.append(str(affs))\n",
    "            contributorsAffiliations = ['; '.join(contributor.get('affiliation', [])) for contributor in contributors]\n",
    "            container = attributes.get('container', {})\n",
    "            container_identifier = container.get('identifier', None)\n",
    "            related_identifiers = attributes.get('relatedIdentifiers', [])\n",
    "            for identifier in related_identifiers:\n",
    "                relationType = identifier.get('relationType', '')\n",
    "                relatedIdentifier = identifier.get('relatedIdentifier', '')\n",
    "            types = attributes.get('types', {})\n",
    "            resourceType = types.get('resourceTypeGeneral', '')\n",
    "            sizes = attributes.get('sizes', [])\n",
    "            cleaned_sizes = [int(re.sub(r'\\D', '', size)) for size in sizes if re.sub(r'\\D', '', size).isdigit()]\n",
    "            total_size = sum(cleaned_sizes) if cleaned_sizes else 'No file size information'   \n",
    "            formats_list = attributes.get('formats', [])\n",
    "            formats = set(formats_list) if formats_list else 'No file information'    \n",
    "            rights_list = attributes.get('rightsList', [])\n",
    "            rights = [right['rights'] for right in rights_list if 'rights' in right] or ['Rights unspecified']\n",
    "            rightsCode = [right['rightsIdentifier'] for right in rights_list if 'rightsIdentifier' in right] or ['Unknown']\n",
    "            views = attributes.get('viewCount', 0)\n",
    "            downloads = attributes.get('downloadCount', 0)\n",
    "            citations = attributes.get('citationCount', 0)\n",
    "            data_select_datacite_new.append({\n",
    "                'doi': doi,\n",
    "                'state': state,\n",
    "                'publisher': publisher,\n",
    "                'publicationYear': publisher_year,\n",
    "                'publicationDate': publisher_date,\n",
    "                'title': title,\n",
    "                'first_author': first_creator,\n",
    "                'last_author': last_creator,\n",
    "                'first_affiliation': first_affiliation,\n",
    "                'last_affiliation': last_affiliation,\n",
    "                'creatorsNames': creatorsNames,\n",
    "                'creatorsAffiliations': creatorsAffiliations,\n",
    "                'contributorsNames': contributorsNames,\n",
    "                'contributorsAffiliations': contributorsAffiliations,\n",
    "                'relationType': relationType,\n",
    "                'relatedIdentifier': relatedIdentifier,\n",
    "                'containerIdentifier': container_identifier,\n",
    "                'type': resourceType,\n",
    "                'depositSize': total_size,\n",
    "                'formats': formats,\n",
    "                'rights': rights,\n",
    "                'rightsCode': rightsCode,\n",
    "                'views': views,\n",
    "                'downloads': downloads,\n",
    "                'citations': citations,\n",
    "                'source': 'repository cross-validation'\n",
    "            })\n",
    "\n",
    "        df_datacite_new = pd.json_normalize(data_select_datacite_new)\n",
    "        df_datacite_new.to_csv(f'outputs/{todayDate}_datacite-additional-cross-validation.csv')\n",
    "    if crossValidate:\n",
    "        df_datacite_all = pd.concat([df_datacite_initial, df_datacite_new], ignore_index=True)\n",
    "    else:\n",
    "        df_datacite_all = df_datacite_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894350a5",
   "metadata": {},
   "source": [
    "### Step 2: Cleaning and filtering\n",
    "\n",
    "This step cleans and filters the results from the DataCite query. Several intermediate files are exported here for specific functions (counting which metadata field an affiliation was detected in and which permutation it is; various metadata assessments). The deduplication process is designed to handle 'parent-child' DOI systems where each deposit receives at least two DOIs; DOI-for-each-version systems where each additional version receives its own DOI; and over-granularization of the automated Figshare process when files linked to a single manuscript are all split into their own deposits. For the Figshare deposits, metadata is 'consolidated' such that the metadata are combined (e.g., DOIs combined into semi-colon-delimited string). DOIs that are for files (e.g., Dataverse installations) are also excluded.\n",
    "\n",
    "It also handles edge cases with respect to inaccurate metadata labels (e.g., deposits in repositories that allow depositors to freeform-edit the 'publisher' field that is crosswalked to DataCite and usually used to indicate the repository where the deposit is stored). Name variation within a given repository is standardized, and all discovered Figshare deposits that were mediated through a publisher partner and that list that publisher as the 'publisher' in the DataCite metadata are standardized to list 'figshare' as the publisher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea774515",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not loadPreviousData and not loadPreviousDataPlus and not loadPreviousDataPlusNCBI:\n",
    "    #creating column for source of detected affiliation\n",
    "    pattern = '|'.join([f'({perm})' for perm in ut_variations])\n",
    "    #search for permutations in the 'affiliations' column\n",
    "    df_datacite_all['affiliation_source'] = df_datacite_all.apply(\n",
    "    lambda row: 'creator.affiliationName' if pd.Series(row['creatorsAffiliations']).str.contains(pattern, case=False, na=False).any()\n",
    "    else ('creator.name' if pd.Series(row['creatorsNames']).str.contains(pattern, case=False, na=False).any()\n",
    "    else ('contributor.affiliationName' if pd.Series(row['contributorsAffiliations']).str.contains(pattern, case=False, na=False).any()\n",
    "    else ('contributor.name' if pd.Series(row['contributorsNames']).str.contains(pattern, case=False, na=False).any() else None))),\n",
    "    axis=1)\n",
    "    #pull out the identified permutation and put it into a new column\n",
    "    df_datacite_all['affiliation_permutation'] = df_datacite_all.apply(\n",
    "    lambda row:\n",
    "        next(\n",
    "            # First pass: exact match (case-sensitive)\n",
    "            (perm for perm in ut_variations\n",
    "             if any(perm == entry for entry in row['creatorsAffiliations'] + row['creatorsNames'] + row['contributorsAffiliations'] + row['contributorsNames'])),\n",
    "            # Second pass: full-phrase match (case-sensitive)\n",
    "            next(\n",
    "                (perm for perm in ut_variations\n",
    "                 if any(\n",
    "                     pd.Series(row['creatorsAffiliations'] + row['creatorsNames'] + row['contributorsAffiliations'] + row['contributorsNames'])\n",
    "                     .str.contains(fr'\\b{re.escape(perm)}\\b', case=True, na=False)\n",
    "                 )),\n",
    "                None\n",
    "            )\n",
    "        ),\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "    #handling version duplication (Figshare, ICPSR, etc.)\n",
    "    ##handling duplication of Figshare deposits (parent vs. child with '.v*')\n",
    "    ###creating separate figshare dataframe for downstream processing, not necessary for other repositories with this DOI mechanism in current workflow\n",
    "    figshare = df_datacite_all[df_datacite_all['doi'].str.contains('figshare')]\n",
    "    df_datacite_no_figshare = df_datacite_all[~df_datacite_all['doi'].str.contains('figshare')]\n",
    "    figshare_no_versions = figshare[~figshare['doi'].str.contains(r'\\.v\\d+$')]\n",
    "    #mediated workflow sometimes creates individual deposit for each file, want to treat as single dataset here\n",
    "    for col in figshare_no_versions.columns:\n",
    "        if figshare_no_versions[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            figshare_no_versions[col] = figshare_no_versions[col].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "    figshare_no_versions['hadPartialDuplicate'] = figshare_no_versions.duplicated(subset=['publisher', 'publicationDate', 'first_author', 'last_author', 'first_affiliation', 'type', 'relatedIdentifier'], keep=False)\n",
    "\n",
    "    #aggregating related entries together\n",
    "    # figshare_no_versions_combined = figshare_no_versions.groupby('relatedIdentifier').agg(lambda x: '; '.join(sorted(map(str, set(x))))).reset_index()\n",
    "    sum_columns = ['depositSize', 'views', 'citations', 'downloads']\n",
    "\n",
    "    def agg_func(column_name):\n",
    "        if column_name in sum_columns:\n",
    "            return 'sum'\n",
    "        else:\n",
    "            return lambda x: sorted(set(x))\n",
    "\n",
    "    agg_funcs = {col: agg_func(col)for col in figshare_no_versions.columns if col != 'relatedIdentifier'}\n",
    "\n",
    "    figshare_no_versions_combined = figshare_no_versions.groupby('relatedIdentifier').agg(agg_funcs).reset_index()\n",
    "    # Convert all list-type columns to comma-separated strings\n",
    "    for col in figshare_no_versions_combined.columns:\n",
    "        if figshare_no_versions_combined[col].apply(lambda x: isinstance(x, list)).any():\n",
    "            figshare_no_versions_combined[col] = figshare_no_versions_combined[col].apply(lambda x: '; '.join(map(str, x)))\n",
    "    figshare_deduplicated = figshare_no_versions_combined.drop_duplicates(subset='relatedIdentifier', keep='first')\n",
    "    df_datacite_v1 = pd.concat([df_datacite_no_figshare, figshare_deduplicated], ignore_index=True)\n",
    "\n",
    "    ##handling duplication of ICPSR, SAGE, Mendeley Data, Zenodo deposits (parent vs. child)\n",
    "    lineageRepos = df_datacite_v1[df_datacite_v1['publisher'].str.contains('ICPSR|Mendeley|SAGE|Zenodo|4TU')]\n",
    "    df_datacite_lineageRepos = df_datacite_v1[~df_datacite_v1['publisher'].str.contains('ICPSR|Mendeley|SAGE|Zenodo|4TU')]\n",
    "    lineageRepos_deduplicated = lineageRepos[~lineageRepos['relationType'].str.contains('IsVersionOf|IsNewVersionOf', case=False, na=False)]\n",
    "    ###the use of .v* and v* as filters works for these repositories but could accidentally remove non-duplicate DOIs if applied to other repositories\n",
    "    lineageRepos_deduplicated = lineageRepos_deduplicated[~lineageRepos_deduplicated['doi'].str.contains(r'\\.v\\d+$')]\n",
    "    dois_to_remove = lineageRepos_deduplicated[(lineageRepos_deduplicated['doi'].str.contains(r'v\\d$') | lineageRepos_deduplicated['doi'].str.contains(r'v\\d-')) & (lineageRepos_deduplicated['publisher'].str.contains('ICPSR', case=False, na=False))]['doi']\n",
    "    # Remove the identified DOIs\n",
    "    lineageRepos_deduplicated = lineageRepos_deduplicated[~lineageRepos_deduplicated['doi'].isin(dois_to_remove)]\n",
    "    df_datacite_v2 = pd.concat([df_datacite_lineageRepos, lineageRepos_deduplicated], ignore_index=True)\n",
    "    \n",
    "    #handling historic Dryad DOI assignment to some files (may not occur for all institutions, does not occur for UT Austin)\n",
    "    df_datacite_dedup = df_datacite_v2[~((df_datacite_v2['publisher'] == 'Dryad') & (df_datacite_v2['doi'].str.count('/') >= 2))]\n",
    "\n",
    "    #handling Code Ocean (software repo, always ends in v*, only retain v1)\n",
    "    df_datacite_v2 = df_datacite_v2[~((df_datacite_v2['publisher'] == 'Code Ocean') & ~df_datacite_v2['doi'].str.endswith('v1'))]\n",
    "\n",
    "    #handling file-level DOI granularity (all Dataverse installations)\n",
    "    ##may need to expand search terms if you find a Dataverse installation without 'Dataverse' in name\n",
    "    df_datacite_dedup = df_datacite_v2[~(df_datacite_v2['publisher'].str.contains('Dataverse|Texas Data Repository', case=False, na=False) & df_datacite_v2['containerIdentifier'].notnull())]\n",
    "    df_datacite_dedup = df_datacite_dedup[~(df_datacite_dedup['doi'].str.count('/') >= 3)]\n",
    "    #handling same granularity in other repositories\n",
    "    df_datacite_dedup = df_datacite_dedup[~((df_datacite_dedup['publisher'] == 'AUSSDA') & (df_datacite_dedup['doi'].str.count('/') > 1))]\n",
    "\n",
    "\n",
    "    #handling blanket 'affiliation' of UT Austin for all DesignSafe deposits\n",
    "    ##DesignSafe is a UT-managed repository and this step is unlikely to be signficant for other institutions; there should also be a metadata fix for this forthcoming\n",
    "    if austin:\n",
    "        df_datacite_dedup = df_datacite_dedup[~((df_datacite_dedup['publisher'] == 'Designsafe-CI') & (df_datacite_dedup['affiliation_permutation'] != 'University of Texas at Austin'))\n",
    "        ]\n",
    "\n",
    "    #handling Dataverse partial duplication (oversplitting of one manuscript's materials)\n",
    "    if dataverseDuplicates:\n",
    "        # Convert list columns to strings\n",
    "        df_datacite_dedup['creatorsNames'] = df_datacite_dedup['creatorsNames'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "        df_datacite_dedup['creatorsAffiliations'] = df_datacite_dedup['creatorsAffiliations'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "        df_datacite_dedup['rights'] = df_datacite_dedup['rights'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "        dataverse = df_datacite_dedup[df_datacite_dedup['publisher'].str.contains('Texas Data Repository|Harvard|Dataverse', case=True, na=False)]\n",
    "        df_datacite_no_dataverse = df_datacite_dedup[~df_datacite_dedup['publisher'].str.contains('Texas Data Repository|Harvard|Dataverse', case=True, na=False)]\n",
    "     \n",
    "        group_variables = ['publisher', 'publicationDate', 'creatorsNames', 'creatorsAffiliations', 'type', 'rights']\n",
    "        sum_columns = ['depositSize', 'views', 'citations', 'downloads']\n",
    "\n",
    "        #ensure list-type columns are hashable for grouping\n",
    "        for col in dataverse.columns:\n",
    "            if dataverse[col].apply(lambda x: isinstance(x, list)).any():\n",
    "                dataverse[col] = dataverse[col].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "\n",
    "        #create column for partial duplicates\n",
    "        dataverse['hadPartialDuplicate'] = dataverse.duplicated(subset=group_variables, keep=False)\n",
    "\n",
    "        #modified entry aggregation function\n",
    "        def agg_func(column_name):\n",
    "            if column_name in sum_columns:\n",
    "                return 'sum'\n",
    "            else:\n",
    "                return lambda x: sorted(set(\n",
    "                    item\n",
    "                    for sublist in x\n",
    "                    for item in (list(sublist) if isinstance(sublist, (list, set)) else [sublist])\n",
    "                ))\n",
    "\n",
    "        #build aggregation dictionary\n",
    "        agg_funcs = {\n",
    "            col: agg_func(col)\n",
    "            for col in dataverse.columns\n",
    "            if col not in group_variables\n",
    "        }\n",
    "        for col in sum_columns:\n",
    "            if col in dataverse.columns:\n",
    "                dataverse[col] = pd.to_numeric(dataverse[col], errors='coerce')\n",
    "        \n",
    "        dataverse_combined = dataverse.groupby(group_variables).agg(agg_funcs).reset_index()\n",
    "\n",
    "        #convert list-type columns to semicolon-separated strings\n",
    "        for col in dataverse_combined.columns:\n",
    "            if dataverse_combined[col].apply(lambda x: isinstance(x, list)).any():\n",
    "                dataverse_combined[col] = dataverse_combined[col].apply(lambda x: '; '.join(map(str, x)))\n",
    "\n",
    "        dataverse_deduplicated = dataverse_combined.drop_duplicates(subset=group_variables, keep='first')\n",
    "        print(f'Number of deposits cut from {len(dataverse)} to {len(dataverse_combined)}')\n",
    "        df_datacite_dedup = pd.concat([df_datacite_no_dataverse, dataverse_combined], ignore_index=True)\n",
    "\n",
    "    #final sweeping dedpulication step, will catch a few odd edge cases that have been manually discovered\n",
    "    ##mainly addresses hundreds of EMSL datasets that seem overly granularized (many deposits share all metadata other than DOI including detailed titles) - will not be relevant for all institutions\n",
    "    df_sorted = df_datacite_dedup.sort_values(by='doi')\n",
    "    df_datacite = df_sorted.drop_duplicates(subset=['title', 'first_author', 'relationType', 'relatedIdentifier', 'containerIdentifier'], keep='first')\n",
    "\n",
    "    #the file exported here is intended only to be used to compare affiliation source fields; the fields will be dropped in later steps in the workflow\n",
    "    df_datacite.to_csv(f'outputs/{todayDate}_datacite-output-for-affiliation-source.csv', index=False) \n",
    "\n",
    "    #additional metadata assessment steps, fields are also dropped in later steps\n",
    "    ##convert mimeType to readable format\n",
    "    format_map = config['FORMAT_MAP']\n",
    "    df_datacite['fileFormat'] = df_datacite['formats'].apply(\n",
    "    lambda formats: (\n",
    "        '; '.join([format_map.get(fmt, fmt) for fmt in formats])\n",
    "        if isinstance(formats, set) else formats\n",
    "        )\n",
    "    )    \n",
    "    # df_datacite['fileFormatsSet'] = df_datacite['mimeTypeSet'].apply(lambda x: '; '.join([format_map.get(fmt, fmt) for fmt in x]) if x != 'no files' else 'no files')\n",
    "    \n",
    "    ##look for software file formats\n",
    "    softwareFormats = set(config['SOFTWARE_FORMATS'].values())\n",
    "    # Assume softwareFormats is a set of friendly software format names\n",
    "    df_datacite['containsCode'] = df_datacite['fileFormat'].apply(\n",
    "        lambda x: any(part.strip() in softwareFormats for part in x.split(';')) if isinstance(x, str) else False\n",
    "    )\n",
    "    df_datacite['onlyCode'] = df_datacite['fileFormat'].apply(\n",
    "    lambda x: all(part.strip() in softwareFormats for part in x.split(';')) if isinstance(x, str) else False\n",
    "    )\n",
    "\n",
    "    ##assess 'descriptiveness of dataset title'\n",
    "    words = config['WORDS']\n",
    "    ###add integers\n",
    "    numbers = list(map(str, range(1, 1000000)))\n",
    "    ###combine all into a single set\n",
    "    nondescriptive_words = set(\n",
    "        words['articles'] +\n",
    "        words['conjunctions'] +\n",
    "        words['prepositions'] +\n",
    "        words['auxiliary_verbs'] +\n",
    "        words['possessives'] +\n",
    "        words['descriptors'] +\n",
    "        words['order'] +\n",
    "        words['version'] +\n",
    "        numbers\n",
    "    )\n",
    "\n",
    "    df_datacite['title_reformatted'] = df_datacite['title'].str.replace('_', ' ') #gets around text linked by underscores counting as 1 word\n",
    "    df_datacite['title_reformatted'] = df_datacite['title_reformatted'].str.lower()\n",
    "    df_datacite[['total_word_count_title', 'descriptive_word_count_title']] = df_datacite['title_reformatted'].apply(lambda x: pd.Series(count_words(x)))\n",
    "\n",
    "    df_datacite['descriptive_word_count_title'] = df_datacite.apply(adjust_descriptive_count, axis=1)\n",
    "    df_datacite['nondescriptive_word_count_title'] = df_datacite['total_word_count_title'] - df_datacite['descriptive_word_count_title']\n",
    "\n",
    "    #standardizing licenses\n",
    "    df_datacite['rights'] = df_datacite['rights'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x).astype(str).str.strip('[]')\n",
    "    df_datacite['rights_standardized'] = 'Rights unclear'  #default value\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Creative Commons Zero|CC0'), 'rights_standardized'] = 'CC0'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Creative Commons Attribution Non Commercial Share Alike'), 'rights_standardized'] = 'CC BY-NC-SA'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Creative Commons Attribution Non Commercial'), 'rights_standardized'] = 'CC BY-NC'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Creative Commons Attribution 3.0|Creative Commons Attribution 4.0|Creative Commons Attribution-NonCommercial'), 'rights_standardized'] = 'CC BY'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('GNU General Public License'), 'rights_standardized'] = 'GNU GPL'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Apache License'), 'rights_standardized'] = 'Apache'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('MIT License'), 'rights_standardized'] = 'MIT'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('BSD'), 'rights_standardized'] = 'BSD'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('ODC-BY'), 'rights_standardized'] = 'ODC-BY'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Open Access'), 'rights_standardized'] = 'Rights unclear'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Closed Access'), 'rights_standardized'] = 'Restricted access'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Restricted Access'), 'rights_standardized'] = 'Restricted access'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('Databrary'), 'rights_standardized'] = 'Custom terms'\n",
    "    df_datacite.loc[df_datacite['rights'].str.contains('UCAR'), 'rights_standardized'] = 'Custom terms'\n",
    "    df_datacite.loc[df_datacite['rights'] == '', 'rights_standardized'] = 'Rights unclear'\n",
    "\n",
    "    df_datacite.to_csv(f'outputs/{todayDate}_datacite-output-for-metadata-assessment.csv', index=False) \n",
    "\n",
    "    #subsetting dataframe\n",
    "    df_datacite_pruned = df_datacite[['publisher', 'doi', 'publicationYear', 'title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'source', 'type']]\n",
    "\n",
    "    #adding column for select high-volume repos\n",
    "    repo_mapping = {\n",
    "    'Dryad': 'Dryad',\n",
    "    'Zenodo': 'Zenodo',\n",
    "    'Texas Data Repository': 'Texas Data Repository'\n",
    "    }\n",
    "\n",
    "    df_datacite_pruned['repository2'] = df_datacite_pruned['publisher'].map(repo_mapping).fillna('Other')\n",
    "\n",
    "    df_datacite_pruned['uni_lead'] = df_datacite_pruned.apply(determine_affiliation, axis=1)\n",
    "\n",
    "    #standardizing repositories with multiple versions of name in dataframe\n",
    "    ##different institutions may need to add additional repositories; nothing will happen if you don't have any of the ones listed below and don't comment the lines out\n",
    "    df_datacite_pruned['publisher'] = df_datacite_pruned['publisher'].fillna('None')\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Digital Rocks', case=False), 'publisher'] = 'Digital Porous Media Portal'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Environmental System Science Data Infrastructure for a Virtual Ecosystem', case=False), 'publisher'] = 'ESS-DIVE'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Texas Data Repository|Texas Research Data Repository', case=False), 'publisher'] = 'Texas Data Repository'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('ICPSR', case=True), 'publisher'] = 'ICPSR'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Environmental Molecular Sciences Laboratory', case=True), 'publisher'] = 'Environ Mol Sci Lab'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('BCO-DMO', case=True), 'publisher'] = 'Biol Chem Ocean Data Mgmt Office'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Taylor & Francis|SAGE|The Royal Society|SciELO journals', case=True), 'publisher'] = 'figshare'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Oak Ridge', case=True), 'publisher'] = 'Oak Ridge National Laboratory'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('PARADIM', case=True), 'publisher'] = 'PARADIM'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('4TU', case=True), 'publisher'] = '4TU.ResearchData'\n",
    "    df_datacite_pruned.loc[df_datacite_pruned['publisher'].str.contains('Scratchpads', case=True), 'publisher'] = 'Global Biodiversity Information Facility (GBIF)'\n",
    "\n",
    "    #EDGE CASES, likely unnecessary for other universities, but you will need to find your own edge cases\n",
    "    ##confusing metadata with UT Austin (but not Dataverse) listed as publisher; have to be manually adjusted over time\n",
    "    if austin:\n",
    "        df_datacite_pruned.loc[(df_datacite_pruned['doi'].str.contains('10.11578/dc')) & (df_datacite_pruned['publisher'].str.contains('University of Texas')), 'publisher'] = 'Department of Energy (DOE) CODE'\n",
    "        ##other edge cases\n",
    "        df_datacite_pruned.loc[df_datacite_pruned['doi'].str.contains('10.23729/547d8c47-3723-4396-8f84-322c02ccadd0'), 'publisher'] = 'Finnish Fairdata' #labeled publisher as author's name\n",
    "\n",
    "    #adding categorization\n",
    "    ##identifying institutional repositories that are not the Texas Data Repository\n",
    "    df_datacite_pruned['non_TDR_IR'] = np.where(df_datacite_pruned['publisher'].str.contains('University|UCLA|UNC|Harvard|ASU|Dataverse', case=True), 'non-TDR institutional', 'not university or TDR')\n",
    "    df_datacite_pruned['US_federal'] = np.where(df_datacite_pruned['publisher'].str.contains('NOAA|NIH|NSF|U.S.|DOE|DOD|DOI|National|Designsafe', case=True), 'Federal US repo', 'not federal US repo')\n",
    "    df_datacite_pruned['GREI'] = np.where(df_datacite_pruned['publisher'].str.contains('Dryad|figshare|Zenodo|Vivli|Mendeley|Open Science Framework', case=False), 'GREI member', 'not GREI member')\n",
    "\n",
    "    df_datacite_pruned = df_datacite_pruned.rename(columns={'publisher': 'repository'})\n",
    "\n",
    "    df_datacite_pruned.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eee7ed",
   "metadata": {},
   "source": [
    "### Step 3: Figshare workaround\n",
    "\n",
    "This step presents two different partial solutions to the general absence of affiliation metadata recording/crosswalking for Figshare deposits. The first solution takes advantage of how the *publisher* is listed for publisher-mediated deposits (as the scholarly publisher, not as 'figshare') to identify deposits mediated by a certain scholarly publisher that mints the Figshare DOIs through DataCite (not all of them do; Taylor & Francis is an example of one that does). These mediated deposits almost always indicate the DOI of the article that they are linked to in a metadata field. It then retrieves a list of articles published by that publisher and affiliated to the focal institution. The article DOIs can then be cross-matched against the mediated Figshare deposits' related identifier DOIs to identify Figshare deposits without affiliation metadata that can be conclusively linked to an affiliated article.\n",
    "\n",
    "The second solution is more time-intensive and less efficient, but it may be the most viable approach for certain use cases, including publishers that mint their mediated Figshare DOIs through Crossref instead of DataCite. It also retrieves affiliated articles from a scholarly publisher partner (e.g., Taylor & Francis) but then constructs a hypothetical dataset DOI based on the observation that many publishers use a system in which '.s00*' is appended to the article DOI, where '*' is a sequential integer. Each hypothetical dataset DOI is then tested to see if it exists. This approach's shortcomings lie in the fact that not all publisher partners mint Figshare DOIs in this way; that there is no way of knowing how many sequential integers to test; and that additional steps would be necessary to retrieve the same level of metadata as the first solution (would require another API call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e334b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### FIGSHARE WORKFLOW ######\n",
    "#These sections are for cleaning up identified figshare deposits or identifying associated ones that lack affiliation metadata\n",
    "\n",
    "if loadPreviousData:\n",
    "    #for reading in previously generated file of all associated datasets\n",
    "    print('Reading in previous DataCite output file\\n')\n",
    "    directory = './outputs' \n",
    "    pattern = '_full-concatenated-dataframe.csv'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(reverse=True)\n",
    "    latest_file = None\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            latest_file = file\n",
    "            break\n",
    "\n",
    "    if latest_file:\n",
    "        file_path = os.path.join(directory, latest_file)\n",
    "        df_datacite_pruned = pd.read_csv(file_path)\n",
    "        print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "    else:\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "### This codeblock will retrieve all figshare deposits with a listed journal/publisher as 'publisher,' extract related identifiers, retrieve all articles published by a certain publisher, cross-reference article DOIs against dataset related identifiers, and produce a match list. ###\n",
    "if figshareWorkflow1:\n",
    "\n",
    "    #figshare DOIs sometimes have a .v* for version number; this toggles whether to include them (True) or only include the parent (False)\n",
    "    countVersions = False\n",
    "\n",
    "    #pull in map of publisher names and OpenAlex codes\n",
    "    publisher_mapping = config['FIGSHARE_PARTNERS']\n",
    "    #create empty object to store results\n",
    "    data_select_datacite = [] \n",
    "    data_select_openalex = []\n",
    "\n",
    "    for publisher_name, openalex_code in publisher_mapping.items():\n",
    "        try:\n",
    "            #update both params for each publisher in map\n",
    "            params_openalex = {\n",
    "            'filter': f'authorships.institutions.ror:https://ror.org/00hj54h04,type:article,from_publication_date:2000-01-01,locations.source.host_organization:{openalex_code}',\n",
    "            'per-page': config['VARIABLES']['PAGE_SIZES']['openalex'],\n",
    "            'select': 'id,doi,title,authorships,primary_location,type',\n",
    "            'mailto': config['EMAIL']['user_email']\n",
    "            }\n",
    "            j = 0\n",
    "            #define different number of pages to retrieve from OpenAlex API based on 'test' vs. 'prod' env\n",
    "            page_limit_openalex = config['VARIABLES']['PAGE_LIMITS']['openalex_test'] if test else config['VARIABLES']['PAGE_LIMITS']['openalex_prod']\n",
    "            #DataCite params (different from general affiliation-based retrieval params)\n",
    "            ## !! Warning: if you do not set a resourceType in the query (recommended if you want to get broad coverage), this will be a very large retrieval. In the test env, there may not be enough records to find a match with a university-affiliated article !!\n",
    "            params_datacite_figshare = {\n",
    "                'affiliation': 'true',\n",
    "                'query': f'(publisher:\"{publisher_name}\") AND types.resourceTypeGeneral:\"{resourceType}\"',\n",
    "                'page[size]': config['VARIABLES']['PAGE_SIZES']['datacite'],\n",
    "                'page[cursor]': 1,\n",
    "            }\n",
    "            page_start_datacite = config['VARIABLES']['PAGE_STARTS']['datacite'] #reset to 0 (default) after large-scale general retrieval through DataCite\n",
    "\n",
    "            print(f'Starting DataCite retrieval for {publisher_name}.\\n')\n",
    "            data_datacite = retrieve_all_data_datacite(url_datacite, params_datacite_figshare)\n",
    "            print(f'Number of datasets found by DataCite API: {len(data_datacite)}\\n')\n",
    "            for item in data_datacite:\n",
    "                attributes = item.get('attributes', {})\n",
    "                doi_dc = attributes.get('doi', None)\n",
    "                publisher_dc = attributes.get('publisher', '')\n",
    "                # publisher_year_dc = attributes.get('publicationYear', '')\n",
    "                registered = attributes.get('registered', '')\n",
    "                if registered:\n",
    "                    publisher_year_dc = datetime.fromisoformat(registered[:-1]).year\n",
    "                else:\n",
    "                    publisher_year_dc = None\n",
    "                title_dc = attributes.get('titles', [{}])[0].get('title', '')\n",
    "                creators_dc = attributes.get('creators', [{}])\n",
    "                affiliations_dc = [affiliation.get('name', '') for creator in creators_dc for affiliation in creator.get('affiliation', [{}])]\n",
    "                related_identifiers = attributes.get('relatedIdentifiers', [])\n",
    "                container_dc = attributes.get('container', {})\n",
    "                container_identifier_dc = container_dc.get('identifier', None)\n",
    "                types = attributes.get('types', {})\n",
    "                resourceType = types.get('resourceTypeGeneral', '')\n",
    "\n",
    "                for rel in related_identifiers: #'explodes' deposits with multiple relatedIdentifiers\n",
    "                    data_select_datacite.append({\n",
    "                        'doi': doi_dc,\n",
    "                        'repository': publisher_dc,\n",
    "                        'publicationYear': publisher_year_dc,\n",
    "                        'title': title_dc,\n",
    "                        'creators': creators_dc,\n",
    "                        'affiliations': affiliations_dc,\n",
    "                        'relationType': rel.get('relationType'),\n",
    "                        'relatedIdentifier': rel.get('relatedIdentifier'),\n",
    "                        'relatedIdentifierType': rel.get('relatedIdentifierType'),\n",
    "                        'containerIdentifier': container_identifier_dc,\n",
    "                        'type': resourceType\n",
    "                    })\n",
    "            print(f'Starting OpenAlex retrieval for {publisher_name}.\\n')\n",
    "            openalex = retrieve_all_data_openalex(url_openalex, params_openalex)\n",
    "            for item in openalex:\n",
    "                doi = item.get('doi')\n",
    "                title = item.get('title')\n",
    "                publication_year = item.get('publication_year')\n",
    "                source_display_name = item.get('primary_location', {}).get('source', {}).get('display_name')\n",
    "                \n",
    "                for authorship in item.get('authorships', []):\n",
    "                    if authorship.get('author_position') == 'first':\n",
    "                        first_author = authorship.get('author', {}).get('display_name')\n",
    "                        first_affiliation = [inst.get('display_name') for inst in authorship.get('institutions', [])]\n",
    "                    if authorship.get('author_position') == 'last':\n",
    "                        last_author = authorship.get('author', {}).get('display_name')\n",
    "                        last_affiliation = [inst.get('display_name') for inst in authorship.get('institutions', [])]\n",
    "                        \n",
    "                        data_select_openalex.append({\n",
    "                            'doi_article': doi,\n",
    "                            'title_article': title,\n",
    "                            'publication_year': publication_year,\n",
    "                            'journal': source_display_name,\n",
    "                            'first_author': first_author,\n",
    "                            'first_affiliation': first_affiliation,\n",
    "                            'last_author': last_author,\n",
    "                            'last_affiliation': last_affiliation\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f'An error occurred for publisher {publisher_name}: {e}')\n",
    "            continue  # Skip to the next iteration\n",
    "\n",
    "    df_datacite_initial = pd.json_normalize(data_select_datacite)\n",
    "    df_datacite_initial.to_csv(f'outputs/{todayDate}_figshare-discovery-initial.csv', index=False)\n",
    "    \n",
    "    if countVersions:\n",
    "        ##These steps will count different versions as distinct datasets and remove the 'parent' (redundant with most recent version)\n",
    "        df_datacite_initial['base'] = df_datacite_initial['doi'].apply(lambda x: x.split('.v')[0])\n",
    "        df_datacite_initial['version'] = df_datacite_initial['doi'].apply(lambda x: int(x.split('.v')[1]) if '.v' in x else 0)\n",
    "        max_versions = df_datacite_initial.groupby('base')['version'].max().reset_index()\n",
    "        df_datacite_initial = df_datacite_initial.merge(max_versions, on='base', suffixes=('', '_max'))\n",
    "        df_deduplicated = df_datacite_initial(subset='base')\n",
    "    else:\n",
    "        ##This step will remove all child deposits with a .v*  to retain only the 'parent'\n",
    "        df_deduplicated = df_datacite_initial[~df_datacite_initial['doi'].str.contains(r'\\.v\\d+$')]\n",
    "\n",
    "    df_datacite_supplement = df_deduplicated[df_deduplicated['relationType'] == 'IsSupplementTo']\n",
    "    #mediated workflow sometimes creates individual deposit for each file, want to treat as single dataset here\n",
    "    df_datacite_supplement['hadPartialDuplicate'] = df_datacite_supplement.duplicated(subset='relatedIdentifier', keep='first')\n",
    "    df_datacite_supplement_dedup = df_datacite_supplement.drop_duplicates(subset='relatedIdentifier', keep='first')\n",
    "    \n",
    "    df_openalex = pd.json_normalize(data_select_openalex)\n",
    "    df_openalex['relatedIdentifier'] = df_openalex['doi_article'].str.replace('https://doi.org/', '')\n",
    "\n",
    "    #output all UT linked deposits, no deduplication (for Figshare validator workflow)\n",
    "    df_openalex_datacite = pd.merge(df_openalex, df_datacite_supplement, on='relatedIdentifier', how='left')\n",
    "    df_openalex_datacite = df_openalex_datacite[df_openalex_datacite['doi'].notnull()]\n",
    "    df_openalex_datacite.to_csv(f'outputs/{todayDate}_figshare-discovery-all.csv', index=False)\n",
    "    df_openalex_datacite = df_openalex_datacite.drop_duplicates(subset='relatedIdentifier', keep='first')\n",
    "\n",
    "    #working with deduplicated dataset for rest of process\n",
    "    df_openalex_datacite_dedup = pd.merge(df_openalex, df_datacite_supplement_dedup, on='relatedIdentifier', how='left')\n",
    "    new_figshare = df_openalex_datacite_dedup[df_openalex_datacite_dedup['doi'].notnull()]\n",
    "    new_figshare = new_figshare.drop_duplicates(subset='doi', keep='first')\n",
    "    new_figshare.to_csv(f'outputs/{todayDate}_figshare-discovery-deduplicated.csv', index=False)\n",
    "    new_figshare = new_figshare[['doi','publicationYear','title', 'first_author', 'first_affiliation', 'last_author', 'last_affiliation', 'type']]\n",
    "\n",
    "    #standardizing licenses\n",
    "    # new_figshare['rights'] = new_figshare['rights'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x).astype(str).str.strip('[]')\n",
    "    # new_figshare['rights_standardized'] = 'Rights unclear'  #default value\n",
    "    # new_figshare.loc[new_figshare['rights'].str.contains('Creative Commons Zero|CC0'), 'rights_standardized'] = 'CC0'\n",
    "    # new_figshare.loc[new_figshare['rights'].str.contains('Creative Commons Attribution Non Commercial Share Alike'), 'rights_standardized'] = 'CC BY-NC-SA'\n",
    "    # new_figshare.loc[new_figshare['rights'].str.contains('Creative Commons Attribution Non Commercial'), 'rights_standardized'] = 'CC BY-NC'\n",
    "    # new_figshare.loc[new_figshare['rights'].str.contains('Creative Commons Attribution 3.0|Creative Commons Attribution 4.0|Creative Commons Attribution-NonCommercial'), 'rights_standardized'] = 'CC BY'\n",
    "    # new_figshare.loc[new_figshare['rights'] == '', 'rights_standardized'] = 'Rights unclear'\n",
    "\n",
    "    #adding in columns to reconcatenate with full dataset\n",
    "    new_figshare['first_affiliation'] = new_figshare['first_affiliation'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)  \n",
    "    new_figshare['last_affiliation'] = new_figshare['last_affiliation'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)    \n",
    "    new_figshare['uni_lead'] = new_figshare.apply(determine_affiliation, axis=1)\n",
    "    new_figshare['repository'] = 'figshare'\n",
    "    new_figshare['source'] = 'DataCite+' #slight differentiation from records only retrieved from DataCite\n",
    "    new_figshare['repository2'] = 'Other'\n",
    "    new_figshare['non_TDR_IR'] = 'not university or TDR'\n",
    "    new_figshare['US_federal'] = 'not federal US repo'\n",
    "    new_figshare['GREI'] = 'GREI member'\n",
    "\n",
    "    df_datacite_plus = pd.concat([df_datacite_pruned, new_figshare], ignore_index=True)\n",
    "    #de-duplicating in case some DOIs were caught twice (for the few publishers that do cross-walk affiliation metadata), you could use a sorting method to determine which one to 'keep'; the default will retain the ones returned from the main workflow\n",
    "    df_datacite_plus_dedup = df_datacite_plus.drop_duplicates(subset='doi', keep='first')\n",
    "    df_datacite_plus_dedup.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-figshare.csv', index=False)\n",
    "\n",
    "### This codeblock identifies publishers known to create figshare deposits (can be any object resource type) with a '.s00*' system, finds affiliated articles, constructs a hypothetical figshare DOI for them, and tests its existence ###\n",
    "# !! Warning: Depending on the number of articles, this can be an extremely time-intensive process !! #\n",
    "\n",
    "if figshareWorkflow2:\n",
    "    #toggle to select which indexer to use: 'OpenAlex' or 'Crossref'\n",
    "    indexer = 'OpenAlex'\n",
    "\n",
    "    #OpenAlex params\n",
    "    j = 0\n",
    "    page_limit_openalex = config['VARIABLES']['PAGE_LIMITS']['openalex']\n",
    "\n",
    "    #Crossref params\n",
    "    k = 0\n",
    "    page_limit_crossref = config['VARIABLES']['PAGE_LIMITS']['crossref']\n",
    "    params_crossref_journal = {\n",
    "        'select': 'DOI,prefix,title,author,container-title,publisher,created',\n",
    "        'filter': 'type:journal-article',\n",
    "        'rows': config['VARIABLES']['PAGE_SIZES']['crossref'],\n",
    "        'query': 'affiliation:University+Texas+Austin',\n",
    "        'mailto': config['EMAIL']['user_email'],\n",
    "        'cursor': '*',\n",
    "    }\n",
    "\n",
    "    params_openalex = {\n",
    "        'filter': 'authorships.institutions.ror:https://ror.org/00hj54h04,locations.source.host_organization:https://openalex.org/P4310315706', #PLOS ID in OpenAlex\n",
    "        'per-page': config['VARIABLES']['PAGE_SIZES']['openalex'],\n",
    "        'select': 'id,doi,title,authorships,primary_location,type',\n",
    "        'mailto': config['EMAIL']['user_email']\n",
    "    }\n",
    "\n",
    "    #JSON dictionary of journals for Crossref API query (PLOS in this example)\n",
    "    with open('journal_list.json', 'r') as file:\n",
    "        journal_list = json.load(file)\n",
    "\n",
    "    if indexer == 'OpenAlex':\n",
    "        openalex = retrieve_all_data_openalex(url_openalex, params_openalex)\n",
    "        df_openalex = pd.json_normalize(openalex)\n",
    "        df_openalex['hypothetical_dataset'] = df_openalex['doi'] + '.s001'\n",
    "        \n",
    "        #Check if each DOI with suffix redirects to a real page and create a new column\n",
    "        df_openalex['Valid'] = df_openalex['hypothetical_dataset'].apply(check_link)\n",
    "        df_openalex.to_csv(f'outputs/{todayDate}_openalex-articles-with-hypothetical-deposits.csv', index=False)\n",
    "        print(f'Number of valid datasets: {len(df_openalex)}.')\n",
    "    else:\n",
    "        crossref_data = retrieve_all_journals(url_crossref_issn, journal_list)\n",
    "\n",
    "        data_journals_select = []\n",
    "        for item in crossref_data:\n",
    "            publisher = item.get('publisher', None)\n",
    "            journal = item.get('container-title', None)[0]\n",
    "            doi = item.get('DOI', '')\n",
    "            title_list = item.get('title', [])\n",
    "            title = title_list[0] if title_list else None\n",
    "            author = item.get('author', None)\n",
    "            created = item.get('created', {})\n",
    "            createdDate = created.get('date-time', None)\n",
    "            \n",
    "            data_journals_select.append({\n",
    "                'publisher': publisher,\n",
    "                'journal': journal, \n",
    "                'doi': doi,\n",
    "                'author': author,\n",
    "                'title': title,\n",
    "                'published': createdDate,\n",
    "        })\n",
    "\n",
    "        df_crossref = pd.DataFrame(data_journals_select)\n",
    "        df_crossref['doi_html'] = 'https://doi.org/' + df_crossref['doi']\n",
    "        df_crossref['hypothetical_dataset'] = df_crossref['doi_html'] + '.s001'\n",
    "\n",
    "        # Check if each DOI with suffix redirects to a real page and create a new column\n",
    "        df_crossref['Valid'] = df_crossref['hypothetical_dataset'].apply(check_link)\n",
    "        df_crossref.to_csv(f'outputs/{todayDate}_crossref-articles-with-hypothetical-deposits.csv', index=False)\n",
    "        print(f'Number of valid datasets: {len(df_crossref)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93871ccd",
   "metadata": {},
   "source": [
    "### Step 4: NCBI workflow\n",
    "\n",
    "This step provides two different mechanisms for programmatically retrieving metadata on institutionally-affiliated BioProjects (the object most similar to a 'dataset' in the DataCite or Crossref schema), one based on automating a manual download of an XML file and the other using the *biopython* module to access the Entrez system. Either way, users should refer to the [NCBI documentation](https://www.ncbi.nlm.nih.gov/books/NBK25497/) for guidance on rate limiting and other restrictions on use. The output is returned as an XML and then coerced into the same structure as the DataCite output, although not all fields are equivalent (e.g., the project number is substituted for a DOI); certain fields are also metadata-deficient since NCBI does not use the DataCite metadata schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NCBI Bioproject #####\n",
    "if ncbiWorkflow:\n",
    "    print('Starting NCBI process.\\n')\n",
    "\n",
    "    #set path for browser\n",
    "    ##works differently for Jupyter vs. .py file\n",
    "    try:\n",
    "        #for .py file\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        #for Jupyter\n",
    "        script_dir = os.getcwd()\n",
    "    if test:\n",
    "        outputs_dir = os.path.join(script_dir, 'test/outputs')\n",
    "    else:\n",
    "        outputs_dir = os.path.join(script_dir, 'outputs')\n",
    "\n",
    "    #check if previous output file exists\n",
    "    directory = './outputs'\n",
    "    pattern = 'bioproject_result'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            existingOutput = True\n",
    "            print(f'A previous \"{pattern}\" download was found in the directory \"{directory}\".')\n",
    "            break\n",
    "    else:\n",
    "        existingOutput = False\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    #read in config file\n",
    "    if not loadNCBIdata:\n",
    "        institution_name = config['INSTITUTION']['name']\n",
    "        #URL encode name\n",
    "        encoded_institution_name = quote(institution_name)\n",
    "        if not biopython:\n",
    "            #set up temporary Firefox 'profile' to direct downloads (profile not saved outside of script)\n",
    "            options = Options()\n",
    "            options.set_preference('browser.download.folderList', 2)\n",
    "            options.set_preference('browser.download.dir', outputs_dir)\n",
    "            options.set_preference('browser.helperApps.neverAsk.saveToDisk', 'application/octet-stream')\n",
    "            #blocking pop-up window to cancel download\n",
    "            options.set_preference('browser.download.manager.showWhenStarting', False)\n",
    "            options.set_preference('browser.download.manager.focusWhenStarting', False)\n",
    "            options.set_preference('browser.download.useDownloadDir', True)\n",
    "            options.set_preference('browser.download.manager.alertOnEXEOpen', False)\n",
    "            options.set_preference('browser.download.manager.closeWhenDone', True)\n",
    "            options.set_preference('browser.download.manager.showAlertOnComplete', False)\n",
    "            options.set_preference('browser.download.manager.useWindow', False)\n",
    "            options.set_preference('services.sync.prefs.sync.browser.download.manager.showWhenStarting', False)\n",
    "            options.set_preference('browser.download.alwaysOpenPanel', False)  # Disable the download panel\n",
    "            options.set_preference('browser.download.panel.shown', False)  # Ensure the download panel is not shown\n",
    "\n",
    "            #initialize Selenium WebDriver\n",
    "            driver = webdriver.Firefox(options=options)\n",
    "            ##searches all fields; searching Submitter Organization specifically does not recover all results\n",
    "            ncbi_url = f'https://www.ncbi.nlm.nih.gov/bioproject?term={encoded_institution_name}'\n",
    "            driver.get(ncbi_url)\n",
    "\n",
    "            try:\n",
    "                #Load page and find the 'Send to' dropdown\n",
    "                send_to_link = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'sendto')))\n",
    "                send_to_link.click()\n",
    "\n",
    "                #Load dropdown and select 'File' radio button\n",
    "                file_option = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'dest_File')))\n",
    "                file_option.click()\n",
    "\n",
    "                #Load 'Format' dropdown and select 'XML'\n",
    "                format_dropdown = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.ID, 'file_format')))\n",
    "                format_dropdown.click()\n",
    "                xml_option = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//option[@value='xml']\")))\n",
    "                xml_option.click()\n",
    "\n",
    "                #click the 'Create File' button\n",
    "                create_file_button = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//button[@cmd='File']\")))\n",
    "                create_file_button.click()\n",
    "\n",
    "                print('Download complete, about to close window.\\n')\n",
    "                time.sleep(10)\n",
    "\n",
    "                #overwrite any existing file with 'bioproject_result.xml' rather than continually creating new version with (*) appended in filename (e.g., bioproject_result(1).xml)\n",
    "                ##will delete previous one and then rename the just-downloaded one with (*) appended\n",
    "                if existingOutput:\n",
    "                    downloaded_file = max([os.path.join(outputs_dir, f) for f in os.listdir(outputs_dir)], key=os.path.getctime)\n",
    "                    target_file = os.path.join(outputs_dir, 'bioproject_result.xml')\n",
    "                    if os.path.exists(target_file):\n",
    "                        os.remove(target_file)\n",
    "                        print(f'Deleted existing file: {target_file}')\n",
    "                    os.rename(downloaded_file, target_file)\n",
    "                    print(f'Renamed {downloaded_file} to {target_file}')\n",
    "\n",
    "            except TimeoutException:\n",
    "                print('Element not found or not clickable within the specified time.')\n",
    "\n",
    "            finally:\n",
    "                driver.quit()\n",
    "        else:\n",
    "            print(\"Starting biopython retrieval\")\n",
    "            #NCBI requires email to be provided\n",
    "            Entrez.email = f'{email}'\n",
    "\n",
    "            #if you get a free API key, increases rate limit from 3/sec to 10/sec\n",
    "            #Entrez.api_key = 'YOUR_NCBI_API_KEY'\n",
    "\n",
    "            search_term = config['INSTITUTION']['name'] #check that this string is the right one in the web interface\n",
    "            handle = Entrez.esearch(db='bioproject', term=search_term, usehistory='y', retmax=1200) #currently at 955\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            webenv = record['WebEnv']\n",
    "            query_key = record['QueryKey']\n",
    "\n",
    "            handle = Entrez.efetch(db='bioproject', query_key=query_key, WebEnv=webenv, retmode='xml')\n",
    "            xml_data = handle.read().decode('utf-8')\n",
    "            handle.close()\n",
    "\n",
    "            with open(f'{outputs_dir}/bioproject_result.xml', 'w', encoding='utf-8') as f:\n",
    "                f.write(xml_data)\n",
    "\n",
    "            print(f'Saved XML record to \"{outputs_dir}/bioproject_result.xml\"')\n",
    "\n",
    "    #read in XML file (required regardless of whether you downloaded version in this run or not)\n",
    "    print('Loading previously generated XML file.\\n')\n",
    "    with open(f'{outputs_dir}/bioproject_result.xml', 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    #wrapping in a root element for parsing if from Selenium output\n",
    "    if not data.strip().startswith('<?xml'):\n",
    "        data = f'<root>{data}</root>'\n",
    "    root = ET.fromstring(data)\n",
    "\n",
    "    #select certain fields from XML\n",
    "    def filter_ncbi(doc):\n",
    "        data_select = {}\n",
    "        project = doc.find('Project')\n",
    "        if project is not None:\n",
    "            project_id = project.find('ProjectID')\n",
    "            if project_id is not None:\n",
    "                archive_id = project_id.find('ArchiveID')\n",
    "                if archive_id is not None:\n",
    "                    data_select['doi'] = archive_id.get('accession') #this is not a DOI but will be aligned with DOI column in main dataframe\n",
    "                    data_select['repository'] = archive_id.get('archive')\n",
    "                    data_select['ID'] = archive_id.get('id')\n",
    "                center_id = project_id.find('CenterID')\n",
    "                if center_id is not None:\n",
    "                    data_select['Center'] = center_id.get('center')\n",
    "                    data_select['CenterName'] = center_id.text\n",
    "            project_descr = project.find('ProjectDescr')\n",
    "            if project_descr is not None:\n",
    "                name = project_descr.find('Name')\n",
    "                if name is not None:\n",
    "                    data_select['Name'] = name.text\n",
    "                title = project_descr.find('Title')\n",
    "                if title is not None:\n",
    "                    data_select['title'] = title.text\n",
    "                description = project_descr.find('Description')\n",
    "                if description is not None:\n",
    "                    data_select['Description'] = description.text\n",
    "        submission = doc.find('Submission')\n",
    "        if submission is not None:\n",
    "            data_select['LastUpdate'] = submission.get('last_update')\n",
    "            data_select['SubmissionID'] = submission.get('submission_id')\n",
    "            data_select['Submitted'] = submission.get('submitted')\n",
    "            organization = submission.find('.//Organization/Name')\n",
    "            if organization is not None:\n",
    "                data_select['Affiliation'] = organization.text\n",
    "\n",
    "        return data_select\n",
    "\n",
    "    #cxtract data from each element and store in a list\n",
    "    data_list = []\n",
    "    for doc in root.findall('DocumentSummary'):\n",
    "        data_list.append(filter_ncbi(doc))\n",
    "\n",
    "    #dataframe conversion and standardization for alignment with main dataframe\n",
    "    ncbi = pd.DataFrame(data_list)\n",
    "    ncbi['publicationYear'] = pd.to_datetime(ncbi['Submitted']).dt.year\n",
    "    ##look for one of the permutation strings listed in config.json\n",
    "    ncbi['first_affiliation'] = ncbi.apply(lambda row: next((perm for perm in ut_variations if perm in row['Affiliation']), None), axis=1)\n",
    "    ncbi['last_affiliation'] = ncbi.apply(lambda row: next((perm for perm in ut_variations if perm in row['Affiliation']), None), axis=1)\n",
    "\n",
    "    ##removing hits that have one of the keywords in a different field like the title\n",
    "    ncbi_df_select = ncbi[ncbi['Affiliation'].str.contains(uni_identifier)]\n",
    "    ncbi_df_select = ncbi_df_select[['repository','doi', 'publicationYear', 'title','first_affiliation']]   \n",
    "    ##adding columns for alignment with main dataframe\n",
    "    ncbi_df_select['first_author'] = 'Not specified'\n",
    "    ncbi_df_select['last_author'] = 'Not specified'\n",
    "    ncbi_df_select['uni_lead'] = 'Affiliated (authorship unclear)'\n",
    "    ncbi_df_select['source'] = 'NCBI'\n",
    "    ncbi_df_select['type'] = 'Dataset'\n",
    "    ncbi_df_select['repository2'] = 'NCBI'\n",
    "    ncbi_df_select['non_TDR_IR'] = 'not university or TDR'\n",
    "    ncbi_df_select['US_federal'] = 'Federal US repo'\n",
    "    ncbi_df_select['GREI'] = 'not GREI member'\n",
    "    # ncbi_df_select['rights'] = 'Rights unclear'\n",
    "    # ncbi_df_select['rights_standardized'] = 'Rights unclear'\n",
    "\n",
    "    if loadPreviousDataPlus:\n",
    "        #for reading in previously generated file of all associated datasets\n",
    "        print('Reading in existing DataCite+ output file\\n')\n",
    "        directory = './outputs' \n",
    "        pattern = '_full-concatenated-dataframe-plus-figshare.csv'\n",
    "\n",
    "        files = os.listdir(directory)\n",
    "        files.sort(reverse=True)\n",
    "        latest_file = None\n",
    "        for file in files:\n",
    "            if pattern in file:\n",
    "                latest_file = file\n",
    "                break\n",
    "\n",
    "        if latest_file:\n",
    "            file_path = os.path.join(directory, latest_file)\n",
    "            df_datacite_plus_dedup = pd.read_csv(file_path)\n",
    "            print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "        else:\n",
    "            print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    df_datacite_plus_ncbi = pd.concat([df_datacite_plus_dedup, ncbi_df_select], ignore_index=True)\n",
    "    df_datacite_plus_ncbi.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-figshare-ncbi.csv', index=False)\n",
    "\n",
    "    # ncbi_df_select.to_csv(f'outputs/{todayDate}_NCBI-select-output-aligned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb5628",
   "metadata": {},
   "source": [
    "### Step 5: Concatenating with externally produced Crossref results\n",
    "\n",
    "If you made a separate Crossref query using the provided accessory script, the results will need to be pulled in to be concatenated with the DataCite results. This step will read in the Crossref results and concatenate them with the DataCite results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec66b32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to load in externally queried Crossref data\n",
    "if any([loadPreviousData, loadPreviousDataPlus, loadPreviousDataPlusNCBI]) and loadCrossrefData:\n",
    "    print('Reading in existing DataCite+ output file\\n')\n",
    "    directory = './outputs'\n",
    "    if loadPreviousDataPlusNCBI: \n",
    "        pattern = '_full-concatenated-dataframe-plus-figshare-ncbi.csv'\n",
    "    elif loadPreviousDataPlus:\n",
    "        pattern = '_full-concatenated-dataframe-plus.csv'\n",
    "    elif loadPreviousData:\n",
    "        pattern = '_full-concatenated-dataframe.csv'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(reverse=True)\n",
    "    latest_file = None\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            latest_file = file\n",
    "            break\n",
    "\n",
    "    if latest_file:\n",
    "        file_path = os.path.join(directory, latest_file)\n",
    "        df_datacite_plus_ncbi = pd.read_csv(file_path)\n",
    "        print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "    else:\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    #set path for browser\n",
    "    print('\\nReading in existing Crossref output file\\n')\n",
    "\n",
    "    directory = './accessory-scripts/accessory-outputs'\n",
    "    pattern = 'true-datasets'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(reverse=True)\n",
    "    latest_file = None\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            latest_file = file\n",
    "            break\n",
    "\n",
    "    if latest_file:\n",
    "        file_path = os.path.join(directory, latest_file)\n",
    "        crossref_true_datasets = pd.read_csv(file_path)\n",
    "        print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "    else:\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    if loadPreviousData:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_pruned, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-crossref.csv', index=False)\n",
    "    elif loadPreviousDataPlus:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-figshare-crossref.csv', index=False)\n",
    "    elif loadPreviousDataPlusNCBI:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus_ncbi, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-figshare-ncbi-crossref.csv', index=False)\n",
    "    elif not loadPreviousData and not loadPreviousData and not figshareWorkflow1:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_pruned, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-crossref.csv', index=False)\n",
    "    elif not loadPreviousData and not loadPreviousData and figshareWorkflow1:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-figshare-crossref.csv', index=False)\n",
    "\n",
    "if not any([loadPreviousData, loadPreviousDataPlus, loadPreviousDataPlusNCBI]) and loadCrossrefData:\n",
    "    print('\\nReading in existing Crossref output file\\n')\n",
    "    directory = './accessory-scripts/accessory-outputs'\n",
    "    pattern = 'true-datasets'\n",
    "\n",
    "    files = os.listdir(directory)\n",
    "    files.sort(reverse=True)\n",
    "    latest_file = None\n",
    "    for file in files:\n",
    "        if pattern in file:\n",
    "            latest_file = file\n",
    "            break\n",
    "\n",
    "    if latest_file:\n",
    "        file_path = os.path.join(directory, latest_file)\n",
    "        crossref_true_datasets = pd.read_csv(file_path)\n",
    "        print(f'The most recent file \"{latest_file}\" has been loaded successfully.')\n",
    "    else:\n",
    "        print(f'No file with \"{pattern}\" was found in the directory \"{directory}\".')\n",
    "\n",
    "    if not df_datacite_pruned.empty and df_datacite_plus_dedup.empty:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_pruned, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-crossref.csv', index=False)\n",
    "    elif not df_datacite_plus_dedup.empty and df_datacite_plus_ncbi.empty:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-figshare-crossref.csv', index=False)\n",
    "    elif not df_datacite_plus_ncbi.empty:\n",
    "        df_datacite_plus_crossref = pd.concat([df_datacite_plus_ncbi, crossref_true_datasets], ignore_index=True)\n",
    "        df_datacite_plus_crossref.to_csv(f'outputs/{todayDate}_full-concatenated-dataframe-plus-figshare-ncbi-crossref.csv', index=False)\n",
    "\n",
    "print('Done.\\n')\n",
    "print(f'Time to run: {datetime.now() - startTime}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
